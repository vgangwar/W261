{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#DATASCI W261: Machine Learning at Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Vineet Gangwar\n",
    "**vineet.gangwar@gmail.com  \n",
    "W261-2: Machine Learning at Scale  \n",
    "Assignment #4  \n",
    "Date: Sep - 29 - 2015**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#HW 4.0. \n",
    "What is MrJob? How is it different to Hadoop MapReduce?  \n",
    "What are the mapper_final(), combiner_final(), reducer_final() methods? When are they called?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**  \n",
    "- *What is MRJob?*  \n",
    "MRJob is a python module, which is developed by Yelp, that provides a Python environment in which to develop, test and run MapReduce jobs.  \n",
    "  \n",
    "  \n",
    "- *How is it different to Hadoop MapReduce?*  \n",
    "MRJob can run locally, on a hadoop cluster and on Amazon EMR. To use python as map and reduce tasks in Hadoop, we use Hadoop streaming to communicate with Hadoop via stdin and stdout. While in MRJob each MapReduce job is defined as a class and the Map and reduce tasks are defined as its methods. MRJob can handle complex input/output via serialization while hadoop can only handle text as input/output. MRJob makes the output values of the mapper available to the reducer as a generator object, while Hadoop sends the key/value pairs line by line via stdin  \n",
    "  \n",
    "  \n",
    "- *What are the mapper_final(), combiner_final(), reducer_final() methods? When are they called?*  \n",
    "Mapper_final, combiner_final and reducer_final are methods provided by MRJob that helps in code organization, housekeeping and optimization. Mapper_final is called after the mapper method reaches it end of input. Similarly, combiner_final and reducer_final are called after the combiner and reducer mthods reach the end of their respective inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#HW 4.1\n",
    "What is serialization in the context of MrJob or Hadoop?  \n",
    "When it used in these frameworks?  \n",
    "What is the default serialization mode for input and outputs for MrJob?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**  \n",
    "\n",
    "- *What is serialization in the context of MrJob or Hadoop?*  \n",
    "  \n",
    "Serialization enables MRJob to pass values (simple and complex) between the tasks of a job. This is necessary because messages between mappers and reducers have to mostly travel over the network. Serialization is the process in which an object is converted into a byte stream that can be sent over the network. De-serialization is the process by which the byte stream is re-converted back into python objects\n",
    "\n",
    "- *When it used in these frameworks?*  \n",
    "  \n",
    "It is used by whenever data is exchanged between tasks of a MapReduce job and between the Hadoop framework and MRJob. Mappers, combiners, and reducers are examples of tasks\n",
    "  \n",
    "- *What is the default serialization mode for input and outputs for MrJob?*  \n",
    "  \n",
    "Default mode for input = RawValueProtocol  \n",
    "Default mode for output = JSONProtocol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#HW 4.2: \n",
    "Recall the Microsoft logfiles data from the async lecture. The logfiles described are located at:\n",
    "\n",
    "https://kdd.ics.uci.edu/databases/msweb/msweb.html  \n",
    "http://archive.ics.uci.edu/ml/machine-learning-databases/anonymous/\n",
    "\n",
    "This dataset records which areas (Vroots) of www.microsoft.com each user visited in a one-week timeframe in Feburary 1998.\n",
    "\n",
    "Here, you must preprocess the data on a single node (i.e., not on a cluster of nodes) from the format:\n",
    "  \n",
    "C,\"10001\",10001   #Visitor id 10001  \n",
    "V,1000,1          #Visit by Visitor 10001 to page id 1000  \n",
    "V,1001,1          #Visit by Visitor 10001 to page id 1001  \n",
    "V,1002,1          #Visit by Visitor 10001 to page id 1002  \n",
    "C,\"10002\",10002   #Visitor id 10001  \n",
    "V  \n",
    "Note: #denotes comments  \n",
    "to the format:  \n",
    "  \n",
    "V,1000,1,C, 10001  \n",
    "V,1001,1,C, 10001  \n",
    "V,1002,1,C, 10001  \n",
    "  \n",
    "Write the python code to accomplish this.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**  \n",
    "The code below reads the input file sequentially. Whenever a line containing customer information is found, it is stored in a string. This string is appended to all subsequent lines containing vroot information and printed to a file. The process continues until the end of input is reached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_filename = 'anonymous-msweb.data'\n",
    "output_filename = 'msweb_processed'\n",
    "\n",
    "output_filehndl = open(output_filename, 'w')\n",
    "\n",
    "current_customer = str()\n",
    "output_line = str()\n",
    "\n",
    "with open(input_filename, 'r') as input_filehndl:\n",
    "    for i, line in enumerate(input_filehndl):\n",
    "        fields = line.strip().split(',')\n",
    "        if fields[0] == 'C':\n",
    "            current_customer = fields[0] + ',' + fields[2]\n",
    "        if fields[0] == 'V':\n",
    "            output_line = line.strip() + ',' + current_customer + '\\n'\n",
    "            output_filehndl.write(output_line)\n",
    "output_filehndl.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#HW 4.3\n",
    "Find the 5 most frequently visited pages using mrjob from the output of 4.2 (i.e., transfromed log file)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**  \n",
    "  \n",
    "*Mapper*  \n",
    "The mapper splits each line of the input and emits the vroot id and the digit 1.\n",
    "  \n",
    "*Reducer*  \n",
    "The reducer sums up the counts of each vroot and stored in this information as list of tuples  \n",
    "  \n",
    "*Reducer_final*  \n",
    "Reducer_final reverse sorts the stored list of tuples by visit count and emits the top 5 vroot ids and their respective counts  \n",
    "  \n",
    "The output is in the format: Vroot_id, Visit_Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting frequent_pages_top5.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile frequent_pages_top5.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import operator\n",
    "\n",
    "class FrequentPages(MRJob):\n",
    "    def mapper(self, line_no, line):\n",
    "        # Extracts the Vroot that was visited\n",
    "        fields = line.strip().split(',')\n",
    "        if fields[0] == 'V':\n",
    "            yield fields[1], 1\n",
    "\n",
    "    def reducer_init(self):\n",
    "        self.vroot_list = list()\n",
    "    \n",
    "    def reducer(self, vroot, visit_counts):\n",
    "        # Sums Vroot visit counts\n",
    "        total = sum(visit_counts)\n",
    "        self.vroot_list.append((vroot, total))\n",
    "    \n",
    "    def reducer_final(self):\n",
    "        # Sorting vroot_list based on frequency\n",
    "        self.vroot_list = sorted(self.vroot_list, key=operator.itemgetter(1), reverse=True)\n",
    "        for item in self.vroot_list[:5]:\n",
    "            yield item\n",
    "        \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper        = self.mapper,\n",
    "                   reducer_init  = self.reducer_init,\n",
    "                   reducer       = self.reducer,\n",
    "                   reducer_final = self.reducer_final)\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'1008', 10836)\n",
      "(u'1034', 9383)\n",
      "(u'1004', 8463)\n",
      "(u'1018', 5330)\n",
      "(u'1017', 5108)\n"
     ]
    }
   ],
   "source": [
    "import imp\n",
    "import frequent_pages_top5\n",
    "imp.reload(frequent_pages_top5)\n",
    "\n",
    "mr_job = frequent_pages_top5.FrequentPages(args=['msweb_processed'])\n",
    "\n",
    "with mr_job.make_runner() as runner:\n",
    "    runner.run()\n",
    "    for line in runner.stream_output():\n",
    "        print mr_job.parse_output_line(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#HW 4.4\n",
    "Find the most frequent visitor of each page using mrjob and the output of 4.2  (i.e., transfromed log file). In this output please include the webpage URL, webpageID and Visitor ID."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**  \n",
    "To reduce output clutter, I am formatted the output in the following:  \n",
    "vroot1_id URL  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;customer1 visit_count  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;customer2 visit_count  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;...  \n",
    "vroot2_id URL  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;customer1 visit_count  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;customer2 visit_count  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;...  \n",
    "...  \n",
    "The question did not sepcify the output format, but it is a matter one line that will result in the output in the format  \n",
    "vroot1_id URL customer1 visit_count  \n",
    "vroot1_id URL customer2 visit_count  \n",
    "vroot2_id URL customer1 visit_count  \n",
    "vroot2_id URL customer2 visit_count  \n",
    "...  \n",
    "Additonally, since the output is very big so I have stored the output into a file and displayed head 50 and tail 50 line\n",
    "\n",
    "**Mapper**  \n",
    "The mapper splits the input lines and emits vroot_id and the digit 1  \n",
    "  \n",
    "**Reducer_init**  \n",
    "Reads in the lines starting with A from the original file and stored vroot_id and url in a dictionary. This dictionary is used by the reducer get url associated with vroot_ids  \n",
    "  \n",
    "**Reducer**  \n",
    "For each key (i.e. vroot_id), the reducer creates a dictionary of customer_id and customer_visit_count mapping.  \n",
    "Then the reducer, uses the dictionary created in reducer_init to obtain the url associated with a vroot_id.  \n",
    "Then it emits all customer_ids and associated visit_counts where the visit_count is maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting frequent_visitor_per_page.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile frequent_visitor_per_page.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class FrequentVisitors(MRJob):    \n",
    "    def mapper(self, line_no, line):\n",
    "        # Extracts the Vroot and customer id\n",
    "        fields = line.strip().split(',')\n",
    "        yield fields[1], fields[4]\n",
    "        \n",
    "    def reducer_init(self):\n",
    "        # Initializing dict for storing customer counts per vroot\n",
    "        self.cust_count_per_vroot = dict()\n",
    "        # Creating dict for vroot_id and url mapping\n",
    "        self.vroot_url = dict()\n",
    "        filename = '/tmp/anonymous-msweb.data'\n",
    "        with open(filename, 'r') as filehndl:\n",
    "            for line in filehndl:\n",
    "                fields = line.strip().split(',')\n",
    "                if fields[0] == 'A':\n",
    "                    vroot = fields[1]\n",
    "                    url = fields[4].replace('\"', '')\n",
    "                    self.vroot_url[vroot] = url\n",
    "    \n",
    "    def reducer(self, vroot, cust_list):\n",
    "        # For each key creating dict of customer_id and visit_count mapping\n",
    "        for cust in cust_list:\n",
    "            self.cust_count_per_vroot.setdefault(cust, 0)\n",
    "            self.cust_count_per_vroot[cust] += 1\n",
    "        # Emit vroot and url\n",
    "        yield vroot, self.vroot_url[vroot]\n",
    "        # Emit cust ids with max count in the dict self.cust_count_per_vroot\n",
    "        max_count = max(self.cust_count_per_vroot.values())\n",
    "        for k, v in self.cust_count_per_vroot.iteritems():\n",
    "            if v == max_count:\n",
    "                # Adding a little format for display as per homework requirements\n",
    "                yield \"    \" + k, str(v)\n",
    "        # Re-initializing dict\n",
    "        self.cust_count_per_vroot = dict()\n",
    "        \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper       = self.mapper,\n",
    "                   reducer_init = self.reducer_init,\n",
    "                   reducer      = self.reducer)\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Displaying head -n 50\n",
      "1000 /regwiz\n",
      "    36585 1\n",
      "    20914 1\n",
      "    35230 1\n",
      "    16073 1\n",
      "    35549 1\n",
      "    31899 1\n",
      "    33730 1\n",
      "    18882 1\n",
      "    38670 1\n",
      "    29358 1\n",
      "    28996 1\n",
      "    10457 1\n",
      "    11544 1\n",
      "    10454 1\n",
      "    22667 1\n",
      "    10511 1\n",
      "    10101 1\n",
      "    30428 1\n",
      "    15204 1\n",
      "    18950 1\n",
      "    16427 1\n",
      "    23697 1\n",
      "    13085 1\n",
      "    37596 1\n",
      "    23049 1\n",
      "    13080 1\n",
      "    33749 1\n",
      "    18420 1\n",
      "    18421 1\n",
      "    33425 1\n",
      "    29086 1\n",
      "    19409 1\n",
      "    40152 1\n",
      "    33037 1\n",
      "    13248 1\n",
      "    18817 1\n",
      "    28109 1\n",
      "    22760 1\n",
      "    32654 1\n",
      "    12930 1\n",
      "    19376 1\n",
      "    33924 1\n",
      "    22765 1\n",
      "    41651 1\n",
      "    16676 1\n",
      "    40702 1\n",
      "    40703 1\n",
      "    42458 1\n",
      "    12700 1\n",
      "\n",
      ">> Displaying tail -n 50\n",
      "    31810 1\n",
      "    21468 1\n",
      "    33561 1\n",
      "    40758 1\n",
      "    37140 1\n",
      "    29417 1\n",
      "    38571 1\n",
      "    21966 1\n",
      "    21622 1\n",
      "    20717 1\n",
      "    42385 1\n",
      "    40390 1\n",
      "    25564 1\n",
      "    39900 1\n",
      "    16980 1\n",
      "    40679 1\n",
      "    29674 1\n",
      "    25846 1\n",
      "    12297 1\n",
      "    21816 1\n",
      "    36955 1\n",
      "    11721 1\n",
      "    31229 1\n",
      "    31635 1\n",
      "    30082 1\n",
      "    17192 1\n",
      "    38981 1\n",
      "    34785 1\n",
      "    14002 1\n",
      "    41273 1\n",
      "    35305 1\n",
      "    22944 1\n",
      "    21712 1\n",
      "    23582 1\n",
      "    12556 1\n",
      "    22826 1\n",
      "    34661 1\n",
      "    21496 1\n",
      "    30564 1\n",
      "    20555 1\n",
      "    12363 1\n",
      "    36568 1\n",
      "    38573 1\n",
      "    17586 1\n",
      "    30922 1\n",
      "    39194 1\n",
      "    10776 1\n",
      "    35944 1\n",
      "    28974 1\n",
      "    41207 1\n"
     ]
    }
   ],
   "source": [
    "import imp\n",
    "import frequent_visitor_per_page\n",
    "imp.reload(frequent_visitor_per_page)\n",
    "\n",
    "# Copying file anonymous-msweb.data to /tmp\n",
    "!cp anonymous-msweb.data /tmp/\n",
    "\n",
    "output_filename = 'output'\n",
    "filehndl = open(output_filename, 'w')\n",
    "mr_job = frequent_visitor_per_page.FrequentVisitors(args=['msweb_processed'])\n",
    "\n",
    "with mr_job.make_runner() as runner:\n",
    "    runner.run()\n",
    "    for line in runner.stream_output():\n",
    "        # Writing job output into a file\n",
    "        outline = \" \".join(mr_job.parse_output_line(line))\n",
    "        filehndl.write(outline + '\\n')\n",
    "filehndl.close()\n",
    "\n",
    "# Displaying head -n 50\n",
    "print \">> Displaying head -n 50\"\n",
    "!head -n 50 output\n",
    "# Displaying tail -n 50\n",
    "print \"\\n>> Displaying tail -n 50\"\n",
    "!tail -n 50 output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#HW 4.5 \n",
    "Here you will use a different dataset consisting of word-frequency distributions \n",
    "for 1,000 Twitter users. These Twitter users use language in very different ways,\n",
    "and were classified by hand according to the criteria:\n",
    "\n",
    "0: Human, where only basic human-human communication is observed.\n",
    "\n",
    "1: Cyborg, where language is primarily borrowed from other sources\n",
    "(e.g., jobs listings, classifieds postings, advertisements, etc...).\n",
    "\n",
    "2: Robot, where language is formulaically derived from unrelated sources\n",
    "(e.g., weather/seismology, police/fire event logs, etc...).\n",
    "\n",
    "3: Spammer, where language is replicated to high multiplicity\n",
    "(e.g., celebrity obsessions, personal promotion, etc... )\n",
    "\n",
    "Check out the preprints of our recent research,\n",
    "which spawned this dataset:\n",
    "\n",
    "http://arxiv.org/abs/1505.04342  \n",
    "http://arxiv.org/abs/1508.01843  \n",
    "\n",
    "The main data lie in the accompanying file:\n",
    "\n",
    "topUsers_Apr-Jul_2014_1000-words.txt\n",
    "\n",
    "and are of the form:\n",
    "\n",
    "USERID,CODE,TOTAL,WORD1_COUNT,WORD2_COUNT,...\n",
    ".\n",
    ".\n",
    "\n",
    "where\n",
    "\n",
    "USERID = unique user identifier\n",
    "CODE = 0/1/2/3 class code\n",
    "TOTAL = sum of the word counts\n",
    "\n",
    "Using this data, you will implement a 1000-dimensional K-means algorithm on the users\n",
    "by their 1000-dimensional word stripes/vectors using several \n",
    "centroid initializations and values of K.\n",
    "\n",
    "Note that each \"point\" is a user as represented by 1000 words, and that\n",
    "word-frequency distributions are generally heavy-tailed power-laws\n",
    "(often called Zipf distributions), and are very rare in the larger class\n",
    "of discrete, random distributions. For each user you will have to normalize\n",
    "by its \"TOTAL\" column. Try several parameterizations and initializations:\n",
    "\n",
    "(A) K=4 uniform random centroid-distributions over the 1000 words  \n",
    "(B) K=2 perturbation-centroids, randomly perturbed from the aggregated (user-wide) distribution   \n",
    "(C) K=4 perturbation-centroids, randomly perturbed from the aggregated (user-wide) distribution   \n",
    "(D) K=4 \"trained\" centroids, determined by the sums across the classes.  \n",
    "\n",
    "and iterate until a threshold (try 0.001) is reached.\n",
    "After convergence, print out a summary of the classes present in each cluster.\n",
    "In particular, report the composition as measured by the total\n",
    "portion of each class type (0-3) contained in each cluster,\n",
    "and discuss your findings and any differences in outcomes across parts A-D.\n",
    "\n",
    "Note that you do not have to compute the aggregated distribution or the \n",
    "class-aggregated distributions, which are rows in the auxiliary file:\n",
    "\n",
    "topUsers_Apr-Jul_2014_1000-words_summaries.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**  \n",
    "###Results\n",
    "Class distribution per cluster  \n",
    "4.5 (A): Uniform Random K=4\n",
    "\n",
    "Number of Iterations: **15**\n",
    "\n",
    "\n",
    "|        |   0 |  1 |  2 |  3 | total |\n",
    "|--------|-----|----|----|----|-------|\n",
    "|cluster |     |    |    |    |       |\n",
    "|0       |   1 | 82 | 34 |  4 |   121 |\n",
    "|1       | 171 |  2 | 17 | 27 |   217 |\n",
    "|2       |   0 |  6 |  3 |  0 |     9 |\n",
    "|3       | 580 |  1 |  0 | 72 |   653 |\n",
    "\n",
    "4.5 (B): Perturbed Random K=2\n",
    "\n",
    "Number of Iterations: **4**\n",
    "\n",
    "|        |   0 |  1 |  2 |  3 | total |\n",
    "|--------|-----|----|----|----|-------|\n",
    "|cluster |     |    |    |    |       |\n",
    "|0       | 751 |  3 | 16 | 99 |   869 |\n",
    "|1       |   1 | 88 | 38 |  4 |   131 |\n",
    "|2       |   0 |  0 |  0 |  0 |     0 |\n",
    "|3       |   0 |  0 |  0 |  0 |     0 |\n",
    "4.5 (C) Perturbed Random K=4\n",
    "\n",
    "Number of Iterations: **8**\n",
    "\n",
    "|        |  0  | 1  | 2  | 3  |total  |\n",
    "|--------|-----|----|----|----|-------|\n",
    "|cluster |     |    |    |    |       |\n",
    "|0       | 93  | 2  |14  |57  |  166  |\n",
    "|1       |658  | 1  | 0  |42  |  701  |\n",
    "|2       |  0  |51  | 2  | 0  |   53  |\n",
    "|3       |  1  |37  |38  | 4  |   80  |\n",
    "\n",
    "4.5 (D) Trained K=4\n",
    "\n",
    "Number of Iterations: **5**\n",
    "\n",
    "|        | 0  | 1  | 2  | 3  |total  |\n",
    "|--------|----|----|----|----|-------|\n",
    "|cluster |    |    |    |    |       |\n",
    "|0       |749 |  3 | 14 | 38 |   804 |\n",
    "|1       |  0 | 51 |  0 |  0 |    51 |\n",
    "|2       |  1 | 37 | 40 |  4 |    82 |\n",
    "|3       |  2 |  0 |  0 | 61 |    63 |\n",
    "\n",
    "###Discussion of results\n",
    "Trained centroids did converge fast but did not result in perfect clustering. When K=4, perturbed random provided better (closer to trained centroids) and faster results than uniform random. Perturbed random with K=2 was the fasted but that's probably because the centroids were far enough. From the above results, perturbed random seems to be the best option for selecting centroids. It will be interesting to check if Kmeans++ is a better algorithm at choosing seed centroids than perturbed random\n",
    "\n",
    "\n",
    "**Code Description**  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**Mapper**  \n",
    "The mapper is helped by the mapper_init method and the two other help functions. The mapper does three things:  \n",
    "- Normalizes each observation by the respective sum of words\n",
    "- Uses an helper function to find the index of the closest centroid\n",
    "- Emits centroid index and a comman separated string containing the observation\n",
    "  \n",
    "Mapper helper functions:  \n",
    "- Mapper_init:     Reads in centroids from an external file as stores in nd.array\n",
    "- Distance:        Returns the Euclidean distance between two 1000 points. (It can handle any number of dimensions)\n",
    "- Closest_Centroid: Given centroids and an observation, it returns of the index of the closest centroid to the observation\n",
    "  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**Reducer**  \n",
    "The reducer receives a centroid index as key and all observations that are closet to it. It does the following:  \n",
    "- Store all the observations in a nd.array\n",
    "- Yield the index and the number of observations (This shows the change in the number of observations per cluster per iteration)\n",
    "- Calculates new centroids as the mean of the observations per cluster\n",
    "- Use Reducer_final method to write the new centroids back into the centroids file\n",
    "  \n",
    "Reducer helper functions:  \n",
    "- Reducer_init:    Reads centroids from a file and makes it available to the reducer\n",
    "- Reducer_final:   Writes back the newly calculated centroids into the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting kmeans_twitter.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile kmeans_twitter.py\n",
    "\n",
    "import mrjob\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import numpy as np\n",
    "\n",
    "def distance(np_data1, np_data2):\n",
    "    # Calculate Euclidean distance between two points in 1000 dimensional space\n",
    "    diff = np_data1 - np_data2\n",
    "    diff_sq = diff**2\n",
    "    diff_sq_sum = sum(diff_sq)\n",
    "    dist = diff_sq_sum**0.5\n",
    "    return dist\n",
    "\n",
    "def closest_centroid(centroids, observation):\n",
    "    # Given centroids and an observation, this function returns the index of the closet centroid to the observation\n",
    "    # Number of centroids\n",
    "    num_centroids = len(centroids)\n",
    "    # Initializing distance array that will hold distances of the observation to the centroids\n",
    "    np.distances = np.zeros(num_centroids)\n",
    "    # Updating distance array\n",
    "    for i, centroid in enumerate(centroids):\n",
    "        np.distances[i] = distance(centroid, observation)\n",
    "    # return closet centroid's index using argmin\n",
    "    return np.argmin(np.distances)\n",
    "\n",
    "class kmeans(MRJob):\n",
    "    def mapper_init(self):\n",
    "        # Reading centroids and storing this as numpy nd.array\n",
    "        filename = '/tmp/centroids'\n",
    "        self.centroids = list()\n",
    "        with open(filename, 'r') as filehndl:\n",
    "            for line in filehndl:\n",
    "                fields = line.strip().split(',')\n",
    "                np_fields = np.array(fields).astype(float)\n",
    "                self.centroids.append(np_fields)\n",
    "        self.centroids = np.array(self.centroids)\n",
    "    \n",
    "    def mapper(self, line_no, line):\n",
    "        # Gets a observation at a time\n",
    "        # 1st step: Normalize the observation using sum of words\n",
    "        # 2nd Step: Find the index of the closet centroid\n",
    "        # 3rd Step: Emit the index and the observation as a comman separated string\n",
    "        fields = line.strip().split(',')\n",
    "        user_id = fields[0]\n",
    "        user_class = fields[1]\n",
    "        sum_words = float(fields[2])\n",
    "        all_words = fields[3:]\n",
    "        np_all_words = np.array(all_words).astype(float) / sum_words\n",
    "        nearest_centroid_idx = closest_centroid(self.centroids, np_all_words)\n",
    "        #print \"np_all_words\", ','.join(np_all_words.astype(str))\n",
    "        yield nearest_centroid_idx, ','.join(np_all_words.astype(str))\n",
    "    \n",
    "    def reducer_init(self):\n",
    "        # Stores all the observations/members for a class. It is used to calculate the new centroid by calculating the means\n",
    "        self.class_members = list()\n",
    "        \n",
    "        # Creating variable to store new_centroids that will be created by the reducer\n",
    "        self.new_centroids = list()\n",
    "        # Initializing self.new_centroids with initial values from the centroids file\n",
    "        filename = '/tmp/centroids'\n",
    "        with open(filename, 'r') as filehndl:\n",
    "            for line in filehndl:\n",
    "                fields = line.strip().split(',')\n",
    "                np_fields = np.array(fields).astype(float)\n",
    "                self.new_centroids.append(np_fields)\n",
    "        self.new_centroids = np.array(self.new_centroids)\n",
    "    \n",
    "    def reducer(self, idx, members):\n",
    "        # Read all the observations/members of a class and store it in nd.array\n",
    "        for member in members:\n",
    "            member_list = member.split(',')\n",
    "            self.class_members.append(member_list)\n",
    "        np_class_members = np.array(self.class_members).astype(float)\n",
    "        \n",
    "        # yield cluster index and number of members\n",
    "        yield idx, np_class_members.shape[0]\n",
    "        \n",
    "        # Finding mean of members i.e. new centroid\n",
    "        new_centroid = np.mean(np_class_members, axis=0)\n",
    "        # Updating new centroid into variable initialized in Reducer_init\n",
    "        self.new_centroids[idx] = new_centroid\n",
    "        # Resetting self.class_members\n",
    "        self.class_members = list()\n",
    "    \n",
    "    def reducer_final(self):\n",
    "        # Write new_centroids into file\n",
    "        filename = '/tmp/centroids'\n",
    "        filehndl = open(filename, 'w')\n",
    "        for i in self.new_centroids:\n",
    "            centroid = ','.join(i.astype(str))\n",
    "            filehndl.write(centroid + '\\n')\n",
    "        filehndl.close()\n",
    "        \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper_init   = self.mapper_init,\n",
    "                   mapper        = self.mapper,\n",
    "                   reducer_init  = self.reducer_init,\n",
    "                   reducer       = self.reducer,\n",
    "                   reducer_final = self.reducer_final)\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Helper functions to obtain seed centroids  \n",
    "The functions in the cell below are used to calculate seed centroids.  \n",
    "\n",
    "- K=4 Uniform random:\n",
    " 1. Read_input_into_ndarray(): Reads and normalizes topUsers_Apr-Jul_2014_1000-words.txt into nd.array\n",
    " 2. uniform_random_centroids(): Randomly selects centroids from the above nd.array\n",
    "- K=2 or 4 Perturbed random:\n",
    " 1. Dataset_Centroid(): Calculates and returns the centroid of the entire dataset\n",
    " 2. Get_perturbed_centroids(): Perturbs the dataset centroid and returns 2 or 4 new centroids\n",
    "- K=4 Trained:\n",
    " 1. Get_trainined_centroids(): Returns normalized mean of the sum of all labeled features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Functions to get centroids\n",
    "import numpy as np\n",
    "\n",
    "# Getting uniform random Centroids\n",
    "# ==========\n",
    "# Reading input file topUsers_Apr-Jul_2014_1000-words.txt into memory\n",
    "# Returns normalized nd.array of the observations\n",
    "def read_input_into_ndarray(filename):\n",
    "    data_list = list()\n",
    "    with open(filename, 'r') as filehndl:\n",
    "        for line in filehndl:\n",
    "            fields = line.strip().split(',')\n",
    "            user_id = fields[0]\n",
    "            user_class = fields[1]\n",
    "            sum_words = float(fields[2])\n",
    "            all_words = fields[3:]\n",
    "            np_all_words = np.array(all_words).astype(float) / sum_words\n",
    "            data_list.append(np_all_words)\n",
    "    np_data_list = np.array(data_list)\n",
    "    return np_data_list\n",
    "\n",
    "# Choosing seed uniform centroids randomly from the data\n",
    "def uniform_random_centroids(np_data_list, num_centroids):\n",
    "    centroids = list()\n",
    "    for i in range(np_data_list.shape[1]):\n",
    "        column = np_data_list[:,i]\n",
    "        choice = np.random.choice(column, size=num_centroids, replace=True)\n",
    "        centroids.append(choice)\n",
    "    np_centroids = np.array(centroids)\n",
    "    np_centroids = np_centroids.transpose()\n",
    "    return np_centroids\n",
    "# ==========\n",
    "\n",
    "# Getting Perturbed Centroids\n",
    "# ==========\n",
    "def Dataset_Centroid():\n",
    "    # Reading aggregrate values of the dataset and return dataset centroid\n",
    "    filename = 'topUsers_Apr-Jul_2014_1000-words_summaries.txt'\n",
    "    with open(filename, 'r') as filehndl:\n",
    "        for line in filehndl:\n",
    "            fields = line.strip().split(',')\n",
    "            if fields[0] == 'ALL_CODES':\n",
    "                sum_all_words = float(fields[2])\n",
    "                all_words = fields[3:]\n",
    "                # finding centroids of dataset\n",
    "                dataset_centroid = np.array(all_words).astype(float) / sum_all_words\n",
    "    return dataset_centroid\n",
    "\n",
    "def get_perturbed_centroids(dataset_centroid, k):\n",
    "    # Generating perturbed centroids\n",
    "    perturbations = np.random.normal(0, 0.0001, (k, 1000))\n",
    "    new_centroids = perturbations + dataset_centroid\n",
    "    return new_centroids\n",
    "# ==========\n",
    "\n",
    "# Getting Trained Centroids\n",
    "# ==========\n",
    "def get_trainined_centroids():\n",
    "    # Reading aggregrate values of the dataset\n",
    "    centroids = list()\n",
    "    filename = 'topUsers_Apr-Jul_2014_1000-words_summaries.txt'\n",
    "    with open(filename, 'r') as filehndl:\n",
    "        for line in filehndl:\n",
    "            fields = line.strip().split(',')\n",
    "            if fields[0] == 'CODE':\n",
    "                sum_all_words = float(fields[2])\n",
    "                all_words = fields[3:]\n",
    "                class_centroid = np.array(all_words).astype(float) / sum_all_words\n",
    "                centroids.append(class_centroid)\n",
    "    centroids = np.array(centroids)\n",
    "    return centroids\n",
    "# =========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional helper functions\n",
    "- write_to_centroids_to_file(): Writes an nd.array containing centroids to file\n",
    "- read_centroids_file(): Reads a file containing centroids and returns a flattened one dimensional result\n",
    "- stop_criterion(): Stopping criteria to stop the algorithm. All dimensions of all successive centroids below a preset threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Other supporting functions\n",
    "\n",
    "# Writing Centroids into file\n",
    "def write_to_centroids_to_file(centroids):\n",
    "    filename = '/tmp/centroids'\n",
    "    filehndl = open(filename, 'w')\n",
    "    for i in centroids:\n",
    "        centroid = ','.join(i.astype(str))\n",
    "        filehndl.write(centroid + '\\n')\n",
    "    filehndl.close()\n",
    "# -----------\n",
    "\n",
    "# To read a flattened centroids file\n",
    "def read_centroids_file():\n",
    "    filename = '/tmp/centroids'\n",
    "    filetxt = list()\n",
    "    with open(filename, 'r') as filehndl:\n",
    "        for line in filehndl:\n",
    "            line = line.strip().split(',')\n",
    "            filetxt.append(line)\n",
    "    flattened = np.array(filetxt).flatten().astype(float)\n",
    "    return flattened\n",
    "\n",
    "# Stopping criteria\n",
    "# Check whether centroids converge\n",
    "def stop_criterion(centroid_points_old, centroid_points_new, t):\n",
    "    diff = centroid_points_old - centroid_points_new\n",
    "    diff = np.absolute(diff)\n",
    "    flag = True\n",
    "    for i in diff:\n",
    "        if(i>t):\n",
    "            flag = False\n",
    "            break\n",
    "    return flag\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##4.5(A)\n",
    "###K=4 uniform random centroid-distributions over the 1000 words\n",
    "It follows the following steps:\n",
    "- Generate Centroids by using helper function described above\n",
    "- Write centroids into file\n",
    "- Run MapReduce job with an upper limit of 1000 iterations\n",
    "- Prints number of members of each cluster after each iteration\n",
    "- Stops after the stopping criteria (described above) threshold of 0.001 is breached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed Centroids:\n",
      "[[  2.47728442e-02   8.58461208e-04   3.51624933e-02 ...,   0.00000000e+00\n",
      "    2.02691746e-05   3.34552059e-04]\n",
      " [  5.38851169e-03   4.87462443e-02   2.42808129e-02 ...,   0.00000000e+00\n",
      "    1.95151568e-04   1.18536782e-04]\n",
      " [  2.41937330e-02   2.58005044e-02   2.09809479e-02 ...,   0.00000000e+00\n",
      "    9.45596671e-05   6.49878148e-05]\n",
      " [  9.91673683e-03   5.26544147e-02   1.22710528e-02 ...,   0.00000000e+00\n",
      "    2.84123196e-05   0.00000000e+00]]\n",
      ">> Iteration: 1\n",
      ">> (Assigned Cluster, Number of Obserations)\n",
      "(0, 156)\n",
      "(1, 20)\n",
      "(2, 2)\n",
      "(3, 822)\n",
      ">> Iteration: 2\n",
      ">> (Assigned Cluster, Number of Obserations)\n",
      "(0, 134)\n",
      "(1, 176)\n",
      "(2, 3)\n",
      "(3, 687)\n",
      ">> Iteration: 3\n",
      ">> (Assigned Cluster, Number of Obserations)\n",
      "(0, 123)\n",
      "(1, 238)\n",
      "(2, 8)\n",
      "(3, 631)\n",
      ">> Iteration: 4\n",
      ">> (Assigned Cluster, Number of Obserations)\n",
      "(0, 121)\n",
      "(1, 273)\n",
      "(2, 10)\n",
      "(3, 596)\n",
      ">> Iteration: 5\n",
      ">> (Assigned Cluster, Number of Obserations)\n",
      "(0, 122)\n",
      "(1, 295)\n",
      "(2, 9)\n",
      "(3, 574)\n",
      ">> Iteration: 6\n",
      ">> (Assigned Cluster, Number of Obserations)\n",
      "(0, 121)\n",
      "(1, 316)\n",
      "(2, 9)\n",
      "(3, 554)\n",
      ">> Iteration: 7\n",
      ">> (Assigned Cluster, Number of Obserations)\n",
      "(0, 121)\n",
      "(1, 318)\n",
      "(2, 9)\n",
      "(3, 552)\n",
      ">> Iteration: 8\n",
      ">> (Assigned Cluster, Number of Obserations)\n",
      "(0, 121)\n",
      "(1, 316)\n",
      "(2, 9)\n",
      "(3, 554)\n",
      ">> Iteration: 9\n",
      ">> (Assigned Cluster, Number of Obserations)\n",
      "(0, 121)\n",
      "(1, 321)\n",
      "(2, 9)\n",
      "(3, 549)\n",
      ">> Iteration: 10\n",
      ">> (Assigned Cluster, Number of Obserations)\n",
      "(0, 121)\n",
      "(1, 304)\n",
      "(2, 9)\n",
      "(3, 566)\n",
      ">> Iteration: 11\n",
      ">> (Assigned Cluster, Number of Obserations)\n",
      "(0, 121)\n",
      "(1, 290)\n",
      "(2, 9)\n",
      "(3, 580)\n",
      ">> Iteration: 12\n",
      ">> (Assigned Cluster, Number of Obserations)\n",
      "(0, 121)\n",
      "(1, 269)\n",
      "(2, 9)\n",
      "(3, 601)\n",
      ">> Iteration: 13\n",
      ">> (Assigned Cluster, Number of Obserations)\n",
      "(0, 121)\n",
      "(1, 249)\n",
      "(2, 9)\n",
      "(3, 621)\n",
      ">> Iteration: 14\n",
      ">> (Assigned Cluster, Number of Obserations)\n",
      "(0, 121)\n",
      "(1, 235)\n",
      "(2, 9)\n",
      "(3, 635)\n",
      ">> Iteration: 15\n",
      ">> (Assigned Cluster, Number of Obserations)\n",
      "(0, 121)\n",
      "(1, 227)\n",
      "(2, 9)\n",
      "(3, 643)\n",
      "Stopping Criteria Reached\n"
     ]
    }
   ],
   "source": [
    "# MapReduce for Uniform Random seed centroids\n",
    "\n",
    "import imp\n",
    "from itertools import chain\n",
    "import kmeans_twitter\n",
    "imp.reload(kmeans_twitter)\n",
    "\n",
    "input_file_name = 'topUsers_Apr-Jul_2014_1000-words.txt'\n",
    "mr_job = kmeans_twitter.kmeans(args=[input_file_name])\n",
    "\n",
    "# Getting Centroids\n",
    "num_centroids = 4\n",
    "np_data_list = read_input_into_ndarray(input_file_name)\n",
    "centroids = uniform_random_centroids(np_data_list, num_centroids)\n",
    "print \"Seed Centroids:\"\n",
    "print centroids\n",
    "\n",
    "# Write Centroid to file\n",
    "write_to_centroids_to_file(centroids)\n",
    "\n",
    "# Running MapReduce twice with overall iteration limit set to 1000\n",
    "for i in range(1000):\n",
    "    print \">> Iteration:\", i+1\n",
    "    print \">> (Assigned Cluster, Number of Obserations)\"\n",
    "    old_centroids = read_centroids_file()\n",
    "    # Running MapReduce job\n",
    "    with mr_job.make_runner() as runner:\n",
    "        runner.run()\n",
    "        # Capturing MapReduce job output and printing to console\n",
    "        for line in runner.stream_output():\n",
    "            print mr_job.parse_output_line(line)\n",
    "    new_centroids = read_centroids_file()\n",
    "    if stop_criterion(old_centroids, new_centroids, 0.001):\n",
    "        print \"Stopping Criteria Reached\"\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Helper functions for summarizing results\n",
    "- Distance(): Returns the Euclidean distance between two 1000 points. (It can handle any number of dimensions)\n",
    "- Closest_Centroid(): Given centroids and an observation, it returns of the index of the closest centroid to the observation\n",
    "- Read_final_centroids(): Reads the final centroids file generated by Kmeans and returns it in an nd.array\n",
    "- input_data_generator(): This function produces a generator. It generates all the 1000 normalized observations in the original dataset along with their label\n",
    "- results_data_structure(): Returns a 4 X 4 pandas DataFrame that will store the class break-up per cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Support functions for summarizing clustering results  \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def distance(np_data1, np_data2):\n",
    "    # Calculate Euclidean distance between two points in 1000 dimensional space\n",
    "    diff = np_data1 - np_data2\n",
    "    diff_sq = diff**2\n",
    "    diff_sq_sum = sum(diff_sq)\n",
    "    dist = diff_sq_sum**0.5\n",
    "    return dist\n",
    "\n",
    "def closest_centroid(centroids, observation):\n",
    "    # Number of centroids\n",
    "    num_centroids = len(centroids)\n",
    "    # Initializing distance array\n",
    "    np.distances = np.zeros(num_centroids)\n",
    "    # Updating distance array\n",
    "    for i, centroid in enumerate(centroids):\n",
    "        np.distances[i] = distance(centroid, observation)\n",
    "    #print np.distances\n",
    "    return np.argmin(np.distances)\n",
    "\n",
    "# Reading final cluster file\n",
    "def read_final_centroids():\n",
    "    filename = '/tmp/centroids'\n",
    "    centroids = list()\n",
    "    with open(filename, 'r') as filehndl:\n",
    "        for line in filehndl:\n",
    "            fields = line.strip().split(',')\n",
    "            np_fields = np.array(fields).astype(float)\n",
    "            centroids.append(np_fields)\n",
    "    centroids = np.array(centroids)\n",
    "    return centroids\n",
    "\n",
    "# Input data generator\n",
    "# Yields label and normalized observation data for each entry in the original dataset\n",
    "def input_data_generator():\n",
    "    filename = 'topUsers_Apr-Jul_2014_1000-words.txt'\n",
    "    with open(filename, 'r') as filehndl:\n",
    "        for line in filehndl:    \n",
    "            fields = line.strip().split(',')\n",
    "            user_id = fields[0]\n",
    "            user_class = fields[1]\n",
    "            sum_words = float(fields[2])\n",
    "            all_words = fields[3:]\n",
    "            np_all_words = np.array(all_words).astype(float) / sum_words\n",
    "            yield (user_class, np_all_words)\n",
    "\n",
    "# Initialize final result data structure\n",
    "def results_data_structure():\n",
    "    results = dict()\n",
    "    results['cluster'] = [0, 1, 2, 3]\n",
    "    results['total'] = [0 for i in range(4)]\n",
    "    results[0] = [0 for i in range(4)]\n",
    "    results[1] = [0 for i in range(4)]\n",
    "    results[2] = [0 for i in range(4)]\n",
    "    results[3] = [0 for i in range(4)]\n",
    "    results = pd.DataFrame(results)\n",
    "    results.set_index('cluster', inplace=True)\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The code below generates the class break-up per cluster\n",
    "The rows are the clusters as assigned by Kmeans.  \n",
    "The columns are the number of observations per label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Cluster Summary\n",
      ">> Rows: Assigned Cluster; Columns: Actual Classes\n",
      "\n",
      "\n",
      "           0   1   2   3  total\n",
      "cluster                        \n",
      "0          1  82  34   4    121\n",
      "1        171   2  17  27    217\n",
      "2          0   6   3   0      9\n",
      "3        580   1   0  72    653\n"
     ]
    }
   ],
   "source": [
    "# Generating summary results for uniform random centroids\n",
    "centroids = read_final_centroids()\n",
    "results = results_data_structure()\n",
    "\n",
    "# Looping through Input data\n",
    "for c, d in input_data_generator():\n",
    "    assigned_centroid = closest_centroid(centroids, d)\n",
    "    results.loc[assigned_centroid, 'total'] += 1\n",
    "    results.loc[assigned_centroid, int(c)] += 1\n",
    "\n",
    "print \">> Cluster Summary\"\n",
    "print \">> Rows: Assigned Cluster; Columns: Actual Classes\"\n",
    "print \"\\n\"\n",
    "print results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##4.5(B)\n",
    "###K=2 perturbation-centroids, randomly perturbed from the aggregated (user-wide) distribution\n",
    "It follows the following steps:\n",
    "- Generate Centroids by using helper function described above\n",
    "- Write centroids into file\n",
    "- Run MapReduce job with an upper limit of 1000 iterations\n",
    "- Prints number of members of each cluster after each iteration\n",
    "- Stops after the stopping criteria (described above) threshold of 0.001 is breached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed Centroids:\n",
      "[[  4.01222448e-02   3.21598335e-02   2.16428587e-02 ...,   1.27805138e-04\n",
      "    2.33599521e-04   3.96367664e-04]\n",
      " [  4.03233081e-02   3.21045269e-02   2.15714409e-02 ...,   8.73486939e-05\n",
      "    2.01127812e-04   4.43482525e-04]]\n",
      ">> Iteration: 1\n",
      ">> (Assigned Cluster, Number of Obserations)\n",
      "(0, 839)\n",
      "(1, 161)\n",
      ">> Iteration: 2\n",
      ">> (Assigned Cluster, Number of Obserations)\n",
      "(0, 866)\n",
      "(1, 134)\n",
      ">> Iteration: 3\n",
      ">> (Assigned Cluster, Number of Obserations)\n",
      "(0, 869)\n",
      "(1, 131)\n",
      ">> Iteration: 4\n",
      ">> (Assigned Cluster, Number of Obserations)\n",
      "(0, 869)\n",
      "(1, 131)\n",
      "Stopping Criteria Reached\n"
     ]
    }
   ],
   "source": [
    "# MapReduce for perturbed centroids with K=2\n",
    "\n",
    "import imp\n",
    "from itertools import chain\n",
    "import kmeans_twitter\n",
    "imp.reload(kmeans_twitter)\n",
    "\n",
    "input_file_name = 'topUsers_Apr-Jul_2014_1000-words.txt'\n",
    "mr_job = kmeans_twitter.kmeans(args=[input_file_name])\n",
    "\n",
    "# Getting Centroids\n",
    "num_centroids = 2\n",
    "centroids = get_perturbed_centroids(Dataset_Centroid(), num_centroids)\n",
    "print \"Seed Centroids:\"\n",
    "print centroids\n",
    "\n",
    "# Write Centroid to file\n",
    "write_to_centroids_to_file(centroids)\n",
    "\n",
    "# Running MapReduce twice with overall iteration limit set to 1000\n",
    "for i in range(1000):\n",
    "    print \">> Iteration:\", i+1\n",
    "    print \">> (Assigned Cluster, Number of Obserations)\"\n",
    "    old_centroids = read_centroids_file()\n",
    "    # Running MapReduce job\n",
    "    with mr_job.make_runner() as runner:\n",
    "        runner.run()\n",
    "        # Capturing MapReduce job output and printing to console\n",
    "        for line in runner.stream_output():\n",
    "            print mr_job.parse_output_line(line)\n",
    "    new_centroids = read_centroids_file()\n",
    "    if stop_criterion(old_centroids, new_centroids, 0.001):\n",
    "        print \"Stopping Criteria Reached\"\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The code below generates the class break-up per cluster\n",
    "The rows are the clusters as assigned by Kmeans.  \n",
    "The columns are the number of observations per label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Cluster Summary\n",
      ">> Rows: Assigned Cluster; Columns: Actual Classes\n",
      "\n",
      "\n",
      "           0   1   2   3  total\n",
      "cluster                        \n",
      "0        751   3  16  99    869\n",
      "1          1  88  38   4    131\n",
      "2          0   0   0   0      0\n",
      "3          0   0   0   0      0\n"
     ]
    }
   ],
   "source": [
    "# Generating summary results for perturbed random centroids for k=2\n",
    "centroids = read_final_centroids()\n",
    "results = results_data_structure()\n",
    "\n",
    "# Looping through Input data\n",
    "for c, d in input_data_generator():\n",
    "    assigned_centroid = closest_centroid(centroids, d)\n",
    "    results.loc[assigned_centroid, 'total'] += 1\n",
    "    results.loc[assigned_centroid, int(c)] += 1\n",
    "\n",
    "print \">> Cluster Summary\"\n",
    "print \">> Rows: Assigned Cluster; Columns: Actual Classes\"\n",
    "print \"\\n\"\n",
    "print results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##4.5(C)\n",
    "###K=4 perturbation-centroids, randomly perturbed from the aggregated (user-wide) distribution\n",
    "It follows the following steps:\n",
    "- Generate Centroids by using helper function described above\n",
    "- Write centroids into file\n",
    "- Run MapReduce job with an upper limit of 1000 iterations\n",
    "- Prints number of members of each cluster after each iteration\n",
    "- Stops after the stopping criteria (described above) threshold of 0.001 is breached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed Centroids:\n",
      "[[ 0.04019582  0.03221462  0.02142127 ...,  0.00023767  0.00024372\n",
      "   0.00020157]\n",
      " [ 0.04028157  0.03220953  0.02162268 ...,  0.00020268  0.00011471\n",
      "   0.00025136]\n",
      " [ 0.04037788  0.03207314  0.02147636 ...,  0.00014003  0.00020488\n",
      "   0.0001804 ]\n",
      " [ 0.04035386  0.03214002  0.02142946 ...,  0.00014809  0.00023687\n",
      "   0.00027616]]\n",
      ">> Iteration: 1\n",
      ">> (Assigned Cluster, Number of Obserations)\n",
      "(0, 347)\n",
      "(1, 456)\n",
      "(2, 134)\n",
      "(3, 63)\n",
      ">> Iteration: 2\n",
      ">> (Assigned Cluster, Number of Obserations)\n",
      "(0, 306)\n",
      "(1, 539)\n",
      "(2, 64)\n",
      "(3, 91)\n",
      ">> Iteration: 3\n",
      ">> (Assigned Cluster, Number of Obserations)\n",
      "(0, 264)\n",
      "(1, 602)\n",
      "(2, 53)\n",
      "(3, 81)\n",
      ">> Iteration: 4\n",
      ">> (Assigned Cluster, Number of Obserations)\n",
      "(0, 212)\n",
      "(1, 655)\n",
      "(2, 53)\n",
      "(3, 80)\n",
      ">> Iteration: 5\n",
      ">> (Assigned Cluster, Number of Obserations)\n",
      "(0, 196)\n",
      "(1, 671)\n",
      "(2, 53)\n",
      "(3, 80)\n",
      ">> Iteration: 6\n",
      ">> (Assigned Cluster, Number of Obserations)\n",
      "(0, 182)\n",
      "(1, 685)\n",
      "(2, 53)\n",
      "(3, 80)\n",
      ">> Iteration: 7\n",
      ">> (Assigned Cluster, Number of Obserations)\n",
      "(0, 171)\n",
      "(1, 696)\n",
      "(2, 53)\n",
      "(3, 80)\n",
      ">> Iteration: 8\n",
      ">> (Assigned Cluster, Number of Obserations)\n",
      "(0, 167)\n",
      "(1, 700)\n",
      "(2, 53)\n",
      "(3, 80)\n",
      "Stopping Criteria Reached\n"
     ]
    }
   ],
   "source": [
    "# MapReduce for perturbed centroids with K=4\n",
    "\n",
    "import imp\n",
    "from itertools import chain\n",
    "import kmeans_twitter\n",
    "imp.reload(kmeans_twitter)\n",
    "\n",
    "input_file_name = 'topUsers_Apr-Jul_2014_1000-words.txt'\n",
    "mr_job = kmeans_twitter.kmeans(args=[input_file_name])\n",
    "\n",
    "# Getting Centroids\n",
    "num_centroids = 4\n",
    "centroids = get_perturbed_centroids(Dataset_Centroid(), num_centroids)\n",
    "print \"Seed Centroids:\"\n",
    "print centroids\n",
    "\n",
    "# Write Centroid to file\n",
    "write_to_centroids_to_file(centroids)\n",
    "\n",
    "# Running MapReduce twice with overall iteration limit set to 1000\n",
    "for i in range(1000):\n",
    "    print \">> Iteration:\", i+1\n",
    "    print \">> (Assigned Cluster, Number of Obserations)\"\n",
    "    old_centroids = read_centroids_file()\n",
    "    # Running MapReduce job\n",
    "    with mr_job.make_runner() as runner:\n",
    "        runner.run()\n",
    "        # Capturing MapReduce job output and printing to console\n",
    "        for line in runner.stream_output():\n",
    "            print mr_job.parse_output_line(line)\n",
    "    new_centroids = read_centroids_file()\n",
    "    if stop_criterion(old_centroids, new_centroids, 0.001):\n",
    "        print \"Stopping Criteria Reached\"\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The code below generates the class break-up per cluster\n",
    "The rows are the clusters as assigned by Kmeans.  \n",
    "The columns are the number of observations per label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Cluster Summary\n",
      ">> Rows: Assigned Cluster; Columns: Actual Classes\n",
      "\n",
      "\n",
      "           0   1   2   3  total\n",
      "cluster                        \n",
      "0         93   2  14  57    166\n",
      "1        658   1   0  42    701\n",
      "2          0  51   2   0     53\n",
      "3          1  37  38   4     80\n"
     ]
    }
   ],
   "source": [
    "# Generating summary results for perturbed random centroids for k=4\n",
    "centroids = read_final_centroids()\n",
    "results = results_data_structure()\n",
    "\n",
    "# Looping through Input data\n",
    "for c, d in input_data_generator():\n",
    "    assigned_centroid = closest_centroid(centroids, d)\n",
    "    results.loc[assigned_centroid, 'total'] += 1\n",
    "    results.loc[assigned_centroid, int(c)] += 1\n",
    "\n",
    "print \">> Cluster Summary\"\n",
    "print \">> Rows: Assigned Cluster; Columns: Actual Classes\"\n",
    "print \"\\n\"\n",
    "print results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##4.5(D)\n",
    "###K=4 \"trained\" centroids, determined by the sums across the classes\n",
    "It follows the following steps:\n",
    "- Generate Centroids by using helper function described above\n",
    "- Write centroids into file\n",
    "- Run MapReduce job with an upper limit of 1000 iterations\n",
    "- Prints number of members of each cluster after each iteration\n",
    "- Stops after the stopping criteria (described above) threshold of 0.001 is breached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed Centroids:\n",
      "[[  1.28071303e-02   4.74992198e-02   2.60213372e-02 ...,   0.00000000e+00\n",
      "    3.21055688e-04   3.11719199e-04]\n",
      " [  1.08473360e-01   2.49464165e-03   1.02660496e-02 ...,   0.00000000e+00\n",
      "    9.71699557e-06   2.02218556e-05]\n",
      " [  6.54587536e-02   4.55253175e-03   2.03868542e-02 ...,   1.26701038e-03\n",
      "    2.13373254e-06   6.40119762e-06]\n",
      " [  3.15310774e-02   4.23890795e-02   1.81846417e-02 ...,   0.00000000e+00\n",
      "    7.84106068e-05   1.05735515e-04]]\n",
      ">> Iteration: 1\n",
      ">> (Assigned Cluster, Number of Obserations)\n",
      "(0, 787)\n",
      "(1, 61)\n",
      "(2, 82)\n",
      "(3, 70)\n",
      ">> Iteration: 2\n",
      ">> (Assigned Cluster, Number of Obserations)\n",
      "(0, 796)\n",
      "(1, 54)\n",
      "(2, 85)\n",
      "(3, 65)\n",
      ">> Iteration: 3\n",
      ">> (Assigned Cluster, Number of Obserations)\n",
      "(0, 802)\n",
      "(1, 51)\n",
      "(2, 84)\n",
      "(3, 63)\n",
      ">> Iteration: 4\n",
      ">> (Assigned Cluster, Number of Obserations)\n",
      "(0, 804)\n",
      "(1, 51)\n",
      "(2, 82)\n",
      "(3, 63)\n",
      ">> Iteration: 5\n",
      ">> (Assigned Cluster, Number of Obserations)\n",
      "(0, 804)\n",
      "(1, 51)\n",
      "(2, 82)\n",
      "(3, 63)\n",
      "Stopping Criteria Reached\n"
     ]
    }
   ],
   "source": [
    "# MapReduce for trainined centroids\n",
    "\n",
    "import imp\n",
    "from itertools import chain\n",
    "import kmeans_twitter\n",
    "imp.reload(kmeans_twitter)\n",
    "\n",
    "input_file_name = 'topUsers_Apr-Jul_2014_1000-words.txt'\n",
    "mr_job = kmeans_twitter.kmeans(args=[input_file_name])\n",
    "\n",
    "# Getting Centroids\n",
    "centroids = get_trainined_centroids()\n",
    "print \"Seed Centroids:\"\n",
    "print centroids\n",
    "\n",
    "# Write Centroid to file\n",
    "write_to_centroids_to_file(centroids)\n",
    "\n",
    "# Running MapReduce twice with overall iteration limit set to 1000\n",
    "for i in range(1000):\n",
    "    print \">> Iteration:\", i+1\n",
    "    print \">> (Assigned Cluster, Number of Obserations)\"\n",
    "    old_centroids = read_centroids_file()\n",
    "    # Running MapReduce job\n",
    "    with mr_job.make_runner() as runner:\n",
    "        runner.run()\n",
    "        # Capturing MapReduce job output and printing to console\n",
    "        for line in runner.stream_output():\n",
    "            print mr_job.parse_output_line(line)\n",
    "    new_centroids = read_centroids_file()\n",
    "    if stop_criterion(old_centroids, new_centroids, 0.001):\n",
    "        print \"Stopping Criteria Reached\"\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The code below generates the class break-up per cluster\n",
    "The rows are the clusters as assigned by Kmeans.  \n",
    "The columns are the number of observations per label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Cluster Summary\n",
      ">> Rows: Assigned Cluster; Columns: Actual Classes\n",
      "\n",
      "\n",
      "           0   1   2   3  total\n",
      "cluster                        \n",
      "0        749   3  14  38    804\n",
      "1          0  51   0   0     51\n",
      "2          1  37  40   4     82\n",
      "3          2   0   0  61     63\n"
     ]
    }
   ],
   "source": [
    "# Generating summary results for trained centroids\n",
    "centroids = read_final_centroids()\n",
    "results = results_data_structure()\n",
    "\n",
    "# Looping through Input data\n",
    "for c, d in input_data_generator():\n",
    "    assigned_centroid = closest_centroid(centroids, d)\n",
    "    results.loc[assigned_centroid, 'total'] += 1\n",
    "    results.loc[assigned_centroid, int(c)] += 1\n",
    "\n",
    "print \">> Cluster Summary\"\n",
    "print \">> Rows: Assigned Cluster; Columns: Actual Classes\"\n",
    "print \"\\n\"\n",
    "print results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
