{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#DATASCI W261: Machine Learning at Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Vineet Gangwar  \n",
    "**vineet.gangwar@gmail.com  \n",
    "W261-2: Machine Learning at Scale  \n",
    "Assignment #3  \n",
    "Date: Sep - 22 - 2015  **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#HW3.0\n",
    "What is a merge sort?  \n",
    "  \n",
    "**Answer**  \n",
    "Merge sort is a sorting algorithm with big O of n log n. This algorithm specifies how to sort 2 sorted lists. The idea is to compare the first element of each list. The smaller element of the two is poped from the list and is pushed into a new list. The same process is repeated until there are no elements left in the original two sorted lists. In general, when sorting an unsorted list, the list is broken into as many lists as there are elements so each list is of length one. This means that the lists of 1 elements are sorted. Then merge sort repeatedly applies the logic mentioned above until there is just one list remaining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where is it used in Hadoop?  \n",
    "  \n",
    "In hadoop, merge sort is used in the shuffle and sort phase where in the data is merge sorted before it is presented to the reducer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How is  a combiner function in the context of Hadoop?  \n",
    "  \n",
    "A combiner function is a mini-reduce step that runs on each mapper node. It can also run at the reducer side before the reduce step. It is an optional optimization step that combines mapper output to reduce the network load. The reducer function needs to be associative and commutative. Also, the mapper output, combiner input, combiner output and reducer input signatures need to be the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give an example where it can be used and justify why it should be used in the context of this problem.  \n",
    "  \n",
    "An example where a combiner can be used is the word count problem. In the word count problem, the mapper emits a pair consisting a word and the number 1 for every word in the document. The framework then needs to send this data to the reducer by consuming network capacity. A combiner can reduce the network traffic by aggregating values by key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the Hadoop shuffle?  \n",
    "  \n",
    "Hadoop shuffle is set of tasks that the frame work does to move data from the mappers to the reducers. It includes hashing keys to reducers and sorting keys so that all key value pairs of the same key goes to the same reducer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the Apriori algorithm? Describe an example use in your domain of expertise. Define confidence and lift.  \n",
    "  \n",
    "Apriori algorithm is used for frequent itemset mining and learning association rules. It makes learning possible over extremely large combinatorial space by identifying frequent singletons and then extending them to larger and larger itemsets.  \n",
    "Confidence is associated with an association rule. An association rule is in the form of I -> j where I is a set of items and j is an item. For this rule, confidence is defined as ratio of support for I and j to the support for I. Support is the number of baskets an itemset appears in. For example, if 40% of all customers in a store buy orange juice and 30% of all customers of the store buys orange juice and milk, then the confidence of the rule orange juice -> milk is 30/40 = 0.75.  \n",
    "Lift is a measure that evaluates the quality of a rule. Lift is a ratio. In the above example, lift is Pr(orange juice and milk | orange juice) / Pr(milk) = 0.75 / Pr(milk). So if 90% of all customers of the store buys milk, then lift = 0.75/.9 = 0.83. This means that of the sub-population who buys orange juice, the probability of buying milk is less that the general population. So rules with left less than 1 are bad rules. Rules with lift equal to 1 means that the rule is no better than randon chance. Rules with lift more than 1 are good rules.  \n",
    "In my domain of IT infrasturcture monitoring, it helps if one knows which IT components are dependent on each other. These components can then be managed as a unit. Apriori can be used to automatically find out frequent itemsets/IT components. Utilization (CPU, Memory, I/O etc) is monitored for all components. A basket can be considered as those components that cross a threshold simultaneously. Example, all machines whose CPU utlization is simulatenously over 90% can be considered as a basket. Frequent itemsets can be thought of being dependent on each other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#HW3.1. \n",
    "Product Recommendations: The action or practice of selling additional products or services \n",
    "to existing customers is called cross-selling. Giving product recommendation is \n",
    "one of the examples of cross-selling that are frequently used by online retailers. \n",
    "One simple method to give product recommendations is to recommend products that are frequently\n",
    "browsed together by the customers.\n",
    "\n",
    "Suppose we want to recommend new products to the customer based on the products they\n",
    "have already browsed on the online website. Write a program using the A-priori algorithm\n",
    "to find products which are frequently browsed together. Fix the support to s = 100 \n",
    "(i.e. product pairs need to occur together at least 100 times to be considered frequent) \n",
    "and find itemsets of size 2 and 3.\n",
    "\n",
    "Use the online browsing behavior dataset at: \n",
    "\n",
    "https://www.dropbox.com/s/zlfyiwa70poqg74/ProductPurchaseData.txt?dl=0\n",
    "\n",
    "Each line in this dataset represents a browsing session of a customer. \n",
    "On each line, each string of 8 characters represents the id of an item browsed during that session. \n",
    "The items are separated by spaces.\n",
    "\n",
    "Do some exploratory data analysis of this dataset. \n",
    "Report your findings such as number of unique products; largest basket, etc. using Hadoop Map-Reduce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**  \n",
    "Here are the findings:  \n",
    "- Number of unique items: 12592\t\n",
    "- Number of baskets: 31101\t\n",
    "- Largest basket: 37\t\n",
    "- Mean number of items per basket: 12\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mapper**  \n",
    "For each basket, the mapper generates a basket_id. It emits a pair containing the basket id and each uique item in the basket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/home/vineetgangwar/anaconda/bin/python\n",
    "import sys\n",
    "\n",
    "basket_id = 0\n",
    "for basket in sys.stdin:\n",
    "    basket_id += 1\n",
    "    items = basket.strip().split()\n",
    "    for item in items:\n",
    "        print \"{0},{1}\\t1\".format(basket_id, item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reducer**  \n",
    "The reducer creates two dictionaries.  \n",
    "- 1st contains the distribution of the individual items  \n",
    "- 2nd contains the key value of basket id and number of items per basket  \n",
    "\n",
    "Using the above two dictionaries, the job outputs:\n",
    "\n",
    "- Number of unique items\n",
    "- Number of baskets\n",
    "- Size of largest basket\n",
    "- Mean of items per basket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/home/vineetgangwar/anaconda/bin/python\n",
    "import sys\n",
    "\n",
    "items_dict = dict()\n",
    "baskets_dict = dict()\n",
    "for line in sys.stdin:\n",
    "    line = line.strip().split()\n",
    "    key = line[0]\n",
    "    value = int(line[1])\n",
    "    \n",
    "    # Obtaining Basket_id, Item_id, count\n",
    "    key = key.split(',')\n",
    "    basket_id = key[0]\n",
    "    item_id = key[1]\n",
    "    \n",
    "    # Adding to items dictionary\n",
    "    if item_id in items_dict:\n",
    "        items_dict[item_id] += value\n",
    "    else:\n",
    "        items_dict[item_id] = value\n",
    "        \n",
    "    # Adding to baskets dictionary\n",
    "    if basket_id in baskets_dict:\n",
    "        baskets_dict[basket_id] += value\n",
    "    else:\n",
    "        baskets_dict[basket_id] = value\n",
    "            \n",
    "# Printing exploratory statistics\n",
    "print \"Number of unique items:\", len(items_dict)\n",
    "print \"Number of baskets:\", len(baskets_dict)\n",
    "print \"Largest basket:\", sorted(baskets_dict.values())[-1]\n",
    "print \"Mean number of items per basket:\", sum(baskets_dict.values()) / len(baskets_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Providing execute permission  \n",
    "Cleaning HDFS  \n",
    "Creating input folder  \n",
    "Copying input file  \n",
    "Executing MapReduce job  \n",
    "Displaying output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/22 10:21:27 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/22 10:21:28 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /input\n",
      "15/09/22 10:21:29 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/22 10:21:29 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /output\n",
      "15/09/22 10:21:30 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/22 10:21:32 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/22 10:21:34 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/22 10:21:35 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "15/09/22 10:21:35 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "15/09/22 10:21:35 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "15/09/22 10:21:35 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "15/09/22 10:21:35 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "15/09/22 10:21:35 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1301456247_0001\n",
      "15/09/22 10:21:35 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "15/09/22 10:21:35 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "15/09/22 10:21:35 INFO mapreduce.Job: Running job: job_local1301456247_0001\n",
      "15/09/22 10:21:35 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "15/09/22 10:21:35 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "15/09/22 10:21:35 INFO mapred.LocalJobRunner: Starting task: attempt_local1301456247_0001_m_000000_0\n",
      "15/09/22 10:21:35 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/09/22 10:21:35 INFO mapred.MapTask: Processing split: hdfs://localhost:54310/input/ProductPurchaseData.txt:0+3458517\n",
      "15/09/22 10:21:35 INFO mapred.MapTask: numReduceTasks: 1\n",
      "15/09/22 10:21:36 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "15/09/22 10:21:36 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "15/09/22 10:21:36 INFO mapred.MapTask: soft limit at 83886080\n",
      "15/09/22 10:21:36 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "15/09/22 10:21:36 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "15/09/22 10:21:36 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "15/09/22 10:21:36 INFO streaming.PipeMapRed: PipeMapRed exec [/home/hadoop/w3/./mapper.py]\n",
      "15/09/22 10:21:36 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "15/09/22 10:21:36 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "15/09/22 10:21:36 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "15/09/22 10:21:36 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "15/09/22 10:21:36 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "15/09/22 10:21:36 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "15/09/22 10:21:36 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "15/09/22 10:21:36 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "15/09/22 10:21:36 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "15/09/22 10:21:36 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "15/09/22 10:21:36 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "15/09/22 10:21:36 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "15/09/22 10:21:36 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 10:21:36 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 10:21:36 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 10:21:36 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 10:21:36 INFO streaming.PipeMapRed: Records R/W=1216/1\n",
      "15/09/22 10:21:36 INFO streaming.PipeMapRed: R/W/S=10000/122556/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 10:21:36 INFO mapreduce.Job: Job job_local1301456247_0001 running in uber mode : false\n",
      "15/09/22 10:21:36 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "15/09/22 10:21:36 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/22 10:21:36 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/22 10:21:36 INFO mapred.LocalJobRunner: \n",
      "15/09/22 10:21:36 INFO mapred.MapTask: Starting flush of map output\n",
      "15/09/22 10:21:36 INFO mapred.MapTask: Spilling map output\n",
      "15/09/22 10:21:36 INFO mapred.MapTask: bufstart = 0; bufend = 6327067; bufvoid = 104857600\n",
      "15/09/22 10:21:36 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 24691104(98764416); length = 1523293/6553600\n",
      "15/09/22 10:21:37 INFO mapred.MapTask: Finished spill 0\n",
      "15/09/22 10:21:37 INFO mapred.Task: Task:attempt_local1301456247_0001_m_000000_0 is done. And is in the process of committing\n",
      "15/09/22 10:21:37 INFO mapred.LocalJobRunner: Records R/W=1216/1\n",
      "15/09/22 10:21:37 INFO mapred.Task: Task 'attempt_local1301456247_0001_m_000000_0' done.\n",
      "15/09/22 10:21:37 INFO mapred.LocalJobRunner: Finishing task: attempt_local1301456247_0001_m_000000_0\n",
      "15/09/22 10:21:37 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "15/09/22 10:21:37 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "15/09/22 10:21:37 INFO mapred.LocalJobRunner: Starting task: attempt_local1301456247_0001_r_000000_0\n",
      "15/09/22 10:21:37 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/09/22 10:21:37 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@650f4afe\n",
      "15/09/22 10:21:37 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=333971456, maxSingleShuffleLimit=83492864, mergeThreshold=220421168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "15/09/22 10:21:37 INFO reduce.EventFetcher: attempt_local1301456247_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "15/09/22 10:21:37 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1301456247_0001_m_000000_0 decomp: 7088717 len: 7088721 to MEMORY\n",
      "15/09/22 10:21:37 INFO reduce.InMemoryMapOutput: Read 7088717 bytes from map-output for attempt_local1301456247_0001_m_000000_0\n",
      "15/09/22 10:21:37 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 7088717, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->7088717\n",
      "15/09/22 10:21:37 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "15/09/22 10:21:37 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/22 10:21:37 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "15/09/22 10:21:37 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/22 10:21:37 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 7088704 bytes\n",
      "15/09/22 10:21:37 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "15/09/22 10:21:37 INFO reduce.MergeManagerImpl: Merged 1 segments, 7088717 bytes to disk to satisfy reduce memory limit\n",
      "15/09/22 10:21:37 INFO reduce.MergeManagerImpl: Merging 1 files, 7088721 bytes from disk\n",
      "15/09/22 10:21:37 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "15/09/22 10:21:37 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/22 10:21:37 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 7088704 bytes\n",
      "15/09/22 10:21:37 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/22 10:21:37 INFO streaming.PipeMapRed: PipeMapRed exec [/home/hadoop/w3/./reducer.py]\n",
      "15/09/22 10:21:37 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "15/09/22 10:21:37 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "15/09/22 10:21:37 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 10:21:37 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 10:21:37 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 10:21:37 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 10:21:37 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 10:21:38 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 10:21:38 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 10:21:38 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 10:21:38 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/22 10:21:38 INFO streaming.PipeMapRed: Records R/W=380824/1\n",
      "15/09/22 10:21:38 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/22 10:21:38 INFO mapred.Task: Task:attempt_local1301456247_0001_r_000000_0 is done. And is in the process of committing\n",
      "15/09/22 10:21:38 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/22 10:21:38 INFO mapred.Task: Task attempt_local1301456247_0001_r_000000_0 is allowed to commit now\n",
      "15/09/22 10:21:38 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1301456247_0001_r_000000_0' to hdfs://localhost:54310/output/_temporary/0/task_local1301456247_0001_r_000000\n",
      "15/09/22 10:21:38 INFO mapred.LocalJobRunner: Records R/W=380824/1 > reduce\n",
      "15/09/22 10:21:38 INFO mapred.Task: Task 'attempt_local1301456247_0001_r_000000_0' done.\n",
      "15/09/22 10:21:38 INFO mapred.LocalJobRunner: Finishing task: attempt_local1301456247_0001_r_000000_0\n",
      "15/09/22 10:21:38 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "15/09/22 10:21:39 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "15/09/22 10:21:39 INFO mapreduce.Job: Job job_local1301456247_0001 completed successfully\n",
      "15/09/22 10:21:39 INFO mapreduce.Job: Counters: 38\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=14387744\n",
      "\t\tFILE: Number of bytes written=21988665\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=6917034\n",
      "\t\tHDFS: Number of bytes written=114\n",
      "\t\tHDFS: Number of read operations=15\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=380824\n",
      "\t\tMap output bytes=6327067\n",
      "\t\tMap output materialized bytes=7088721\n",
      "\t\tInput split bytes=104\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=380821\n",
      "\t\tReduce shuffle bytes=7088721\n",
      "\t\tReduce input records=380824\n",
      "\t\tReduce output records=4\n",
      "\t\tSpilled Records=761648\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=53\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=571473920\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3458517\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=114\n",
      "15/09/22 10:21:39 INFO streaming.StreamJob: Output directory: /output/\n",
      "15/09/22 10:21:40 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Number of unique items: 12592\t\n",
      "Number of baskets: 31101\t\n",
      "Largest basket: 37\t\n",
      "Mean number of items per basket: 12\t\n"
     ]
    }
   ],
   "source": [
    "# Setting execute permissions for mapper and reducer\n",
    "!chmod 755 mapper.py\n",
    "!chmod 755 reducer.py\n",
    "\n",
    "# Deleting folders from HDFS\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rm -r /input\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rm -r /output\n",
    "\n",
    "# Creating folder in HDFS\n",
    "!/usr/local/hadoop/bin/hdfs dfs -mkdir -p /input\n",
    "\n",
    "# Copying input file for the job into HDFS\n",
    "!/usr/local/hadoop/bin/hdfs dfs -put ProductPurchaseData.txt /input/\n",
    "\n",
    "# Running MapReduce job\n",
    "!/usr/local/hadoop/bin/hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.6.0.jar -mapper mapper.py -reducer reducer.py -input /input/* -output /output/\n",
    "\n",
    "#Reading output into a variable\n",
    "job_output = !/usr/local/hadoop/bin/hdfs dfs -cat /output/part*\n",
    "\n",
    "# Printing MapReduce output\n",
    "for line in job_output:\n",
    "    print line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#HW3.2\n",
    "(Computationally prohibitive but then again Hadoop can handle this)\n",
    "\n",
    "Note: for this part the writeup will require a specific rule ordering but the program need not sort the output.\n",
    "\n",
    "List the top 5 rules with corresponding confidence scores in decreasing order of confidence score \n",
    "for frequent (100>count) itemsets of size 2. \n",
    "A rule is of the form: \n",
    "\n",
    "(item1) ⇒ item2.\n",
    "\n",
    "Fix the ordering of the rule lexicographically (left to right), \n",
    "and break ties in confidence (between rules, if any exist) \n",
    "by taking the first ones in lexicographically increasing order. \n",
    "Use Hadoop MapReduce to complete this part of the assignment; \n",
    "use a single mapper and single reducer; use a combiner if you think it will help and justify. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**  \n",
    "Top 5 rules: (This matches exactly with Christian Borgelt Apriori implementation in HW 3.3)  \n",
    "  \n",
    "Rule\t\tConfidence Value\tSupport(X u Y)\tSupport(Y)  \n",
    "(DAI93865)->FRO40251\t1.0\t208\t208  \n",
    "(GRO85051)->FRO40251\t0.999176276771\t1213\t1214  \n",
    "(GRO38636)->FRO40251\t0.990654205607\t106\t107  \n",
    "(ELE12951)->FRO40251\t0.990566037736\t105\t106  \n",
    "(DAI88079)->FRO40251\t0.986725663717\t446\t452  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mapper**  \n",
    "This mapper uses the stripes pattern.  \n",
    "- For every basket, it uses unique items in the basket to create pairs\n",
    "- It then creates stripes that is a dictionary of key value pairs\n",
    "- Key is an item\n",
    "- Value is a dictionary of items and items counts\n",
    "- In the value dictionary the mapper adds another key called Singleton. This value contains the support for the main item   \n",
    "e.g.  \n",
    "{  \n",
    "  FRO11987: {ELE17451: 10, ELE89019: 20, SNA90258: 25, GRO99222: 90, singleton: 150},  \n",
    "  ELE89019: {ELE17451: 1, ELE89019:5, SNA90258: 70, singleton: 124}  \n",
    "}  \n",
    "  \n",
    "Once the stripes structure is complete, the mapper emits the key/value pairs in the stripes dictionary.  \n",
    "Note: Every pair is added twice to the stripes data structure - for both item1 and item2  \n",
    "  \n",
    "e.g. output  \n",
    "FRO11987\\t{ELE17451: 10, ELE89019: 20, SNA90258: 25, GRO99222: 90, singleton: 150}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/home/vineetgangwar/anaconda/bin/python\n",
    "import itertools\n",
    "import operator\n",
    "import sys\n",
    "\n",
    "singleton = dict()\n",
    "stripes = dict()\n",
    "\n",
    "def add_to_stripes(pair):\n",
    "    item1, item2 = pair\n",
    "    # Every pair is added twice to the stripes data structure - for both item1 and item2\n",
    "    if item1 not in stripes:\n",
    "        temp_dict = {item2: 1}\n",
    "        stripes[item1] = temp_dict\n",
    "    else:\n",
    "        temp_dict = stripes[item1]\n",
    "        if item2 not in temp_dict:\n",
    "            temp_dict[item2] = 1\n",
    "        else:\n",
    "            temp_dict[item2] += 1\n",
    "        stripes[item1] = temp_dict\n",
    "        \n",
    "    if item2 not in stripes:\n",
    "        temp_dict = {item1: 1}\n",
    "        stripes[item2] = temp_dict\n",
    "    else:\n",
    "        temp_dict = stripes[item2]\n",
    "        if item1 not in temp_dict:\n",
    "            temp_dict[item1] = 1\n",
    "        else:\n",
    "            temp_dict[item1] += 1\n",
    "\n",
    "            \n",
    "for basket in sys.stdin:\n",
    "    items = basket.strip().split()\n",
    "    unique_items = set(items)\n",
    "    \n",
    "    # Creating pairs\n",
    "    pairs = itertools.combinations(unique_items, 2)\n",
    "    for pair in pairs:\n",
    "        add_to_stripes(pair)\n",
    "        \n",
    "    # Creating singletons\n",
    "    for item in unique_items:\n",
    "        if item in singleton:\n",
    "            singleton[item] += 1\n",
    "        else:\n",
    "            singleton[item] = 1\n",
    "            \n",
    "# Printing output of mapper\n",
    "for key, value in stripes.iteritems():\n",
    "    value['singleton'] = singleton[key]\n",
    "    print \"{0}\\t{1}\".format(key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combiner**  \n",
    "The combiner merges outputs of each line output by the mappers. It is possible that multiple mappers are running in the same node with the following output:\n",
    "Output of mapper 1:  \n",
    "FRO11987\\t{ELE17451: 10, ELE89019: 20, SNA90258: 25, GRO99222: 90, singleton: 150}  \n",
    "Output of mapper 2:  \n",
    "FRO11987\\t{ELE17451: 10, ELE89019: 20, SNA90258: 25, GRO99222: 90, singleton: 150}  \n",
    "So instead of sending these two over the network, the combiner produces a single line:  \n",
    "FRO11987\\t{ELE17451: 20, ELE89019: 40, SNA90258: 50, GRO99222: 180, singleton: 300}  \n",
    "  \n",
    "The combiner uses the loads method of json module to convert the input value into a dictionary  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting combiner.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile combiner.py\n",
    "#!/home/vineetgangwar/anaconda/bin/python\n",
    "import sys\n",
    "import json\n",
    "\n",
    "stripes = dict()\n",
    "\n",
    "def merge_stripe_create_stripes(stripe):\n",
    "    fields = stripe.strip().split('\\t')\n",
    "    key = fields[0]\n",
    "    value = fields[1]\n",
    "    value = value.replace(\"'\", '\"')\n",
    "    # Convert value into a dictionary object\n",
    "    value = json.loads(value)\n",
    "    \n",
    "    if key not in stripes:\n",
    "        stripes[key] = value\n",
    "    else:\n",
    "        temp_dict = stripes[key]\n",
    "        for k, v in value.iteritems():\n",
    "            if k not in temp_dict:\n",
    "                temp_dict[k] = v\n",
    "            else:\n",
    "                temp_dict[k] += v\n",
    "        stripes[key] = temp_dict\n",
    "\n",
    "# Main Combiner code\n",
    "for stripe in sys.stdin:\n",
    "    # Creating merged stripes dictionary\n",
    "    merge_stripe_create_stripes(stripe)\n",
    "\n",
    "# Printing combiner output\n",
    "for key, value in stripes.iteritems():\n",
    "    expanded_value = \" \".join([k+':' + str(v) for k, v in value.iteritems()])\n",
    "    print \"{0}\\t{1}\".format(key, json.dumps(value))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reducer**  \n",
    "The reducer first merges all stripes  \n",
    "It then creates a dictionary whose key is a rule and the value is the association rule  \n",
    "- 1st for each item is drops all pairs whose support is less than 100\n",
    "- 2nd it gets the support for item 1 from the singleton key\n",
    "- It then calculates the confidence  \n",
    "\n",
    "It then sorts the confidence dictionary. It there is a tie between rules then the rule is sorted lexicographically. This is achieved in a two step sorting using the sorted function and custom sorting.  \n",
    "After sorting, the reducer outputs the top 5 rules \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/home/vineetgangwar/anaconda/bin/python\n",
    "import sys\n",
    "import json\n",
    "import operator\n",
    "\n",
    "stripes = dict()\n",
    "\n",
    "def merge_stripe_create_stripes(stripe):\n",
    "    fields = stripe.strip().split('\\t')\n",
    "    key = fields[0]\n",
    "    value = fields[1]\n",
    "    value = value.replace(\"'\", '\"')\n",
    "    # Convert value into a dictionary object\n",
    "    value = json.loads(value)\n",
    "    \n",
    "    if key not in stripes:\n",
    "        stripes[key] = value\n",
    "    else:\n",
    "        temp_dict = stripes[key]\n",
    "        for k, v in value.iteritems():\n",
    "            if k not in temp_dict:\n",
    "                temp_dict[k] = v\n",
    "            else:\n",
    "                temp_dict[k] += v\n",
    "        stripes[key] = temp_dict\n",
    "\n",
    "# Main Reducer code\n",
    "for stripe in sys.stdin:\n",
    "    # Creating merged stripes dictionary\n",
    "    merge_stripe_create_stripes(stripe)\n",
    "\n",
    "confidence = list()    \n",
    "\n",
    "for item, value in stripes.iteritems():\n",
    "    frequent_pairs = {k:v for k, v in value.iteritems() if v >= 100 if k != 'singleton'}\n",
    "    # print item, value\n",
    "    if len(frequent_pairs) > 0:\n",
    "        singleton_support = value['singleton']\n",
    "        # print confidence\n",
    "        for key_f, value_f in frequent_pairs.iteritems():\n",
    "            key_confidence = \"({0})->{1}\".format(item, key_f)\n",
    "            value_confidence = float(value_f)/singleton_support\n",
    "            confidence.append((key_confidence, value_confidence, value_f, singleton_support))\n",
    "        # print item, frequent_pairs, len(frequent_pairs), all_pair_count\n",
    "\n",
    "# Sorting stage1: Sorting my (item1)-> item2. This sorting will be preserved in the next stage\n",
    "sort_stage1 = sorted(confidence, key=operator.itemgetter(0))\n",
    "sort_stage2 = sorted(sort_stage1, key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "# Printing Reducer output\n",
    "print \"Rule\\t\\tConfidence Value\\tSupport(X u Y)\\tSupport(Y)\"\n",
    "for rule, confidence_values, value_f, singleton_support in sort_stage2[:5]:\n",
    "    print \"{0}\\t{1}\\t{2}\\t{3}\".format(rule, confidence_values, value_f, singleton_support)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Providing execute permission  \n",
    "Cleaning HDFS  \n",
    "Creating input folder  \n",
    "Copying input file  \n",
    "Executing MapReduce job  \n",
    "Displaying output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/22 10:23:48 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/22 10:23:49 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /input\n",
      "15/09/22 10:23:50 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/22 10:23:50 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /output\n",
      "15/09/22 10:23:52 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/22 10:23:53 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/22 10:23:55 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/22 10:23:56 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "15/09/22 10:23:56 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "15/09/22 10:23:56 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "15/09/22 10:23:56 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "15/09/22 10:23:56 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "15/09/22 10:23:56 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local821721418_0001\n",
      "15/09/22 10:23:56 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "15/09/22 10:23:56 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "15/09/22 10:23:56 INFO mapreduce.Job: Running job: job_local821721418_0001\n",
      "15/09/22 10:23:56 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "15/09/22 10:23:56 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "15/09/22 10:23:56 INFO mapred.LocalJobRunner: Starting task: attempt_local821721418_0001_m_000000_0\n",
      "15/09/22 10:23:56 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/09/22 10:23:56 INFO mapred.MapTask: Processing split: hdfs://localhost:54310/input/ProductPurchaseData.txt:0+3458517\n",
      "15/09/22 10:23:56 INFO mapred.MapTask: numReduceTasks: 1\n",
      "15/09/22 10:23:56 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "15/09/22 10:23:56 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "15/09/22 10:23:56 INFO mapred.MapTask: soft limit at 83886080\n",
      "15/09/22 10:23:56 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "15/09/22 10:23:56 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "15/09/22 10:23:56 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "15/09/22 10:23:56 INFO streaming.PipeMapRed: PipeMapRed exec [/home/hadoop/w3/./mapper.py]\n",
      "15/09/22 10:23:56 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "15/09/22 10:23:56 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "15/09/22 10:23:56 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "15/09/22 10:23:56 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "15/09/22 10:23:56 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "15/09/22 10:23:56 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "15/09/22 10:23:56 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "15/09/22 10:23:56 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "15/09/22 10:23:56 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "15/09/22 10:23:56 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "15/09/22 10:23:56 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "15/09/22 10:23:56 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "15/09/22 10:23:57 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 10:23:57 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 10:23:57 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 10:23:57 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 10:23:57 INFO mapreduce.Job: Job job_local821721418_0001 running in uber mode : false\n",
      "15/09/22 10:23:57 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "15/09/22 10:23:57 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:10000=10000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "15/09/22 10:23:59 INFO streaming.PipeMapRed: Records R/W=31101/1\n",
      "15/09/22 10:24:00 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/22 10:24:00 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/22 10:24:00 INFO mapred.LocalJobRunner: \n",
      "15/09/22 10:24:00 INFO mapred.MapTask: Starting flush of map output\n",
      "15/09/22 10:24:00 INFO mapred.MapTask: Spilling map output\n",
      "15/09/22 10:24:00 INFO mapred.MapTask: bufstart = 0; bufend = 26744263; bufvoid = 104857600\n",
      "15/09/22 10:24:00 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26164032(104656128); length = 50365/6553600\n",
      "15/09/22 10:24:00 INFO streaming.PipeMapRed: PipeMapRed exec [/home/hadoop/w3/./combiner.py]\n",
      "15/09/22 10:24:00 INFO Configuration.deprecation: mapred.skip.map.auto.incr.proc.count is deprecated. Instead, use mapreduce.map.skip.proc-count.auto-incr\n",
      "15/09/22 10:24:00 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 10:24:00 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 10:24:00 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 10:24:00 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 10:24:01 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:10000=10000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "15/09/22 10:24:02 INFO streaming.PipeMapRed: Records R/W=12592/1\n",
      "15/09/22 10:24:02 INFO mapred.LocalJobRunner: Records R/W=12592/1 > sort\n",
      "15/09/22 10:24:03 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "15/09/22 10:24:04 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/22 10:24:04 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/22 10:24:04 INFO mapred.MapTask: Finished spill 0\n",
      "15/09/22 10:24:04 INFO mapred.Task: Task:attempt_local821721418_0001_m_000000_0 is done. And is in the process of committing\n",
      "15/09/22 10:24:04 INFO mapred.LocalJobRunner: Records R/W=12592/1\n",
      "15/09/22 10:24:04 INFO mapred.Task: Task 'attempt_local821721418_0001_m_000000_0' done.\n",
      "15/09/22 10:24:04 INFO mapred.LocalJobRunner: Finishing task: attempt_local821721418_0001_m_000000_0\n",
      "15/09/22 10:24:04 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "15/09/22 10:24:04 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "15/09/22 10:24:04 INFO mapred.LocalJobRunner: Starting task: attempt_local821721418_0001_r_000000_0\n",
      "15/09/22 10:24:04 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/09/22 10:24:04 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@49643715\n",
      "15/09/22 10:24:04 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=333971456, maxSingleShuffleLimit=83492864, mergeThreshold=220421168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "15/09/22 10:24:04 INFO reduce.EventFetcher: attempt_local821721418_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "15/09/22 10:24:04 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local821721418_0001_m_000000_0 decomp: 26789854 len: 26789858 to MEMORY\n",
      "15/09/22 10:24:04 INFO reduce.InMemoryMapOutput: Read 26789854 bytes from map-output for attempt_local821721418_0001_m_000000_0\n",
      "15/09/22 10:24:04 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 26789854, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->26789854\n",
      "15/09/22 10:24:04 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "15/09/22 10:24:04 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/22 10:24:04 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "15/09/22 10:24:04 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/22 10:24:04 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 26789841 bytes\n",
      "15/09/22 10:24:04 INFO reduce.MergeManagerImpl: Merged 1 segments, 26789854 bytes to disk to satisfy reduce memory limit\n",
      "15/09/22 10:24:04 INFO reduce.MergeManagerImpl: Merging 1 files, 26789858 bytes from disk\n",
      "15/09/22 10:24:04 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "15/09/22 10:24:04 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/22 10:24:04 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 26789841 bytes\n",
      "15/09/22 10:24:04 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/22 10:24:04 INFO streaming.PipeMapRed: PipeMapRed exec [/home/hadoop/w3/./reducer.py]\n",
      "15/09/22 10:24:04 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "15/09/22 10:24:04 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "15/09/22 10:24:04 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 10:24:04 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 10:24:04 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 10:24:04 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 10:24:04 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "15/09/22 10:24:05 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 10:24:05 INFO streaming.PipeMapRed: Records R/W=12592/1\n",
      "15/09/22 10:24:05 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/22 10:24:05 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/22 10:24:05 INFO mapred.Task: Task:attempt_local821721418_0001_r_000000_0 is done. And is in the process of committing\n",
      "15/09/22 10:24:05 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/22 10:24:05 INFO mapred.Task: Task attempt_local821721418_0001_r_000000_0 is allowed to commit now\n",
      "15/09/22 10:24:05 INFO output.FileOutputCommitter: Saved output of task 'attempt_local821721418_0001_r_000000_0' to hdfs://localhost:54310/output/_temporary/0/task_local821721418_0001_r_000000\n",
      "15/09/22 10:24:05 INFO mapred.LocalJobRunner: Records R/W=12592/1 > reduce\n",
      "15/09/22 10:24:05 INFO mapred.Task: Task 'attempt_local821721418_0001_r_000000_0' done.\n",
      "15/09/22 10:24:05 INFO mapred.LocalJobRunner: Finishing task: attempt_local821721418_0001_r_000000_0\n",
      "15/09/22 10:24:05 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "15/09/22 10:24:06 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "15/09/22 10:24:06 INFO mapreduce.Job: Job job_local821721418_0001 completed successfully\n",
      "15/09/22 10:24:06 INFO mapreduce.Job: Counters: 38\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=53790018\n",
      "\t\tFILE: Number of bytes written=81090832\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=6917034\n",
      "\t\tHDFS: Number of bytes written=260\n",
      "\t\tHDFS: Number of read operations=15\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=12592\n",
      "\t\tMap output bytes=26744263\n",
      "\t\tMap output materialized bytes=26789858\n",
      "\t\tInput split bytes=104\n",
      "\t\tCombine input records=12592\n",
      "\t\tCombine output records=12592\n",
      "\t\tReduce input groups=12592\n",
      "\t\tReduce shuffle bytes=26789858\n",
      "\t\tReduce input records=12592\n",
      "\t\tReduce output records=6\n",
      "\t\tSpilled Records=25184\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=27\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=572522496\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3458517\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=260\n",
      "15/09/22 10:24:06 INFO streaming.StreamJob: Output directory: /output/\n",
      "15/09/22 10:24:07 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Rule\t\tConfidence Value\tSupport(X u Y)\tSupport(Y)\n",
      "(DAI93865)->FRO40251\t1.0\t208\t208\n",
      "(GRO85051)->FRO40251\t0.999176276771\t1213\t1214\n",
      "(GRO38636)->FRO40251\t0.990654205607\t106\t107\n",
      "(ELE12951)->FRO40251\t0.990566037736\t105\t106\n",
      "(DAI88079)->FRO40251\t0.986725663717\t446\t452\n"
     ]
    }
   ],
   "source": [
    "# Setting execute permissions for mapper and reducer\n",
    "!chmod 755 mapper.py\n",
    "!chmod 755 combiner.py\n",
    "!chmod 755 reducer.py\n",
    "\n",
    "# Deleting folders from HDFS\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rm -r /input\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rm -r /output\n",
    "\n",
    "# Creating folder in HDFS\n",
    "!/usr/local/hadoop/bin/hdfs dfs -mkdir -p /input\n",
    "\n",
    "# Copying input file for the job into HDFS\n",
    "!/usr/local/hadoop/bin/hdfs dfs -put ProductPurchaseData.txt /input/\n",
    "\n",
    "# Running MapReduce job\n",
    "!/usr/local/hadoop/bin/hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.6.0.jar -mapper mapper.py -combiner combiner.py -reducer reducer.py -input /input/* -output /output/\n",
    "\n",
    "#Reading output into a variable\n",
    "job_output = !/usr/local/hadoop/bin/hdfs dfs -cat /output/part*\n",
    "\n",
    "# Printing first 100 lines of MapReduce output\n",
    "for line in job_output:\n",
    "    print line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#HW3.3\n",
    "\n",
    "Benchmark your results using the pyFIM implementation of the Apriori algorithm\n",
    "(Apriori - Association Rule Induction / Frequent Item Set Mining implemented by Christian Borgelt). \n",
    "You can download pyFIM from here: \n",
    "\n",
    "http://www.borgelt.net/pyfim.html\n",
    "\n",
    "Comment on the results from both implementations (your Hadoop MapReduce of apriori versus pyFIM) \n",
    "in terms of results and execution times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**  \n",
    "Christian Borgelt's pyFIM is an extension module that implements some of the functionality of the **apriori** command-line tool. The command line tool is also written by Christian Borgelt. I am using the command line tool to benchmark my results of HW3.2  \n",
    "Command line options used:  \n",
    "- -tr  \n",
    "Return association rules\n",
    "- -s-100  \n",
    "Minimum support if an itemset is set to an absolute number of 100\n",
    "- -m2  \n",
    "Minimum number of items per set is set to 2\n",
    "- -n2  \n",
    "Maximum number of items per set is set to 2\n",
    "- -c1  \n",
    "Displays all rules with confidence greater than 1%\n",
    "- -v\" (%a %c %b)\"  \n",
    "Output format: %a is Absolute itemset support; %c is Rule confidence as a fraction; %b is Absolute body set support  \n",
    "  \n",
    "The output from Christian Borgelt's Apriori command exactly matches with my output in HW3.2  \n",
    "\n",
    "In the code below, I am re-formated the output of the command so that it matches mine in HW3.2 and then done sorting exactly as done in HW3.2 above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./apriori - find frequent item sets with the apriori algorithm\n",
      "version 6.19 (2015.08.18)        (c) 1996-2015   Christian Borgelt\n",
      "reading ProductPurchaseData.txt ... [12592 item(s), 31101 transaction(s)] done [0.04s].\n",
      "building transaction tree ... [40498 node(s)] done [0.00s].\n",
      "checking subsets of size 1 2 done [0.53s].\n",
      "writing apriori_output ... [186345 rule(s)] done [0.07s].\n",
      "all: 186345\n",
      "Rule\t\tConfidence Value\tSupport(X u Y)\tSupport(Y)\n",
      "DAI93865->FRO40251\t1\t208\t208\n",
      "GRO85051->FRO40251\t0.999176\t1213\t1214\n",
      "GRO38636->FRO40251\t0.990654\t106\t107\n",
      "ELE12951->FRO40251\t0.990566\t105\t106\n",
      "DAI88079->FRO40251\t0.986726\t446\t452\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "# Executing Apriori command line tool\n",
    "!./apriori -tr -s-100 -m2 -n2 -c1 -Z -v\" (%a %c %b)\" -eo ProductPurchaseData.txt apriori_output\n",
    "\n",
    "# Capturing output\n",
    "result = !cat apriori_output\n",
    "\n",
    "# Re-arranging output of Apriori to match HW3.2\n",
    "re_arranged_result = list()\n",
    "\n",
    "for line in result:\n",
    "    # Replaceing <-() with space\n",
    "    line = line.replace('<-', ' ')\n",
    "    line = line.replace('(', ' ')\n",
    "    line = line.replace(')', ' ')\n",
    "    # Splitting in fields\n",
    "    fields = line.split()\n",
    "    # Creating tuple appending\n",
    "    re_arranged_result.append((fields[1]+'->'+fields[0], fields[3], fields[2], fields[4]))\n",
    "    \n",
    "# Sorting re-arranged output as per HW3.2\n",
    "# Sorting stage1: Sorting my (item1)-> item2. This sorting will be preserved in the next stage\n",
    "sort_stage1 = sorted(re_arranged_result, key=operator.itemgetter(0))\n",
    "sort_stage2 = sorted(sort_stage1, key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "# Printing Reducer output\n",
    "print \"Rule\\t\\tConfidence Value\\tSupport(X u Y)\\tSupport(Y)\"\n",
    "for rule, confidence_values, value_f, all_pair_count in sort_stage2[:5]:\n",
    "    print \"{0}\\t{1}\\t{2}\\t{3}\".format(rule, confidence_values, value_f, all_pair_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#HW3.4\n",
    "\n",
    "(Conceptual Exercise)\n",
    "\n",
    "Suppose that you wished to perform the Apriori algorithm once again,\n",
    "though this time now with the goal of listing the top 5 rules with corresponding confidence scores \n",
    "in decreasing order of confidence score for itemsets of size 3 using Hadoop MapReduce.\n",
    "A rule is now of the form: \n",
    "\n",
    "(item1, item2) ⇒ item3 \n",
    "\n",
    "Recall that the Apriori algorithm is iterative for increasing itemset size,\n",
    "working off of the frequent itemsets of the previous size to explore \n",
    "ONLY the NECESSARY subset of a large combinatorial space. \n",
    "Describe how you might design a framework to perform this exercise.\n",
    "\n",
    "In particular, focus on the following:\n",
    "  - map-reduce steps required\n",
    "  - enumeration of item sets and filtering for frequent candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**  \n",
    "To solve this problem, multiple MapReduce jobs are required  \n",
    "  \n",
    "**Map Reduce Job 1:**   \n",
    "  \n",
    "*Mapper*  \n",
    "For every basket:  \n",
    "- Split the basket into unique items\n",
    "- Each unique item in the basket emit the item along with the digit 1\n",
    "\n",
    "*Reducer*  \n",
    "- Generate singleton dictionary, that is, support for each item  \n",
    "- Filter out singletons that do not qualify as frequent itemset\n",
    "Emit each item in the filtered singletons\n",
    "\n",
    "**Map Reduce Job2**:  \n",
    "  \n",
    "The output of the 1st MapReduce job should be available to all the mappers of the 2nd MapReduce job. During mapper initialize phase the mappers can read in the filtered singletons from a file into memory  \n",
    "  \n",
    "*Mapper*  \n",
    "For every basket:\n",
    "- Split the basket into unique items\n",
    "- Drop those items that are not present in the filtered singleton dictionary\n",
    "- Create combinations of pairs from the remaining items in the basket\n",
    "- Emit the pair along with the digit 1\n",
    "\n",
    "*Reducer*  \n",
    "- Generate pair dictionary, that is, support for each pair\n",
    "- Filter out pairs that do not qualify as frequent itemset\n",
    "Emit each item in the filtered pairs  \n",
    "  \n",
    "**Map Reduce Job3**:  \n",
    "\n",
    "The output of the 2nd MapReduce job should be available to all the mappers of the 3rd MapReduce job. During mapper initialize phase the mappers can read in the filtered pairs from a file into memory  \n",
    "  \n",
    "*Mapper*  \n",
    "For every basket:\n",
    "- Split the basket into unique items\n",
    "- Create combinations of pairs from the unique items in the basket\n",
    "- Drop those pairs that are not present in the filtered pairs dictionary\n",
    "- Create a list of unique items from the remaining pairs that are frequent items\n",
    "- Create triples from the remaining unique items\n",
    "- Emit the triple along with the digit 1\n",
    "\n",
    "*Reducer*  \n",
    "- Generate triples dictionary, that is, support for each triple\n",
    "- Filter out triples that do not qualify as frequent itemset\n",
    "\n",
    "Read in the filtered pairs diuctionary from an external file  \n",
    "- Calculate confidence for each (Item1, Item2) -> Item3 an store in a dictionary\n",
    "- Sort the dictionary and print out the top 5 rules  \n",
    "\n",
    "External read of singleton and pairs from a file should be ok because the size will not be too large  \n",
    "\n",
    "Note:  \n",
    "I think the above method will be efficient becuase in practise the frequent itemsets are not expected to be large. The support threshold is usually set at 1% of basket count"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
