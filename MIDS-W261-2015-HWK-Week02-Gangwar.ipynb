{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DATASCI W261: Machine Learning at Scale "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Vineet Gangwar\n",
    "**vineet.gangwar@gmail.com **  \n",
    "**W261-2: Machine Learning at Scale**  \n",
    "**Assignment #2**   \n",
    "**Date: Sep - 15 - 2015**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#HW2.0  \n",
    "- What is a race condition in the context of parallel computation? Give an example.\n",
    "- What is MapReduce?\n",
    "- How does it differ from Hadoop?\n",
    "- Which programming paradigm is Hadoop based on? Explain and give a simple example in code and show the code running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Race Condition**  \n",
    "A race condition in the context of parallel computation is a condition where the system gets into an unintended state because of multiple threads acting upon the same shared data. For example, a thread might want to check the current value of a variable and if the value is positive, the thread will calculate the log and store the log into the variable. However, between the check and act phase of the thread a second thread comes along changes the value to 0. Then the first thread will overwrite 0 with the log of the previous data. This is incorrect behaviour because the most current value of the variable is lost.  \n",
    "\n",
    "**MapReduce**  \n",
    "Map Reduce is a programming model which follows a divide and conquer strategy to parallely process embarrasingly parallel problems on a cluster of machines  \n",
    "\n",
    "**How does it differ from Hadoop**  \n",
    "Hadoop is a software platform that implements the Map Reduce programming paradigm. It also provides a filesystem that supports the Map Reduce implementation. So MapReduce is a model while Hadoop is an implementation of that model  \n",
    "\n",
    "**Which programming paradigm is Hadoop based on?**  \n",
    "Hadoop is based on the MapReduce programming model  \n",
    "\n",
    "**Explain and give a simple example in code and show the code running.**  \n",
    "To solve a problem in Hadoop the user needs to provide a map job and a reduce job. Hadoop splits the input data and passes the chunks to as many map tasks. It then implements a barrier. Once all the map tasks are complete, the framework distributes the keys to the reduce jobs based on a hash function. The framework also sorts the keys before they are presented to the reducer jobs. The reduce jobs work on the input data and their output constitutes the output of the entire MapReduce job.  \n",
    "**HW2.1** is an example of a running MapReduce code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#HW2.1: Sort in Hadoop MapReduce\n",
    "Given as input: Records of the form < integer, “NA” >, where integer is any integer, and “NA” is just the empty string.\n",
    "Output: sorted key value pairs of the form < integer, “NA” >; what happens if you have multiple reducers? Do you need additional steps? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below MapReduce code relies on the sorting feature of the Hadoop framework to achieve sorting. Hadoop sorts keys before it sends the output from the mappers to the reducers. In this case as I am using just one reducer I am ensuring that Hadoop sorts all the 10K keys. The (Identity) reducer just prints out the input it gets after a minor formating.  \n",
    "Hadoop by default does a text sort on the keys. I used the following to enable numeric sorting of the keys:  \n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator  -D  mapred.text.key.comparator.options=-n  \n",
    "I used the option -D mapred.reduce.tasks=1 to ensure only one reducer task. If multiple reducers are used then the output will be as many files with the keys randomly distributed amongst them. Even though the keys within each reducer's output file will be sorted by no sorting will be achived across the files.  \n",
    "To sort we will need another MapReduce with just one reducer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating 10000 random records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "n = 10000\n",
    "filename = 'random_records'\n",
    "\n",
    "# Generating 10000 numbers\n",
    "random_list = range(n)\n",
    "# Shuffling the numbers\n",
    "random.shuffle(random_list)\n",
    "\n",
    "# Writing out the file in the format <Integer, NA>\n",
    "filehndl = open(filename, 'w')\n",
    "for num in random_list:\n",
    "    filehndl.write('{0}, NA\\n'.format(num))\n",
    "filehndl.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mapper**  \n",
    "The mapper reads each line and convert the input format from < Interger, NA > to < Interger\\tNA >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/home/vineetgangwar/anaconda/bin/python\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "    fields = line.split(',')\n",
    "    fields = [field.strip() for field in fields]\n",
    "    print '{0}\\t{1}'.format(fields[0], fields[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reducer**  \n",
    "Writing reducer to filesystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/home/vineetgangwar/anaconda/bin/python\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "    fields = line.split('\\t')\n",
    "    fields = [field.strip() for field in fields]\n",
    "    print '{0}, {1}'.format(fields[0], fields[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod 755 mapper.py\n",
    "!chmod 755 reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving input file into HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/15 11:46:17 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/15 11:46:18 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /input\n",
      "15/09/15 11:46:19 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/15 11:46:19 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /output\n",
      "15/09/15 11:46:20 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/15 11:46:21 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/15 11:46:24 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/15 11:46:24 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "15/09/15 11:46:24 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "15/09/15 11:46:24 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "15/09/15 11:46:24 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "15/09/15 11:46:24 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "15/09/15 11:46:24 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "15/09/15 11:46:24 INFO Configuration.deprecation: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "15/09/15 11:46:24 INFO Configuration.deprecation: mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "15/09/15 11:46:25 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1334444185_0001\n",
      "15/09/15 11:46:25 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "15/09/15 11:46:25 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "15/09/15 11:46:25 INFO mapreduce.Job: Running job: job_local1334444185_0001\n",
      "15/09/15 11:46:25 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "15/09/15 11:46:25 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "15/09/15 11:46:25 INFO mapred.LocalJobRunner: Starting task: attempt_local1334444185_0001_m_000000_0\n",
      "15/09/15 11:46:25 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/09/15 11:46:25 INFO mapred.MapTask: Processing split: hdfs://localhost:54310/input/random_records:0+88890\n",
      "15/09/15 11:46:25 INFO mapred.MapTask: numReduceTasks: 1\n",
      "15/09/15 11:46:25 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "15/09/15 11:46:25 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "15/09/15 11:46:25 INFO mapred.MapTask: soft limit at 83886080\n",
      "15/09/15 11:46:25 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "15/09/15 11:46:25 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "15/09/15 11:46:25 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "15/09/15 11:46:25 INFO streaming.PipeMapRed: PipeMapRed exec [/home/hadoop/w2/./mapper.py]\n",
      "15/09/15 11:46:25 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "15/09/15 11:46:25 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "15/09/15 11:46:25 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "15/09/15 11:46:25 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "15/09/15 11:46:25 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "15/09/15 11:46:25 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "15/09/15 11:46:25 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "15/09/15 11:46:25 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "15/09/15 11:46:25 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "15/09/15 11:46:25 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "15/09/15 11:46:25 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "15/09/15 11:46:25 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "15/09/15 11:46:25 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 11:46:25 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 11:46:25 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 11:46:25 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 11:46:25 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 11:46:25 INFO streaming.PipeMapRed: Records R/W=10000/1\n",
      "15/09/15 11:46:25 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/15 11:46:25 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/15 11:46:25 INFO mapred.LocalJobRunner: \n",
      "15/09/15 11:46:25 INFO mapred.MapTask: Starting flush of map output\n",
      "15/09/15 11:46:25 INFO mapred.MapTask: Spilling map output\n",
      "15/09/15 11:46:25 INFO mapred.MapTask: bufstart = 0; bufend = 78890; bufvoid = 104857600\n",
      "15/09/15 11:46:25 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26174400(104697600); length = 39997/6553600\n",
      "15/09/15 11:46:26 INFO mapred.MapTask: Finished spill 0\n",
      "15/09/15 11:46:26 INFO mapred.Task: Task:attempt_local1334444185_0001_m_000000_0 is done. And is in the process of committing\n",
      "15/09/15 11:46:26 INFO mapred.LocalJobRunner: Records R/W=10000/1\n",
      "15/09/15 11:46:26 INFO mapred.Task: Task 'attempt_local1334444185_0001_m_000000_0' done.\n",
      "15/09/15 11:46:26 INFO mapred.LocalJobRunner: Finishing task: attempt_local1334444185_0001_m_000000_0\n",
      "15/09/15 11:46:26 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "15/09/15 11:46:26 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "15/09/15 11:46:26 INFO mapred.LocalJobRunner: Starting task: attempt_local1334444185_0001_r_000000_0\n",
      "15/09/15 11:46:26 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/09/15 11:46:26 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@30722c6a\n",
      "15/09/15 11:46:26 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=333971456, maxSingleShuffleLimit=83492864, mergeThreshold=220421168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "15/09/15 11:46:26 INFO reduce.EventFetcher: attempt_local1334444185_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "15/09/15 11:46:26 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1334444185_0001_m_000000_0 decomp: 98892 len: 98896 to MEMORY\n",
      "15/09/15 11:46:26 INFO reduce.InMemoryMapOutput: Read 98892 bytes from map-output for attempt_local1334444185_0001_m_000000_0\n",
      "15/09/15 11:46:26 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 98892, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->98892\n",
      "15/09/15 11:46:26 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "15/09/15 11:46:26 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/15 11:46:26 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "15/09/15 11:46:26 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/15 11:46:26 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 98888 bytes\n",
      "15/09/15 11:46:26 INFO mapreduce.Job: Job job_local1334444185_0001 running in uber mode : false\n",
      "15/09/15 11:46:26 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "15/09/15 11:46:26 INFO reduce.MergeManagerImpl: Merged 1 segments, 98892 bytes to disk to satisfy reduce memory limit\n",
      "15/09/15 11:46:26 INFO reduce.MergeManagerImpl: Merging 1 files, 98896 bytes from disk\n",
      "15/09/15 11:46:26 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "15/09/15 11:46:26 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/15 11:46:26 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 98888 bytes\n",
      "15/09/15 11:46:26 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/15 11:46:26 INFO streaming.PipeMapRed: PipeMapRed exec [/home/hadoop/w2/./reducer.py]\n",
      "15/09/15 11:46:26 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "15/09/15 11:46:26 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "15/09/15 11:46:26 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 11:46:26 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 11:46:26 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 11:46:26 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 11:46:26 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 11:46:26 INFO streaming.PipeMapRed: Records R/W=10000/1\n",
      "15/09/15 11:46:26 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/15 11:46:26 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/15 11:46:26 INFO mapred.Task: Task:attempt_local1334444185_0001_r_000000_0 is done. And is in the process of committing\n",
      "15/09/15 11:46:26 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/15 11:46:26 INFO mapred.Task: Task attempt_local1334444185_0001_r_000000_0 is allowed to commit now\n",
      "15/09/15 11:46:26 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1334444185_0001_r_000000_0' to hdfs://localhost:54310/output/_temporary/0/task_local1334444185_0001_r_000000\n",
      "15/09/15 11:46:26 INFO mapred.LocalJobRunner: Records R/W=10000/1 > reduce\n",
      "15/09/15 11:46:26 INFO mapred.Task: Task 'attempt_local1334444185_0001_r_000000_0' done.\n",
      "15/09/15 11:46:26 INFO mapred.LocalJobRunner: Finishing task: attempt_local1334444185_0001_r_000000_0\n",
      "15/09/15 11:46:26 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "15/09/15 11:46:27 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "15/09/15 11:46:27 INFO mapreduce.Job: Job job_local1334444185_0001 completed successfully\n",
      "15/09/15 11:46:27 INFO mapreduce.Job: Counters: 38\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=408076\n",
      "\t\tFILE: Number of bytes written=1021422\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=177780\n",
      "\t\tHDFS: Number of bytes written=98890\n",
      "\t\tHDFS: Number of read operations=15\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=10000\n",
      "\t\tMap output records=10000\n",
      "\t\tMap output bytes=78890\n",
      "\t\tMap output materialized bytes=98896\n",
      "\t\tInput split bytes=95\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=10000\n",
      "\t\tReduce shuffle bytes=98896\n",
      "\t\tReduce input records=10000\n",
      "\t\tReduce output records=10000\n",
      "\t\tSpilled Records=20000\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=62\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=572522496\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=88890\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=98890\n",
      "15/09/15 11:46:27 INFO streaming.StreamJob: Output directory: /output/\n",
      "0, NA\t\n",
      "1, NA\t\n",
      "2, NA\t\n",
      "3, NA\t\n",
      "4, NA\t\n",
      "5, NA\t\n",
      "6, NA\t\n",
      "7, NA\t\n",
      "8, NA\t\n",
      "9, NA\t\n",
      "10, NA\t\n",
      "11, NA\t\n",
      "12, NA\t\n",
      "13, NA\t\n",
      "14, NA\t\n",
      "15, NA\t\n",
      "16, NA\t\n",
      "17, NA\t\n",
      "18, NA\t\n",
      "19, NA\t\n",
      "20, NA\t\n",
      "21, NA\t\n",
      "22, NA\t\n",
      "23, NA\t\n",
      "24, NA\t\n",
      "25, NA\t\n",
      "26, NA\t\n",
      "27, NA\t\n",
      "28, NA\t\n",
      "29, NA\t\n",
      "30, NA\t\n",
      "31, NA\t\n",
      "32, NA\t\n",
      "33, NA\t\n",
      "34, NA\t\n",
      "35, NA\t\n",
      "36, NA\t\n",
      "37, NA\t\n",
      "38, NA\t\n",
      "39, NA\t\n",
      "40, NA\t\n",
      "41, NA\t\n",
      "42, NA\t\n",
      "43, NA\t\n",
      "44, NA\t\n",
      "45, NA\t\n",
      "46, NA\t\n",
      "47, NA\t\n",
      "48, NA\t\n",
      "49, NA\t\n",
      "50, NA\t\n",
      "51, NA\t\n",
      "52, NA\t\n",
      "53, NA\t\n",
      "54, NA\t\n",
      "55, NA\t\n",
      "56, NA\t\n",
      "57, NA\t\n",
      "58, NA\t\n",
      "59, NA\t\n",
      "60, NA\t\n",
      "61, NA\t\n",
      "62, NA\t\n",
      "63, NA\t\n",
      "64, NA\t\n",
      "65, NA\t\n",
      "66, NA\t\n",
      "67, NA\t\n",
      "68, NA\t\n",
      "69, NA\t\n",
      "70, NA\t\n",
      "71, NA\t\n",
      "72, NA\t\n",
      "73, NA\t\n",
      "74, NA\t\n",
      "75, NA\t\n",
      "76, NA\t\n",
      "77, NA\t\n",
      "78, NA\t\n",
      "79, NA\t\n",
      "80, NA\t\n",
      "81, NA\t\n",
      "82, NA\t\n",
      "83, NA\t\n",
      "84, NA\t\n",
      "85, NA\t\n",
      "86, NA\t\n",
      "87, NA\t\n",
      "88, NA\t\n",
      "89, NA\t\n",
      "90, NA\t\n",
      "91, NA\t\n",
      "92, NA\t\n",
      "93, NA\t\n",
      "94, NA\t\n",
      "95, NA\t\n",
      "96, NA\t\n",
      "97, NA\t\n",
      "98, NA\t\n",
      "99, NA\t\n"
     ]
    }
   ],
   "source": [
    "# Deleting folders from HDFS\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rm -r /input\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rm -r /output\n",
    "\n",
    "# Creating folder in HDFS\n",
    "!/usr/local/hadoop/bin/hdfs dfs -mkdir -p /input\n",
    "\n",
    "# Copying input file for the job into HDFS\n",
    "!/usr/local/hadoop/bin/hdfs dfs -put random_records /input/\n",
    "\n",
    "# Running MapReducer job\n",
    "# With numeric sort of Keys\n",
    "# Number of reducers = 1\n",
    "!/usr/local/hadoop/bin/hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.6.0.jar -D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator  -D  mapred.text.key.comparator.options=-n  -D mapred.reduce.tasks=1 -mapper mapper.py -reducer reducer.py -input /input/* -output /output/\n",
    "\n",
    "#Reading output into a variable\n",
    "job_output = !/usr/local/hadoop/bin/hdfs dfs -cat /output/*\n",
    "\n",
    "# Printing first 100 lines of MapReduce output\n",
    "for line in job_output[1:101]:\n",
    "    print line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#HW2.2  \n",
    "Using the Enron data from HW1 and Hadoop MapReduce streaming, write mapper/reducer pair that  will determine the number of occurrences of a single, user-specified word. Examine the word “assistance” and report your results.\n",
    "\n",
    "   To do so, make sure that\n",
    "   \n",
    "   - mapper.py counts all occurrences of a single word, and\n",
    "   - reducer.py collates the counts of the single word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mapper**  \n",
    "As informed by **Jake** on google groups, I am **only using email content for calculations**. As a result the word **assistance** is returning a **count of 9 instead of 10**  \n",
    "Error in input data:  \n",
    "Two emails have only 2 tab separator. For such emails, the mapper assumes that subject is missing\n",
    "\n",
    "**This job can display counts of a single word, a list of words or all words**  \n",
    "The input words are sent to the MapReduce task using -cmdenv option in the environment variable vocab_input\n",
    "\n",
    "Code Description:  \n",
    "The mapper splits each email and creates separate variables for email_id, truth, subject and message. It then counts the number of times each word occurs in the message and then outputs word, word_count, email_id and truth separated by tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/home/vineetgangwar/anaconda/bin/python\n",
    "import sys\n",
    "import re\n",
    "import os\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # Splitting each line/email based on tab\n",
    "    fields = line.strip().split('\\t')\n",
    "    email_id = fields[0]\n",
    "    truth = fields[1]\n",
    "    if len(fields) == 4:\n",
    "        subject = fields[2]\n",
    "        message = fields[3]\n",
    "    else:\n",
    "        subject = \"\"\n",
    "        message = fields[2]\n",
    "    \n",
    "    # Finding set of unique words and list of all words\n",
    "    message = message.lower()\n",
    "    message = re.sub('[^a-z]', ' ', message)    # Converting non-alpha to space\n",
    "    words = message.split()\n",
    "    unique_words = set(words)\n",
    "    \n",
    "    ## Capturing envirnment variable vocab_input to obtain user inout data\n",
    "    env_vars = os.environ\n",
    "    vocab = env_vars['vocab_input'].split(',')\n",
    "    # Handling for hw1.5 i.e. if '*' then vocab is all words\n",
    "    if vocab[0] == '*':\n",
    "        vocab = unique_words\n",
    "\n",
    "    # Loop through all unique words and calculating count\n",
    "    # And printing word, word_count, email_id, truth\n",
    "    for word in vocab:\n",
    "        word_count = words.count(word)\n",
    "        print '{0}\\t{1}\\t{2}\\t{3}'.format(word, word_count, email_id, truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reducer**  \n",
    "The reducer first accumulates all the mapper output data in a list object. It then loops through the list and does two things:\n",
    "- Uses the words in the list to create dictionary keys\n",
    "- Uses the word_counts in the list to increment the dictionary values where the word == dictionary key\n",
    "\n",
    "It then prints out the dictionary which gives words and their corresponding counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/home/vineetgangwar/anaconda/bin/python\n",
    "import sys\n",
    "\n",
    "# The vocab variable can take the following values:\n",
    "# A single word - such as assistance\n",
    "# A list of words - such as \"assistance valium him her\"\n",
    "# \"*\" - meaning all words\n",
    "\n",
    "# This list will store the output of all mappers as a list of lists\n",
    "# e.g.\n",
    "# [\n",
    "# ['assistance', '1', '0018.2003-12-18.GP', '1'],\n",
    "# ['assistance', '3', '0018.2001-07-13.SA_and_HP', '1'],\n",
    "# ['enlargementwithatypo', '0', '0001.1999-12-10.farmer', '0']\n",
    "# ]\n",
    "key_value_list = list()\n",
    "\n",
    "for line in sys.stdin:\n",
    "    fields = line.strip().split('\\t')\n",
    "    key_value_list.append(fields)\n",
    "\n",
    "# This dict will be used to store counts of terms\n",
    "word_count_dict = dict()\n",
    "\n",
    "# Looping through the list of lists created by def readfiles()\n",
    "for item in key_value_list:\n",
    "    if item[0] in word_count_dict.keys():   # If word exists update count\n",
    "        word_count_dict[item[0]] = int(word_count_dict[item[0]]) + int(item[1])\n",
    "    else:                                   # If new word then create key and store count\n",
    "        word_count_dict[item[0]] = int(item[1])\n",
    "\n",
    "\n",
    "for key, value in word_count_dict.iteritems():\n",
    "    print  key + '\\t' + str(value)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning HDFS and moving input file into HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/15 11:46:45 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/15 11:46:46 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /input\n",
      "15/09/15 11:46:47 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/15 11:46:47 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /output\n",
      "15/09/15 11:46:48 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/15 11:46:50 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/15 11:46:51 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/15 11:46:52 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "15/09/15 11:46:52 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "15/09/15 11:46:52 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "15/09/15 11:46:52 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "15/09/15 11:46:52 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "15/09/15 11:46:52 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1522778894_0001\n",
      "15/09/15 11:46:52 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "15/09/15 11:46:52 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "15/09/15 11:46:52 INFO mapreduce.Job: Running job: job_local1522778894_0001\n",
      "15/09/15 11:46:52 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "15/09/15 11:46:52 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "15/09/15 11:46:52 INFO mapred.LocalJobRunner: Starting task: attempt_local1522778894_0001_m_000000_0\n",
      "15/09/15 11:46:52 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/09/15 11:46:52 INFO mapred.MapTask: Processing split: hdfs://localhost:54310/input/enronemail_1h.txt:0+203979\n",
      "15/09/15 11:46:52 INFO mapred.MapTask: numReduceTasks: 1\n",
      "15/09/15 11:46:52 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "15/09/15 11:46:52 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "15/09/15 11:46:52 INFO mapred.MapTask: soft limit at 83886080\n",
      "15/09/15 11:46:52 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "15/09/15 11:46:52 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "15/09/15 11:46:52 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "15/09/15 11:46:52 INFO streaming.PipeMapRed: PipeMapRed exec [/home/hadoop/w2/./mapper.py]\n",
      "15/09/15 11:46:52 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "15/09/15 11:46:52 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "15/09/15 11:46:52 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "15/09/15 11:46:52 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "15/09/15 11:46:52 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "15/09/15 11:46:52 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "15/09/15 11:46:52 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "15/09/15 11:46:52 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "15/09/15 11:46:52 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "15/09/15 11:46:52 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "15/09/15 11:46:52 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "15/09/15 11:46:52 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "15/09/15 11:46:52 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 11:46:52 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 11:46:52 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 11:46:52 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/15 11:46:52 INFO streaming.PipeMapRed: Records R/W=100/1\n",
      "15/09/15 11:46:52 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/15 11:46:52 INFO mapred.LocalJobRunner: \n",
      "15/09/15 11:46:52 INFO mapred.MapTask: Starting flush of map output\n",
      "15/09/15 11:46:52 INFO mapred.MapTask: Spilling map output\n",
      "15/09/15 11:46:52 INFO mapred.MapTask: bufstart = 0; bufend = 3772; bufvoid = 104857600\n",
      "15/09/15 11:46:52 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214000(104856000); length = 397/6553600\n",
      "15/09/15 11:46:52 INFO mapred.MapTask: Finished spill 0\n",
      "15/09/15 11:46:52 INFO mapred.Task: Task:attempt_local1522778894_0001_m_000000_0 is done. And is in the process of committing\n",
      "15/09/15 11:46:52 INFO mapred.LocalJobRunner: Records R/W=100/1\n",
      "15/09/15 11:46:52 INFO mapred.Task: Task 'attempt_local1522778894_0001_m_000000_0' done.\n",
      "15/09/15 11:46:52 INFO mapred.LocalJobRunner: Finishing task: attempt_local1522778894_0001_m_000000_0\n",
      "15/09/15 11:46:52 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "15/09/15 11:46:52 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "15/09/15 11:46:52 INFO mapred.LocalJobRunner: Starting task: attempt_local1522778894_0001_r_000000_0\n",
      "15/09/15 11:46:52 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/09/15 11:46:52 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@67ac602\n",
      "15/09/15 11:46:52 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=333971456, maxSingleShuffleLimit=83492864, mergeThreshold=220421168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "15/09/15 11:46:52 INFO reduce.EventFetcher: attempt_local1522778894_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "15/09/15 11:46:52 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1522778894_0001_m_000000_0 decomp: 3974 len: 3978 to MEMORY\n",
      "15/09/15 11:46:52 INFO reduce.InMemoryMapOutput: Read 3974 bytes from map-output for attempt_local1522778894_0001_m_000000_0\n",
      "15/09/15 11:46:52 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 3974, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->3974\n",
      "15/09/15 11:46:52 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "15/09/15 11:46:53 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/15 11:46:53 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "15/09/15 11:46:53 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/15 11:46:53 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3961 bytes\n",
      "15/09/15 11:46:53 INFO reduce.MergeManagerImpl: Merged 1 segments, 3974 bytes to disk to satisfy reduce memory limit\n",
      "15/09/15 11:46:53 INFO reduce.MergeManagerImpl: Merging 1 files, 3978 bytes from disk\n",
      "15/09/15 11:46:53 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "15/09/15 11:46:53 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/15 11:46:53 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3961 bytes\n",
      "15/09/15 11:46:53 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/15 11:46:53 INFO streaming.PipeMapRed: PipeMapRed exec [/home/hadoop/w2/./reducer.py]\n",
      "15/09/15 11:46:53 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "15/09/15 11:46:53 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "15/09/15 11:46:53 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 11:46:53 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 11:46:53 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 11:46:53 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/15 11:46:53 INFO streaming.PipeMapRed: Records R/W=100/1\n",
      "15/09/15 11:46:53 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/15 11:46:53 INFO mapred.Task: Task:attempt_local1522778894_0001_r_000000_0 is done. And is in the process of committing\n",
      "15/09/15 11:46:53 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/15 11:46:53 INFO mapred.Task: Task attempt_local1522778894_0001_r_000000_0 is allowed to commit now\n",
      "15/09/15 11:46:53 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1522778894_0001_r_000000_0' to hdfs://localhost:54310/output/_temporary/0/task_local1522778894_0001_r_000000\n",
      "15/09/15 11:46:53 INFO mapred.LocalJobRunner: Records R/W=100/1 > reduce\n",
      "15/09/15 11:46:53 INFO mapred.Task: Task 'attempt_local1522778894_0001_r_000000_0' done.\n",
      "15/09/15 11:46:53 INFO mapred.LocalJobRunner: Finishing task: attempt_local1522778894_0001_r_000000_0\n",
      "15/09/15 11:46:53 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "15/09/15 11:46:53 INFO mapreduce.Job: Job job_local1522778894_0001 running in uber mode : false\n",
      "15/09/15 11:46:53 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "15/09/15 11:46:53 INFO mapreduce.Job: Job job_local1522778894_0001 completed successfully\n",
      "15/09/15 11:46:53 INFO mapreduce.Job: Counters: 38\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=218246\n",
      "\t\tFILE: Number of bytes written=734516\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=407958\n",
      "\t\tHDFS: Number of bytes written=13\n",
      "\t\tHDFS: Number of read operations=15\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=100\n",
      "\t\tMap output bytes=3772\n",
      "\t\tMap output materialized bytes=3978\n",
      "\t\tInput split bytes=98\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce shuffle bytes=3978\n",
      "\t\tReduce input records=100\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=200\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=8\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=572522496\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=203979\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=13\n",
      "15/09/15 11:46:53 INFO streaming.StreamJob: Output directory: /output/\n",
      "assistance\t9\n"
     ]
    }
   ],
   "source": [
    "# Deleting folders from HDFS\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rm -r /input\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rm -r /output\n",
    "\n",
    "# Creating folder\n",
    "!/usr/local/hadoop/bin/hdfs dfs -mkdir -p /input\n",
    "\n",
    "# Copying input file for the job\n",
    "!/usr/local/hadoop/bin/hdfs dfs -put enronemail_1h.txt /input/\n",
    "\n",
    "# Executing the MapReduce job\n",
    "!/usr/local/hadoop/bin/hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.6.0.jar -mapper mapper.py -reducer reducer.py -cmdenv vocab_input=assistance -input /input/* -output /output/\n",
    "\n",
    "# Reading output into a variable\n",
    "job_output = !/usr/local/hadoop/bin/hdfs dfs -cat /output/*\n",
    "\n",
    "# Printing output\n",
    "for line in job_output[1:101]:\n",
    "    print line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#HW2.3  \n",
    "Using the Enron data from HW1 and Hadoop MapReduce, write  a mapper/reducer pair that will classify the email messages by a single, user-specified word. Examine the word “assistance” and report your results. To do so, make sure that\n",
    "   \n",
    "   - mapper.py\n",
    "   - reducer.py \n",
    "\n",
    "performs a single word multinomial Naive Bayes classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result**  \n",
    "The algorithm obtained an accurracy of 0.56"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mapper**  \n",
    "Using mapper for **HW2.2** above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reducer**  \n",
    "Step 1:  \n",
    "The reducer first accumulates all the mapper output data in a list object. It then uses the list object to create a Pandas DataFrame that contains the term frequency per document:\n",
    "- Index/Row names are the email_ids\n",
    "- Column headers are words/features. If the user inputs a word or a list of words from the command line then the headers are those words. This includes words such as enlargementWITHATypo. If the user specified * then all unique words from all emails become the column headers\n",
    "- The DataFrame also contains another column called 'TRUTH' that contains the true class of each email\n",
    "\n",
    "Step 2:  \n",
    "Next, the reducer calculates probabilities from the above DataFrame and stores them in new objects. It used Pandas DataFrame methods such as groupby, sum for the calculations:\n",
    "- It calculates and stores the prior probabilities in a dictionary\n",
    "- It calculates and stores P(word|class) in a Pandas DataFrame. The index of the DataFrame are words and the columns are 'SPAM' and 'HAM'. Laplace smoothing is applied in this step\n",
    "\n",
    "Step 3:  \n",
    "The Reducer then calculates P(email|class) and stores these in yet another Pandas DataFrame. This DataFrame has email_ids as the index and has the following column headings - SPAM, HAM, PREDICT, TRUTH. 'SPAM' stores the log probability of the email given SPAM. 'HAM' stores the log probability of the email given HAM. 'PREDICT' stores the predict class of the email based on the calculated log probabilities. 'TRUTH' contains the true class of the email.\n",
    "The log probabilities of email given class is calculated as follows:\n",
    "- For each email, the reducer refers the 1st DataFrame i.e. the one that contains the Term Frequecy per DataFrame. It gets a dictionary of words and counts where the counts are greater than zero.\n",
    "- It then uses the prior proabilities dictionary and the DataFrame containing P(word|class) to calculate P(email|document)\n",
    "- It then stores the log probabilities in the DataFrame created in step 3\n",
    "\n",
    "Step 4:\n",
    "The Reducer outputs the results in the format - Email_id \\t TRUTH \\t Predicted_Class. It also calculates the accuracy and prints it also.\n",
    "\n",
    "Note:  \n",
    "The reducer has an additional functionalilty wherein it drops words/tokens/features whose terms frequency is less than 3. This feature can be turned on/off from the command line. The -cmdenv drop_cols=no disables this feature while -cmdenv drop_cols=yes enables this feature. In HW2.5 this feature is set to yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/home/vineetgangwar/anaconda/bin/python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# The vocab variable can take the following values:\n",
    "# A single word - such as assistance\n",
    "# A list of words - such as \"assistance valium him her\"\n",
    "# \"*\" - meaning all words\n",
    "\n",
    "# This function creates a dataframe with email_ids as the index\n",
    "# and all words as the column headings.\n",
    "# Each cell contains the count of occurrences of each word in each email\n",
    "# This function returns a tuple of vocab and the DataFrame\n",
    "\n",
    "def create_dataframe(key_value_list):\n",
    "    ## Creating dataframe of email id and truth pairs\n",
    "    # Creating list of email_ids and truths\n",
    "    email_ids = list()\n",
    "    truths = list()\n",
    "    for item in key_value_list:\n",
    "        email_id = item[2]\n",
    "        truth = int(item[3])\n",
    "        if email_id not in email_ids:\n",
    "            email_ids.append(email_id)\n",
    "            truths.append(truth)\n",
    "    # Creating dictionary\n",
    "    id_truth_dict = dict()\n",
    "    id_truth_dict['email_id'] = email_ids\n",
    "    id_truth_dict['TRUTH'] = truths\n",
    "    # Converting into dataframe\n",
    "    id_truth = pd.DataFrame(id_truth_dict)\n",
    "\n",
    "    ## Creating data frame to store word counts and email in matrix\n",
    "    # Creating words and ids list to create an empty data frame email_id X word list\n",
    "    set_of_words = set()\n",
    "    set_of_ids = set()\n",
    "    for item in key_value_list:\n",
    "        set_of_words.add(item[0])\n",
    "        set_of_ids.add(item[2])\n",
    "\n",
    "    set_of_words = list(set_of_words)\n",
    "    set_of_ids = list(set_of_ids)\n",
    "    num_of_ids = len(set_of_ids)\n",
    "\n",
    "    # Creating dict of zeros to convert into a dataframe\n",
    "    zeros_dict = dict()\n",
    "    for i in range(len(set_of_words)):\n",
    "        zeros_dict[set_of_words[i]] = [0 for x in range(num_of_ids)]\n",
    "    # Adding ids\n",
    "    zeros_dict['email_id'] = set_of_ids\n",
    "\n",
    "    # Converting into dataframe\n",
    "    id_wordlist = pd.DataFrame(zeros_dict)\n",
    "\n",
    "    # Merging dataframe to add truth also\n",
    "    df = pd.merge(id_wordlist, id_truth, on='email_id', how='inner')\n",
    "    df.set_index('email_id', inplace=True)\n",
    "\n",
    "    # Updating counts\n",
    "    for item in key_value_list:\n",
    "        email_id = item[2]\n",
    "        word_count = item[1]\n",
    "        word = item[0]\n",
    "        df.loc[email_id, word] = int(word_count)\n",
    "\n",
    "    return set_of_words, df\n",
    "\n",
    "# This function calcuates the following probabilities:\n",
    "# Priors in a Dict() called priors\n",
    "# A DataFrame containing probabilities of all words given class. This Dataframe called \n",
    "# word_prob_class has the following structure:\n",
    "# words X class\n",
    "\n",
    "def calculating_probs(vocab, df):\n",
    "    category = {'spam': 1, 'ham': 0}\n",
    "    ## Calculating probabilities\n",
    "    # Calculating priors probabilites and storing in a dict\n",
    "    prob_prior_spam = df.groupby('TRUTH').size()[1].astype(float) / len(df)\n",
    "    prob_prior_ham = df.groupby('TRUTH').size()[0].astype(float) / len(df)\n",
    "    priors = {'spam': prob_prior_spam, 'ham': prob_prior_ham}\n",
    "\n",
    "    # Calculating term count in spam and ham for the given vocab\n",
    "    term_count_spam = df.groupby('TRUTH').sum().sum(axis=1)[1]\n",
    "    term_count_ham = df.groupby('TRUTH').sum().sum(axis=1)[0]\n",
    "    term_count_category = {'spam': term_count_spam, 'ham': term_count_ham}\n",
    "\n",
    "    # Calculating counts of words in vocab per catergory\n",
    "    words_per_category = df.groupby('TRUTH').sum().transpose()\n",
    "\n",
    "    # Calculating word probabilities per class\n",
    "    word_probs_class = words_per_category.copy()\n",
    "    for cat_key, cat_value in category.iteritems():\n",
    "        word_probs_class[cat_value] = word_probs_class[cat_value] / term_count_category[cat_key]\n",
    "    # Applying laplace smoothing\n",
    "    # For Spam\n",
    "    word_probs_class[1][word_probs_class[1] == 0] = float(1) / (term_count_category['spam'] + len(vocab))\n",
    "    # For ham\n",
    "    word_probs_class[0][word_probs_class[0] == 0] = float(1) / (term_count_category['ham'] + len(vocab))\n",
    "    return priors, word_probs_class\n",
    "\n",
    "def drop_cols_less_than_3(df):\n",
    "    cols_more_3 = dict(df.sum(axis=0) >= 3)\n",
    "    cols_to_include = [key for key, val in cols_more_3.iteritems() if val == True]\n",
    "    df = df[cols_to_include]\n",
    "    return df\n",
    "\n",
    "# Main execution of the Reducer starts here\n",
    "\n",
    "# This list will store the output of all mappers as a list of lists\n",
    "# e.g.\n",
    "# [\n",
    "# ['assistance', '1', '0018.2003-12-18.GP', '1'],\n",
    "# ['assistance', '3', '0018.2001-07-13.SA_and_HP', '1'],\n",
    "# ['enlargementwithatypo', '0', '0001.1999-12-10.farmer', '0']\n",
    "# ]\n",
    "key_value_list = list()\n",
    "for line in sys.stdin:\n",
    "    fields = line.strip().split('\\t')\n",
    "    key_value_list.append(fields)\n",
    "\n",
    "# Creating pandas DataFrame from input data\n",
    "vocab, df = create_dataframe(key_value_list)\n",
    "\n",
    "# If flag for dropping cols is set then dropping for words that occurred less than 3 times in the corpus\n",
    "env_vars = os.environ\n",
    "drop_cols = env_vars['drop_cols']\n",
    "if drop_cols == 'yes':\n",
    "    df = drop_cols_less_than_3(df)\n",
    "\n",
    "# Getting priors and words given class probabilities\n",
    "priors, word_probs_class = calculating_probs(vocab, df)\n",
    "\n",
    "# Creating Pandas DataFrame to store final probabilities\n",
    "# Creating dataframe to store probabilities\n",
    "# Structure of DataFrame has email_ds in the index\n",
    "# and spam, ham, TRUTH, PREDICT as columns\n",
    "df_probs = df.copy(deep=True)\n",
    "header_to_remove = list(df_probs.columns.values)\n",
    "header_to_remove.remove('TRUTH')\n",
    "df_probs.drop(header_to_remove, inplace=True, axis=1)\n",
    "df_probs['spam'] = [0 for x in range(df.index.values.shape[0])]\n",
    "df_probs['ham'] = [0 for x in range(df.index.values.shape[0])]\n",
    "df_probs['PREDICT'] = [0 for x in range(df.index.values.shape[0])]\n",
    "\n",
    "# Looping through all emails and calculating probabilites of email given class\n",
    "# and storing in a DataFrame\n",
    "category = {'spam': 1, 'ham': 0}\n",
    "for email_id in df_probs.index:\n",
    "    # Creating dict of all words whose count != 0 per email\n",
    "    words_in_email = dict(df.loc[email_id, df.loc[email_id] != 0])\n",
    "    # Removing 'TRUTH'\n",
    "    if 'TRUTH' in words_in_email:\n",
    "        words_in_email.pop('TRUTH')\n",
    "\n",
    "    for cat_key, cat_value in category.iteritems():\n",
    "        running_prob = math.log(priors[cat_key])\n",
    "\n",
    "        for word, count in words_in_email.iteritems():\n",
    "            running_prob += count * math.log(word_probs_class.loc[word, cat_value])\n",
    "\n",
    "        df_probs.loc[email_id, cat_key] = running_prob\n",
    "\n",
    "# Calculating predictions\n",
    "df_probs['PREDICT'] = (df_probs['spam'] > df_probs['ham']).astype(int)\n",
    "\n",
    "# Printing output\n",
    "for email_id in df_probs.index:\n",
    "    print email_id, '\\t', int(df_probs.loc[email_id, 'TRUTH']), '\\t', int(df_probs.loc[email_id, 'PREDICT'])\n",
    "\n",
    "# Calculating and printing accuracy accurracy\n",
    "correct = df_probs['TRUTH'] == df_probs['PREDICT']\n",
    "print 'Accuracy:', float(np.sum(correct.astype(int))) / len(df_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning HDFS, moving input file into HDFS, executing MapReduce Job and displaying output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/15 11:47:10 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/15 11:47:11 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /input\n",
      "15/09/15 11:47:12 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/15 11:47:12 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /output\n",
      "15/09/15 11:47:13 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/15 11:47:15 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/15 11:47:17 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/15 11:47:17 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "15/09/15 11:47:17 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "15/09/15 11:47:17 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "15/09/15 11:47:17 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "15/09/15 11:47:17 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "15/09/15 11:47:17 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1708650624_0001\n",
      "15/09/15 11:47:18 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "15/09/15 11:47:18 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "15/09/15 11:47:18 INFO mapreduce.Job: Running job: job_local1708650624_0001\n",
      "15/09/15 11:47:18 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "15/09/15 11:47:18 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "15/09/15 11:47:18 INFO mapred.LocalJobRunner: Starting task: attempt_local1708650624_0001_m_000000_0\n",
      "15/09/15 11:47:18 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/09/15 11:47:18 INFO mapred.MapTask: Processing split: hdfs://localhost:54310/input/enronemail_1h.txt:0+203979\n",
      "15/09/15 11:47:18 INFO mapred.MapTask: numReduceTasks: 1\n",
      "15/09/15 11:47:18 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "15/09/15 11:47:18 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "15/09/15 11:47:18 INFO mapred.MapTask: soft limit at 83886080\n",
      "15/09/15 11:47:18 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "15/09/15 11:47:18 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "15/09/15 11:47:18 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "15/09/15 11:47:18 INFO streaming.PipeMapRed: PipeMapRed exec [/home/hadoop/w2/./mapper.py]\n",
      "15/09/15 11:47:18 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "15/09/15 11:47:18 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "15/09/15 11:47:18 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "15/09/15 11:47:18 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "15/09/15 11:47:18 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "15/09/15 11:47:18 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "15/09/15 11:47:18 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "15/09/15 11:47:18 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "15/09/15 11:47:18 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "15/09/15 11:47:18 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "15/09/15 11:47:18 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "15/09/15 11:47:18 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "15/09/15 11:47:18 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 11:47:18 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 11:47:18 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 11:47:18 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/15 11:47:18 INFO streaming.PipeMapRed: Records R/W=100/1\n",
      "15/09/15 11:47:18 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/15 11:47:18 INFO mapred.LocalJobRunner: \n",
      "15/09/15 11:47:18 INFO mapred.MapTask: Starting flush of map output\n",
      "15/09/15 11:47:18 INFO mapred.MapTask: Spilling map output\n",
      "15/09/15 11:47:18 INFO mapred.MapTask: bufstart = 0; bufend = 3772; bufvoid = 104857600\n",
      "15/09/15 11:47:18 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214000(104856000); length = 397/6553600\n",
      "15/09/15 11:47:18 INFO mapred.MapTask: Finished spill 0\n",
      "15/09/15 11:47:18 INFO mapred.Task: Task:attempt_local1708650624_0001_m_000000_0 is done. And is in the process of committing\n",
      "15/09/15 11:47:18 INFO mapred.LocalJobRunner: Records R/W=100/1\n",
      "15/09/15 11:47:18 INFO mapred.Task: Task 'attempt_local1708650624_0001_m_000000_0' done.\n",
      "15/09/15 11:47:18 INFO mapred.LocalJobRunner: Finishing task: attempt_local1708650624_0001_m_000000_0\n",
      "15/09/15 11:47:18 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "15/09/15 11:47:18 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "15/09/15 11:47:18 INFO mapred.LocalJobRunner: Starting task: attempt_local1708650624_0001_r_000000_0\n",
      "15/09/15 11:47:18 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/09/15 11:47:18 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@10f70ce1\n",
      "15/09/15 11:47:18 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=333971456, maxSingleShuffleLimit=83492864, mergeThreshold=220421168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "15/09/15 11:47:18 INFO reduce.EventFetcher: attempt_local1708650624_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "15/09/15 11:47:18 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1708650624_0001_m_000000_0 decomp: 3974 len: 3978 to MEMORY\n",
      "15/09/15 11:47:18 INFO reduce.InMemoryMapOutput: Read 3974 bytes from map-output for attempt_local1708650624_0001_m_000000_0\n",
      "15/09/15 11:47:18 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 3974, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->3974\n",
      "15/09/15 11:47:18 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "15/09/15 11:47:18 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/15 11:47:18 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "15/09/15 11:47:18 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/15 11:47:18 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3961 bytes\n",
      "15/09/15 11:47:18 INFO reduce.MergeManagerImpl: Merged 1 segments, 3974 bytes to disk to satisfy reduce memory limit\n",
      "15/09/15 11:47:18 INFO reduce.MergeManagerImpl: Merging 1 files, 3978 bytes from disk\n",
      "15/09/15 11:47:18 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "15/09/15 11:47:18 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/15 11:47:18 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3961 bytes\n",
      "15/09/15 11:47:18 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/15 11:47:18 INFO streaming.PipeMapRed: PipeMapRed exec [/home/hadoop/w2/./reducer.py]\n",
      "15/09/15 11:47:18 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "15/09/15 11:47:18 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "15/09/15 11:47:18 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 11:47:18 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 11:47:18 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 11:47:18 INFO streaming.PipeMapRed: Records R/W=100/1\n",
      "15/09/15 11:47:18 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/15 11:47:18 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/15 11:47:18 INFO mapred.Task: Task:attempt_local1708650624_0001_r_000000_0 is done. And is in the process of committing\n",
      "15/09/15 11:47:18 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/15 11:47:18 INFO mapred.Task: Task attempt_local1708650624_0001_r_000000_0 is allowed to commit now\n",
      "15/09/15 11:47:18 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1708650624_0001_r_000000_0' to hdfs://localhost:54310/output/_temporary/0/task_local1708650624_0001_r_000000\n",
      "15/09/15 11:47:18 INFO mapred.LocalJobRunner: Records R/W=100/1 > reduce\n",
      "15/09/15 11:47:18 INFO mapred.Task: Task 'attempt_local1708650624_0001_r_000000_0' done.\n",
      "15/09/15 11:47:18 INFO mapred.LocalJobRunner: Finishing task: attempt_local1708650624_0001_r_000000_0\n",
      "15/09/15 11:47:18 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "15/09/15 11:47:19 INFO mapreduce.Job: Job job_local1708650624_0001 running in uber mode : false\n",
      "15/09/15 11:47:19 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "15/09/15 11:47:19 INFO mapreduce.Job: Job job_local1708650624_0001 completed successfully\n",
      "15/09/15 11:47:19 INFO mapreduce.Job: Counters: 38\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=218246\n",
      "\t\tFILE: Number of bytes written=734568\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=407958\n",
      "\t\tHDFS: Number of bytes written=2888\n",
      "\t\tHDFS: Number of read operations=15\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=100\n",
      "\t\tMap output bytes=3772\n",
      "\t\tMap output materialized bytes=3978\n",
      "\t\tInput split bytes=98\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce shuffle bytes=3978\n",
      "\t\tReduce input records=100\n",
      "\t\tReduce output records=101\n",
      "\t\tSpilled Records=200\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=22\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=571473920\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=203979\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2888\n",
      "15/09/15 11:47:19 INFO streaming.StreamJob: Output directory: /output/\n",
      "0010.2003-12-18.GP \t1 \t0\n",
      "0010.2001-06-28.SA_and_HP \t1 \t0\n",
      "0001.2000-01-17.beck \t0 \t0\n",
      "0018.1999-12-14.kaminski \t0 \t0\n",
      "0005.1999-12-12.kaminski \t0 \t0\n",
      "0011.2001-06-29.SA_and_HP \t1 \t0\n",
      "0008.2004-08-01.BG \t1 \t0\n",
      "0009.1999-12-14.farmer \t0 \t0\n",
      "0017.2003-12-18.GP \t1 \t0\n",
      "0011.2001-06-28.SA_and_HP \t1 \t0\n",
      "0015.2001-07-05.SA_and_HP \t1 \t0\n",
      "0015.2001-02-12.kitchen \t0 \t0\n",
      "0009.2001-06-26.SA_and_HP \t1 \t0\n",
      "0017.1999-12-14.kaminski \t0 \t0\n",
      "0012.2000-01-17.beck \t0 \t0\n",
      "0003.2000-01-17.beck \t0 \t0\n",
      "0004.2001-06-12.SA_and_HP \t1 \t0\n",
      "0008.2001-06-12.SA_and_HP \t1 \t0\n",
      "0007.2001-02-09.kitchen \t0 \t0\n",
      "0016.2004-08-01.BG \t1 \t0\n",
      "0015.2000-06-09.lokay \t0 \t0\n",
      "0016.1999-12-15.farmer \t0 \t0\n",
      "0013.2004-08-01.BG \t1 \t0\n",
      "0005.2003-12-18.GP \t1 \t0\n",
      "0012.2001-02-09.kitchen \t0 \t0\n",
      "0011.1999-12-14.farmer \t0 \t0\n",
      "0009.2001-02-09.kitchen \t0 \t0\n",
      "0006.2001-02-08.kitchen \t0 \t0\n",
      "0014.2003-12-19.GP \t1 \t0\n",
      "0010.1999-12-14.farmer \t0 \t0\n",
      "0010.2004-08-01.BG \t1 \t0\n",
      "0014.1999-12-14.kaminski \t0 \t0\n",
      "0006.1999-12-13.kaminski \t0 \t0\n",
      "0005.1999-12-14.farmer \t0 \t0\n",
      "0003.2001-02-08.kitchen \t0 \t0\n",
      "0001.2001-02-07.kitchen \t0 \t0\n",
      "0008.2001-02-09.kitchen \t0 \t0\n",
      "0017.2004-08-02.BG \t1 \t0\n",
      "0007.2003-12-18.GP \t1 \t0\n",
      "0014.2004-08-01.BG \t1 \t0\n",
      "0006.2003-12-18.GP \t1 \t0\n",
      "0016.2001-07-05.SA_and_HP \t1 \t0\n",
      "0005.2000-06-06.lokay \t0 \t0\n",
      "0014.2001-07-04.SA_and_HP \t1 \t0\n",
      "0001.2001-04-02.williams \t0 \t0\n",
      "0012.2000-06-08.lokay \t0 \t0\n",
      "0014.1999-12-15.farmer \t0 \t0\n",
      "0009.2000-06-07.lokay \t0 \t0\n",
      "0001.1999-12-10.farmer \t0 \t0\n",
      "0008.2001-06-25.SA_and_HP \t1 \t0\n",
      "0017.2001-04-03.williams \t0 \t0\n",
      "0014.2001-02-12.kitchen \t0 \t0\n",
      "0016.2001-07-06.SA_and_HP \t1 \t0\n",
      "0015.1999-12-15.farmer \t0 \t0\n",
      "0004.1999-12-10.kaminski \t0 \t0\n",
      "0001.2000-06-06.lokay \t0 \t0\n",
      "0011.2004-08-01.BG \t1 \t0\n",
      "0004.2004-08-01.BG \t1 \t0\n",
      "0018.2003-12-18.GP \t1 \t0\n",
      "0007.1999-12-14.farmer \t0 \t0\n",
      "0016.2003-12-19.GP \t1 \t0\n",
      "0004.1999-12-14.farmer \t0 \t0\n",
      "0015.2003-12-19.GP \t1 \t0\n",
      "0006.2004-08-01.BG \t1 \t0\n",
      "0009.2003-12-18.GP \t1 \t0\n",
      "0002.1999-12-13.farmer \t0 \t0\n",
      "0008.2003-12-18.GP \t1 \t0\n",
      "0010.1999-12-14.kaminski \t0 \t0\n",
      "0007.2000-01-17.beck \t0 \t0\n",
      "0003.1999-12-14.farmer \t0 \t0\n",
      "0003.2004-08-01.BG \t1 \t0\n",
      "0017.2004-08-01.BG \t1 \t0\n",
      "0013.2001-06-30.SA_and_HP \t1 \t0\n",
      "0003.1999-12-10.kaminski \t0 \t0\n",
      "0012.1999-12-14.farmer \t0 \t0\n",
      "0009.1999-12-13.kaminski \t0 \t0\n",
      "0018.2001-07-13.SA_and_HP \t1 \t0\n",
      "0002.2001-02-07.kitchen \t0 \t0\n",
      "0007.2004-08-01.BG \t1 \t0\n",
      "0012.1999-12-14.kaminski \t0 \t0\n",
      "0005.2001-06-23.SA_and_HP \t1 \t0\n",
      "0013.1999-12-14.kaminski \t0 \t0\n",
      "0007.1999-12-13.kaminski \t0 \t0\n",
      "0017.2000-01-17.beck \t0 \t0\n",
      "0006.2001-06-25.SA_and_HP \t1 \t0\n",
      "0006.2001-04-03.williams \t0 \t0\n",
      "0005.2001-02-08.kitchen \t0 \t0\n",
      "0002.2003-12-18.GP \t1 \t0\n",
      "0003.2003-12-18.GP \t1 \t0\n",
      "0013.2001-04-03.williams \t0 \t0\n",
      "0004.2001-04-02.williams \t0 \t0\n",
      "0010.2001-02-09.kitchen \t0 \t0\n",
      "0001.1999-12-10.kaminski \t0 \t0\n",
      "0013.1999-12-14.farmer \t0 \t0\n",
      "0015.1999-12-14.kaminski \t0 \t0\n",
      "0012.2003-12-19.GP \t1 \t0\n",
      "0016.2001-02-12.kitchen \t0 \t0\n",
      "0002.2004-08-01.BG \t1 \t0\n",
      "0002.2001-05-25.SA_and_HP \t1 \t0\n",
      "0011.2003-12-18.GP \t1 \t0\n",
      "Accuracy: 0.56\t\n"
     ]
    }
   ],
   "source": [
    "# Deleting folders from HDFS\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rm -r /input\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rm -r /output\n",
    "\n",
    "# Creating folder\n",
    "!/usr/local/hadoop/bin/hdfs dfs -mkdir -p /input\n",
    "\n",
    "# Copying input file for the job\n",
    "!/usr/local/hadoop/bin/hdfs dfs -put enronemail_1h.txt /input/\n",
    "\n",
    "!/usr/local/hadoop/bin/hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.6.0.jar -mapper mapper.py -reducer reducer.py -cmdenv vocab_input=assistance -cmdenv drop_cols=no -input /input/* -output /output/\n",
    "\n",
    "#Reading output into a variable\n",
    "job_output = !/usr/local/hadoop/bin/hdfs dfs -cat /output/*\n",
    "\n",
    "# Printing output\n",
    "for line in job_output[1:]:\n",
    "    print line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#HW2.4  \n",
    "Using the Enron data from HW1 and in the Hadoop MapReduce framework, write  a mapper/reducer pair that will classify the email messages using multinomial Naive Bayes Classifier using a list of one or more user-specified words. Examine the words “assistance”, “valium”, and “enlargementWithATypo” and report your results\n",
    "\n",
    "To do so, make sure that\n",
    "\n",
    "   - mapper.py \n",
    "   - reducer.py \n",
    "\n",
    "performs the multiple-word multinomial Naive Bayes classification via the chosen list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result**  \n",
    "The algorithm reports an accuracy of 0.56 with input terms - assistance,valium,enlargementWithATypo. HW1.4 also reported an accuracy of 0.56. However, HW1.4 had reported an accuracy of 0.59 for the same input terms but with tokens selected from the entire email rather than just the email message.  \n",
    "To test the algorithm I passed the following terms - assistance,valium,enlargementWithATypo,him. The algorithm reported an accuracy of 0.62. Further, in HW2.5 when I used all the terms, it reported and accuracy of 0.98"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mapper**  \n",
    "Using mapper from **HW2.2** above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reducer**  \n",
    "Using reducer from **HW2.3** above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm reports an accuracy of 0.56 with input terms - assistance,valium,enlargementWithATypo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/15 11:47:30 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/15 11:47:31 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /input\n",
      "15/09/15 11:47:32 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/15 11:47:32 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /output\n",
      "15/09/15 11:47:33 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/15 11:47:35 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/15 11:47:36 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/15 11:47:37 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "15/09/15 11:47:37 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "15/09/15 11:47:37 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "15/09/15 11:47:37 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "15/09/15 11:47:37 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "15/09/15 11:47:37 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1058894346_0001\n",
      "15/09/15 11:47:37 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "15/09/15 11:47:37 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "15/09/15 11:47:37 INFO mapreduce.Job: Running job: job_local1058894346_0001\n",
      "15/09/15 11:47:37 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "15/09/15 11:47:37 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "15/09/15 11:47:37 INFO mapred.LocalJobRunner: Starting task: attempt_local1058894346_0001_m_000000_0\n",
      "15/09/15 11:47:37 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/09/15 11:47:37 INFO mapred.MapTask: Processing split: hdfs://localhost:54310/input/enronemail_1h.txt:0+203979\n",
      "15/09/15 11:47:37 INFO mapred.MapTask: numReduceTasks: 1\n",
      "15/09/15 11:47:37 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "15/09/15 11:47:37 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "15/09/15 11:47:37 INFO mapred.MapTask: soft limit at 83886080\n",
      "15/09/15 11:47:37 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "15/09/15 11:47:37 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "15/09/15 11:47:37 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "15/09/15 11:47:37 INFO streaming.PipeMapRed: PipeMapRed exec [/home/hadoop/w2/./mapper.py]\n",
      "15/09/15 11:47:37 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "15/09/15 11:47:37 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "15/09/15 11:47:37 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "15/09/15 11:47:37 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "15/09/15 11:47:37 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "15/09/15 11:47:37 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "15/09/15 11:47:37 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "15/09/15 11:47:37 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "15/09/15 11:47:37 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "15/09/15 11:47:37 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "15/09/15 11:47:37 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "15/09/15 11:47:37 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "15/09/15 11:47:37 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 11:47:37 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 11:47:37 INFO streaming.PipeMapRed: Records R/W=72/1\n",
      "15/09/15 11:47:37 INFO streaming.PipeMapRed: R/W/S=100/102/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 11:47:37 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/15 11:47:37 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/15 11:47:37 INFO mapred.LocalJobRunner: \n",
      "15/09/15 11:47:37 INFO mapred.MapTask: Starting flush of map output\n",
      "15/09/15 11:47:37 INFO mapred.MapTask: Spilling map output\n",
      "15/09/15 11:47:37 INFO mapred.MapTask: bufstart = 0; bufend = 11916; bufvoid = 104857600\n",
      "15/09/15 11:47:37 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26213200(104852800); length = 1197/6553600\n",
      "15/09/15 11:47:37 INFO mapred.MapTask: Finished spill 0\n",
      "15/09/15 11:47:37 INFO mapred.Task: Task:attempt_local1058894346_0001_m_000000_0 is done. And is in the process of committing\n",
      "15/09/15 11:47:37 INFO mapred.LocalJobRunner: Records R/W=72/1\n",
      "15/09/15 11:47:37 INFO mapred.Task: Task 'attempt_local1058894346_0001_m_000000_0' done.\n",
      "15/09/15 11:47:37 INFO mapred.LocalJobRunner: Finishing task: attempt_local1058894346_0001_m_000000_0\n",
      "15/09/15 11:47:37 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "15/09/15 11:47:37 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "15/09/15 11:47:37 INFO mapred.LocalJobRunner: Starting task: attempt_local1058894346_0001_r_000000_0\n",
      "15/09/15 11:47:37 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/09/15 11:47:37 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@665db0a0\n",
      "15/09/15 11:47:37 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=333971456, maxSingleShuffleLimit=83492864, mergeThreshold=220421168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "15/09/15 11:47:37 INFO reduce.EventFetcher: attempt_local1058894346_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "15/09/15 11:47:37 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1058894346_0001_m_000000_0 decomp: 12518 len: 12522 to MEMORY\n",
      "15/09/15 11:47:37 INFO reduce.InMemoryMapOutput: Read 12518 bytes from map-output for attempt_local1058894346_0001_m_000000_0\n",
      "15/09/15 11:47:37 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 12518, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->12518\n",
      "15/09/15 11:47:37 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "15/09/15 11:47:37 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/15 11:47:37 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "15/09/15 11:47:37 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/15 11:47:37 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 12505 bytes\n",
      "15/09/15 11:47:37 INFO reduce.MergeManagerImpl: Merged 1 segments, 12518 bytes to disk to satisfy reduce memory limit\n",
      "15/09/15 11:47:37 INFO reduce.MergeManagerImpl: Merging 1 files, 12522 bytes from disk\n",
      "15/09/15 11:47:37 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "15/09/15 11:47:37 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/15 11:47:37 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 12505 bytes\n",
      "15/09/15 11:47:37 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/15 11:47:37 INFO streaming.PipeMapRed: PipeMapRed exec [/home/hadoop/w2/./reducer.py]\n",
      "15/09/15 11:47:37 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "15/09/15 11:47:37 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "15/09/15 11:47:38 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 11:47:38 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 11:47:38 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 11:47:38 INFO streaming.PipeMapRed: Records R/W=300/1\n",
      "15/09/15 11:47:38 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/15 11:47:38 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/15 11:47:38 INFO mapred.Task: Task:attempt_local1058894346_0001_r_000000_0 is done. And is in the process of committing\n",
      "15/09/15 11:47:38 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/15 11:47:38 INFO mapred.Task: Task attempt_local1058894346_0001_r_000000_0 is allowed to commit now\n",
      "15/09/15 11:47:38 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1058894346_0001_r_000000_0' to hdfs://localhost:54310/output/_temporary/0/task_local1058894346_0001_r_000000\n",
      "15/09/15 11:47:38 INFO mapred.LocalJobRunner: Records R/W=300/1 > reduce\n",
      "15/09/15 11:47:38 INFO mapred.Task: Task 'attempt_local1058894346_0001_r_000000_0' done.\n",
      "15/09/15 11:47:38 INFO mapred.LocalJobRunner: Finishing task: attempt_local1058894346_0001_r_000000_0\n",
      "15/09/15 11:47:38 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "15/09/15 11:47:38 INFO mapreduce.Job: Job job_local1058894346_0001 running in uber mode : false\n",
      "15/09/15 11:47:38 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "15/09/15 11:47:38 INFO mapreduce.Job: Job job_local1058894346_0001 completed successfully\n",
      "15/09/15 11:47:38 INFO mapreduce.Job: Counters: 38\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=235334\n",
      "\t\tFILE: Number of bytes written=760312\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=407958\n",
      "\t\tHDFS: Number of bytes written=2888\n",
      "\t\tHDFS: Number of read operations=15\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=300\n",
      "\t\tMap output bytes=11916\n",
      "\t\tMap output materialized bytes=12522\n",
      "\t\tInput split bytes=98\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=3\n",
      "\t\tReduce shuffle bytes=12522\n",
      "\t\tReduce input records=300\n",
      "\t\tReduce output records=101\n",
      "\t\tSpilled Records=600\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=46\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=571473920\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=203979\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2888\n",
      "15/09/15 11:47:38 INFO streaming.StreamJob: Output directory: /output/\n",
      "0010.2003-12-18.GP \t1 \t0\n",
      "0010.2001-06-28.SA_and_HP \t1 \t0\n",
      "0001.2000-01-17.beck \t0 \t0\n",
      "0018.1999-12-14.kaminski \t0 \t0\n",
      "0005.1999-12-12.kaminski \t0 \t0\n",
      "0011.2001-06-29.SA_and_HP \t1 \t0\n",
      "0008.2004-08-01.BG \t1 \t0\n",
      "0009.1999-12-14.farmer \t0 \t0\n",
      "0017.2003-12-18.GP \t1 \t0\n",
      "0011.2001-06-28.SA_and_HP \t1 \t0\n",
      "0015.2001-07-05.SA_and_HP \t1 \t0\n",
      "0015.2001-02-12.kitchen \t0 \t0\n",
      "0009.2001-06-26.SA_and_HP \t1 \t0\n",
      "0018.2001-07-13.SA_and_HP \t1 \t0\n",
      "0012.2000-01-17.beck \t0 \t0\n",
      "0003.2000-01-17.beck \t0 \t0\n",
      "0004.2001-06-12.SA_and_HP \t1 \t0\n",
      "0008.2001-06-12.SA_and_HP \t1 \t0\n",
      "0007.2001-02-09.kitchen \t0 \t0\n",
      "0016.2004-08-01.BG \t1 \t0\n",
      "0015.2000-06-09.lokay \t0 \t0\n",
      "0016.1999-12-15.farmer \t0 \t0\n",
      "0013.2004-08-01.BG \t1 \t0\n",
      "0005.2003-12-18.GP \t1 \t0\n",
      "0012.2001-02-09.kitchen \t0 \t0\n",
      "0011.1999-12-14.farmer \t0 \t0\n",
      "0009.2001-02-09.kitchen \t0 \t0\n",
      "0006.2001-02-08.kitchen \t0 \t0\n",
      "0014.2003-12-19.GP \t1 \t0\n",
      "0010.1999-12-14.farmer \t0 \t0\n",
      "0010.2004-08-01.BG \t1 \t0\n",
      "0014.1999-12-14.kaminski \t0 \t0\n",
      "0006.1999-12-13.kaminski \t0 \t0\n",
      "0005.1999-12-14.farmer \t0 \t0\n",
      "0003.2001-02-08.kitchen \t0 \t0\n",
      "0001.2001-02-07.kitchen \t0 \t0\n",
      "0008.2001-02-09.kitchen \t0 \t0\n",
      "0007.2003-12-18.GP \t1 \t0\n",
      "0017.2004-08-02.BG \t1 \t0\n",
      "0014.2004-08-01.BG \t1 \t0\n",
      "0006.2003-12-18.GP \t1 \t0\n",
      "0016.2001-07-05.SA_and_HP \t1 \t0\n",
      "0005.2000-06-06.lokay \t0 \t0\n",
      "0014.2001-07-04.SA_and_HP \t1 \t0\n",
      "0001.2001-04-02.williams \t0 \t0\n",
      "0012.2000-06-08.lokay \t0 \t0\n",
      "0014.1999-12-15.farmer \t0 \t0\n",
      "0009.2000-06-07.lokay \t0 \t0\n",
      "0001.1999-12-10.farmer \t0 \t0\n",
      "0008.2001-06-25.SA_and_HP \t1 \t0\n",
      "0017.2001-04-03.williams \t0 \t0\n",
      "0014.2001-02-12.kitchen \t0 \t0\n",
      "0016.2001-07-06.SA_and_HP \t1 \t0\n",
      "0015.1999-12-15.farmer \t0 \t0\n",
      "0009.1999-12-13.kaminski \t0 \t0\n",
      "0001.2000-06-06.lokay \t0 \t0\n",
      "0011.2004-08-01.BG \t1 \t0\n",
      "0004.2004-08-01.BG \t1 \t0\n",
      "0018.2003-12-18.GP \t1 \t0\n",
      "0007.1999-12-14.farmer \t0 \t0\n",
      "0016.2003-12-19.GP \t1 \t0\n",
      "0004.1999-12-14.farmer \t0 \t0\n",
      "0015.2003-12-19.GP \t1 \t0\n",
      "0006.2004-08-01.BG \t1 \t0\n",
      "0009.2003-12-18.GP \t1 \t0\n",
      "0002.1999-12-13.farmer \t0 \t0\n",
      "0008.2003-12-18.GP \t1 \t0\n",
      "0010.1999-12-14.kaminski \t0 \t0\n",
      "0007.2000-01-17.beck \t0 \t0\n",
      "0003.1999-12-14.farmer \t0 \t0\n",
      "0003.2004-08-01.BG \t1 \t0\n",
      "0017.2004-08-01.BG \t1 \t0\n",
      "0013.2001-06-30.SA_and_HP \t1 \t0\n",
      "0003.1999-12-10.kaminski \t0 \t0\n",
      "0012.1999-12-14.farmer \t0 \t0\n",
      "0004.1999-12-10.kaminski \t0 \t0\n",
      "0017.1999-12-14.kaminski \t0 \t0\n",
      "0002.2001-02-07.kitchen \t0 \t0\n",
      "0007.2004-08-01.BG \t1 \t0\n",
      "0012.1999-12-14.kaminski \t0 \t0\n",
      "0005.2001-06-23.SA_and_HP \t1 \t0\n",
      "0013.1999-12-14.kaminski \t0 \t0\n",
      "0007.1999-12-13.kaminski \t0 \t0\n",
      "0017.2000-01-17.beck \t0 \t0\n",
      "0006.2001-06-25.SA_and_HP \t1 \t0\n",
      "0006.2001-04-03.williams \t0 \t0\n",
      "0005.2001-02-08.kitchen \t0 \t0\n",
      "0002.2003-12-18.GP \t1 \t0\n",
      "0003.2003-12-18.GP \t1 \t0\n",
      "0013.2001-04-03.williams \t0 \t0\n",
      "0004.2001-04-02.williams \t0 \t0\n",
      "0010.2001-02-09.kitchen \t0 \t0\n",
      "0001.1999-12-10.kaminski \t0 \t0\n",
      "0013.1999-12-14.farmer \t0 \t0\n",
      "0015.1999-12-14.kaminski \t0 \t0\n",
      "0012.2003-12-19.GP \t1 \t0\n",
      "0016.2001-02-12.kitchen \t0 \t0\n",
      "0002.2004-08-01.BG \t1 \t0\n",
      "0002.2001-05-25.SA_and_HP \t1 \t0\n",
      "0011.2003-12-18.GP \t1 \t0\n",
      "Accuracy: 0.56\t\n"
     ]
    }
   ],
   "source": [
    "# Deleting folders from HDFS\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rm -r /input\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rm -r /output\n",
    "\n",
    "# Creating folder\n",
    "!/usr/local/hadoop/bin/hdfs dfs -mkdir -p /input\n",
    "\n",
    "# Copying input file for the job\n",
    "!/usr/local/hadoop/bin/hdfs dfs -put enronemail_1h.txt /input/\n",
    "\n",
    "!/usr/local/hadoop/bin/hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.6.0.jar -mapper mapper.py -reducer reducer.py -cmdenv vocab_input=assistance,valium,enlargementWithATypo -cmdenv drop_cols=no -input /input/* -output /output/\n",
    "#!/usr/local/hadoop/bin/hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.6.0.jar -mapper mapper.py -cmdenv vocab_input=assistance,valium,enlargementWithATypo -cmdenv drop_cols=no -input /input/* -output /output/\n",
    "\n",
    "#Reading output into a variable\n",
    "job_output = !/usr/local/hadoop/bin/hdfs dfs -cat /output/*\n",
    "\n",
    "# Printing output\n",
    "for line in job_output[1:]:\n",
    "    print line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm reports an accuracy of 0.62 with input terms - assistance,valium,enlargementWithATypo,him"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/15 11:48:08 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/15 11:48:08 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /input\n",
      "15/09/15 11:48:09 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/15 11:48:10 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /output\n",
      "15/09/15 11:48:11 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/15 11:48:12 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/15 11:48:14 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/15 11:48:14 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "15/09/15 11:48:14 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "15/09/15 11:48:14 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "15/09/15 11:48:14 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "15/09/15 11:48:15 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "15/09/15 11:48:15 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1799952260_0001\n",
      "15/09/15 11:48:15 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "15/09/15 11:48:15 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "15/09/15 11:48:15 INFO mapreduce.Job: Running job: job_local1799952260_0001\n",
      "15/09/15 11:48:15 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "15/09/15 11:48:15 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "15/09/15 11:48:15 INFO mapred.LocalJobRunner: Starting task: attempt_local1799952260_0001_m_000000_0\n",
      "15/09/15 11:48:15 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/09/15 11:48:15 INFO mapred.MapTask: Processing split: hdfs://localhost:54310/input/enronemail_1h.txt:0+203979\n",
      "15/09/15 11:48:15 INFO mapred.MapTask: numReduceTasks: 1\n",
      "15/09/15 11:48:15 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "15/09/15 11:48:15 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "15/09/15 11:48:15 INFO mapred.MapTask: soft limit at 83886080\n",
      "15/09/15 11:48:15 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "15/09/15 11:48:15 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "15/09/15 11:48:15 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "15/09/15 11:48:15 INFO streaming.PipeMapRed: PipeMapRed exec [/home/hadoop/w2/./mapper.py]\n",
      "15/09/15 11:48:15 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "15/09/15 11:48:15 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "15/09/15 11:48:15 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "15/09/15 11:48:15 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "15/09/15 11:48:15 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "15/09/15 11:48:15 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "15/09/15 11:48:15 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "15/09/15 11:48:15 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "15/09/15 11:48:15 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "15/09/15 11:48:15 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "15/09/15 11:48:15 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "15/09/15 11:48:15 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "15/09/15 11:48:15 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 11:48:15 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 11:48:15 INFO streaming.PipeMapRed: Records R/W=72/1\n",
      "15/09/15 11:48:15 INFO streaming.PipeMapRed: R/W/S=100/217/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 11:48:15 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/15 11:48:15 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/15 11:48:15 INFO mapred.LocalJobRunner: \n",
      "15/09/15 11:48:15 INFO mapred.MapTask: Starting flush of map output\n",
      "15/09/15 11:48:15 INFO mapred.MapTask: Spilling map output\n",
      "15/09/15 11:48:15 INFO mapred.MapTask: bufstart = 0; bufend = 14988; bufvoid = 104857600\n",
      "15/09/15 11:48:15 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26212800(104851200); length = 1597/6553600\n",
      "15/09/15 11:48:15 INFO mapred.MapTask: Finished spill 0\n",
      "15/09/15 11:48:15 INFO mapred.Task: Task:attempt_local1799952260_0001_m_000000_0 is done. And is in the process of committing\n",
      "15/09/15 11:48:15 INFO mapred.LocalJobRunner: Records R/W=72/1\n",
      "15/09/15 11:48:15 INFO mapred.Task: Task 'attempt_local1799952260_0001_m_000000_0' done.\n",
      "15/09/15 11:48:15 INFO mapred.LocalJobRunner: Finishing task: attempt_local1799952260_0001_m_000000_0\n",
      "15/09/15 11:48:15 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "15/09/15 11:48:15 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "15/09/15 11:48:15 INFO mapred.LocalJobRunner: Starting task: attempt_local1799952260_0001_r_000000_0\n",
      "15/09/15 11:48:15 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/09/15 11:48:15 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@10f70ce1\n",
      "15/09/15 11:48:15 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=333971456, maxSingleShuffleLimit=83492864, mergeThreshold=220421168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "15/09/15 11:48:15 INFO reduce.EventFetcher: attempt_local1799952260_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "15/09/15 11:48:15 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1799952260_0001_m_000000_0 decomp: 15790 len: 15794 to MEMORY\n",
      "15/09/15 11:48:15 INFO reduce.InMemoryMapOutput: Read 15790 bytes from map-output for attempt_local1799952260_0001_m_000000_0\n",
      "15/09/15 11:48:15 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 15790, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->15790\n",
      "15/09/15 11:48:15 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "15/09/15 11:48:15 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/15 11:48:15 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "15/09/15 11:48:15 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/15 11:48:15 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 15777 bytes\n",
      "15/09/15 11:48:15 INFO reduce.MergeManagerImpl: Merged 1 segments, 15790 bytes to disk to satisfy reduce memory limit\n",
      "15/09/15 11:48:15 INFO reduce.MergeManagerImpl: Merging 1 files, 15794 bytes from disk\n",
      "15/09/15 11:48:15 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "15/09/15 11:48:15 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/15 11:48:15 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 15777 bytes\n",
      "15/09/15 11:48:15 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/15 11:48:15 INFO streaming.PipeMapRed: PipeMapRed exec [/home/hadoop/w2/./reducer.py]\n",
      "15/09/15 11:48:15 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "15/09/15 11:48:15 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "15/09/15 11:48:15 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 11:48:15 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 11:48:15 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 11:48:16 INFO streaming.PipeMapRed: Records R/W=400/1\n",
      "15/09/15 11:48:16 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/15 11:48:16 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/15 11:48:16 INFO mapred.Task: Task:attempt_local1799952260_0001_r_000000_0 is done. And is in the process of committing\n",
      "15/09/15 11:48:16 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/15 11:48:16 INFO mapred.Task: Task attempt_local1799952260_0001_r_000000_0 is allowed to commit now\n",
      "15/09/15 11:48:16 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1799952260_0001_r_000000_0' to hdfs://localhost:54310/output/_temporary/0/task_local1799952260_0001_r_000000\n",
      "15/09/15 11:48:16 INFO mapred.LocalJobRunner: Records R/W=400/1 > reduce\n",
      "15/09/15 11:48:16 INFO mapred.Task: Task 'attempt_local1799952260_0001_r_000000_0' done.\n",
      "15/09/15 11:48:16 INFO mapred.LocalJobRunner: Finishing task: attempt_local1799952260_0001_r_000000_0\n",
      "15/09/15 11:48:16 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "15/09/15 11:48:16 INFO mapreduce.Job: Job job_local1799952260_0001 running in uber mode : false\n",
      "15/09/15 11:48:16 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "15/09/15 11:48:16 INFO mapreduce.Job: Job job_local1799952260_0001 completed successfully\n",
      "15/09/15 11:48:16 INFO mapreduce.Job: Counters: 38\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=241878\n",
      "\t\tFILE: Number of bytes written=770144\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=407958\n",
      "\t\tHDFS: Number of bytes written=2888\n",
      "\t\tHDFS: Number of read operations=15\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=400\n",
      "\t\tMap output bytes=14988\n",
      "\t\tMap output materialized bytes=15794\n",
      "\t\tInput split bytes=98\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=4\n",
      "\t\tReduce shuffle bytes=15794\n",
      "\t\tReduce input records=400\n",
      "\t\tReduce output records=101\n",
      "\t\tSpilled Records=800\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=19\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=571473920\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=203979\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2888\n",
      "15/09/15 11:48:16 INFO streaming.StreamJob: Output directory: /output/\n",
      "0010.2003-12-18.GP \t1 \t0\n",
      "0010.2001-06-28.SA_and_HP \t1 \t1\n",
      "0001.2000-01-17.beck \t0 \t0\n",
      "0018.1999-12-14.kaminski \t0 \t0\n",
      "0005.1999-12-12.kaminski \t0 \t1\n",
      "0011.2001-06-29.SA_and_HP \t1 \t0\n",
      "0008.2004-08-01.BG \t1 \t0\n",
      "0009.1999-12-14.farmer \t0 \t0\n",
      "0017.2003-12-18.GP \t1 \t0\n",
      "0011.2001-06-28.SA_and_HP \t1 \t1\n",
      "0015.2001-07-05.SA_and_HP \t1 \t0\n",
      "0015.2001-02-12.kitchen \t0 \t0\n",
      "0009.2001-06-26.SA_and_HP \t1 \t0\n",
      "0018.2001-07-13.SA_and_HP \t1 \t1\n",
      "0012.2000-01-17.beck \t0 \t0\n",
      "0003.2000-01-17.beck \t0 \t0\n",
      "0004.2001-06-12.SA_and_HP \t1 \t0\n",
      "0008.2001-06-12.SA_and_HP \t1 \t0\n",
      "0007.2001-02-09.kitchen \t0 \t0\n",
      "0016.2004-08-01.BG \t1 \t0\n",
      "0015.2000-06-09.lokay \t0 \t0\n",
      "0016.1999-12-15.farmer \t0 \t0\n",
      "0013.2004-08-01.BG \t1 \t1\n",
      "0005.2003-12-18.GP \t1 \t0\n",
      "0012.2001-02-09.kitchen \t0 \t0\n",
      "0011.1999-12-14.farmer \t0 \t0\n",
      "0009.2001-02-09.kitchen \t0 \t0\n",
      "0006.2001-02-08.kitchen \t0 \t0\n",
      "0014.2003-12-19.GP \t1 \t0\n",
      "0010.1999-12-14.farmer \t0 \t0\n",
      "0010.2004-08-01.BG \t1 \t0\n",
      "0014.1999-12-14.kaminski \t0 \t0\n",
      "0006.1999-12-13.kaminski \t0 \t0\n",
      "0005.1999-12-14.farmer \t0 \t0\n",
      "0003.2001-02-08.kitchen \t0 \t0\n",
      "0001.2001-02-07.kitchen \t0 \t0\n",
      "0008.2001-02-09.kitchen \t0 \t0\n",
      "0017.2004-08-02.BG \t1 \t0\n",
      "0007.2003-12-18.GP \t1 \t0\n",
      "0014.2004-08-01.BG \t1 \t0\n",
      "0006.2003-12-18.GP \t1 \t0\n",
      "0016.2001-07-05.SA_and_HP \t1 \t0\n",
      "0002.2004-08-01.BG \t1 \t1\n",
      "0014.2001-07-04.SA_and_HP \t1 \t0\n",
      "0001.2001-04-02.williams \t0 \t0\n",
      "0012.2000-06-08.lokay \t0 \t0\n",
      "0014.1999-12-15.farmer \t0 \t0\n",
      "0009.2000-06-07.lokay \t0 \t0\n",
      "0001.1999-12-10.farmer \t0 \t0\n",
      "0008.2001-06-25.SA_and_HP \t1 \t0\n",
      "0017.2001-04-03.williams \t0 \t0\n",
      "0014.2001-02-12.kitchen \t0 \t0\n",
      "0016.2001-07-06.SA_and_HP \t1 \t0\n",
      "0015.1999-12-15.farmer \t0 \t0\n",
      "0004.1999-12-10.kaminski \t0 \t1\n",
      "0001.2000-06-06.lokay \t0 \t0\n",
      "0011.2004-08-01.BG \t1 \t0\n",
      "0004.2004-08-01.BG \t1 \t0\n",
      "0018.2003-12-18.GP \t1 \t1\n",
      "0007.1999-12-14.farmer \t0 \t0\n",
      "0016.2003-12-19.GP \t1 \t0\n",
      "0004.1999-12-14.farmer \t0 \t0\n",
      "0015.2003-12-19.GP \t1 \t0\n",
      "0006.2004-08-01.BG \t1 \t0\n",
      "0009.2003-12-18.GP \t1 \t1\n",
      "0002.1999-12-13.farmer \t0 \t0\n",
      "0008.2003-12-18.GP \t1 \t0\n",
      "0010.1999-12-14.kaminski \t0 \t0\n",
      "0007.2000-01-17.beck \t0 \t0\n",
      "0003.1999-12-14.farmer \t0 \t0\n",
      "0003.2004-08-01.BG \t1 \t0\n",
      "0017.2004-08-01.BG \t1 \t1\n",
      "0013.2001-06-30.SA_and_HP \t1 \t0\n",
      "0003.1999-12-10.kaminski \t0 \t0\n",
      "0012.1999-12-14.farmer \t0 \t0\n",
      "0009.1999-12-13.kaminski \t0 \t0\n",
      "0017.1999-12-14.kaminski \t0 \t0\n",
      "0002.2001-02-07.kitchen \t0 \t0\n",
      "0007.2004-08-01.BG \t1 \t0\n",
      "0012.1999-12-14.kaminski \t0 \t0\n",
      "0005.2001-06-23.SA_and_HP \t1 \t0\n",
      "0005.2000-06-06.lokay \t0 \t0\n",
      "0013.1999-12-14.kaminski \t0 \t0\n",
      "0007.1999-12-13.kaminski \t0 \t0\n",
      "0017.2000-01-17.beck \t0 \t0\n",
      "0006.2001-06-25.SA_and_HP \t1 \t0\n",
      "0006.2001-04-03.williams \t0 \t0\n",
      "0005.2001-02-08.kitchen \t0 \t0\n",
      "0002.2003-12-18.GP \t1 \t0\n",
      "0003.2003-12-18.GP \t1 \t0\n",
      "0013.2001-04-03.williams \t0 \t0\n",
      "0004.2001-04-02.williams \t0 \t0\n",
      "0010.2001-02-09.kitchen \t0 \t0\n",
      "0001.1999-12-10.kaminski \t0 \t0\n",
      "0013.1999-12-14.farmer \t0 \t0\n",
      "0015.1999-12-14.kaminski \t0 \t0\n",
      "0012.2003-12-19.GP \t1 \t0\n",
      "0016.2001-02-12.kitchen \t0 \t0\n",
      "0002.2001-05-25.SA_and_HP \t1 \t0\n",
      "0011.2003-12-18.GP \t1 \t0\n",
      "Accuracy: 0.62\t\n"
     ]
    }
   ],
   "source": [
    "# Deleting folders from HDFS\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rm -r /input\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rm -r /output\n",
    "\n",
    "# Creating folder\n",
    "!/usr/local/hadoop/bin/hdfs dfs -mkdir -p /input\n",
    "\n",
    "# Copying input file for the job\n",
    "!/usr/local/hadoop/bin/hdfs dfs -put enronemail_1h.txt /input/\n",
    "\n",
    "!/usr/local/hadoop/bin/hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.6.0.jar -mapper mapper.py -reducer reducer.py -cmdenv vocab_input=assistance,valium,enlargementWithATypo,him -cmdenv drop_cols=no -input /input/* -output /output/\n",
    "\n",
    "#Reading output into a variable\n",
    "job_output = !/usr/local/hadoop/bin/hdfs dfs -cat /output/*\n",
    "\n",
    "# Printing output\n",
    "for line in job_output[1:]:\n",
    "    print line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#HW2.5  \n",
    "Using the Enron data from HW1 an in the  Hadoop MapReduce framework, write  a mapper/reducer for a multinomial Naive Bayes Classifier that will classify the email messages using  words present. Also drop words with a frequency of less than three (3). How does it affect the misclassifcation error of learnt naive multinomial Bayesian Classifiers on the training dataset:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result**  \n",
    "The algorithm reported an accuracy of 0.98. This is higher than the accurracy reported in HW1.5. This probabily due to the change in corpus (HW 1 uses entire email, while HW 2 only uses the email content).  \n",
    "I find no change in the accuracy when terms with a frequency of less than 3 are not dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mapper**  \n",
    "Using Mapper from **HW2.2** above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reducer**  \n",
    "Using Reducer from **HW2.3** above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm reported an accuracy of 0.98 when using all words and dropping terms with a frequency of less than 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/15 11:48:29 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/15 11:48:30 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /input\n",
      "15/09/15 11:48:31 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/15 11:48:31 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /output\n",
      "15/09/15 11:48:32 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/15 11:48:34 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/15 11:48:35 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/15 11:48:36 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "15/09/15 11:48:36 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "15/09/15 11:48:36 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "15/09/15 11:48:36 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "15/09/15 11:48:36 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "15/09/15 11:48:36 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1657829270_0001\n",
      "15/09/15 11:48:36 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "15/09/15 11:48:36 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "15/09/15 11:48:36 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "15/09/15 11:48:36 INFO mapreduce.Job: Running job: job_local1657829270_0001\n",
      "15/09/15 11:48:36 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "15/09/15 11:48:36 INFO mapred.LocalJobRunner: Starting task: attempt_local1657829270_0001_m_000000_0\n",
      "15/09/15 11:48:36 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/09/15 11:48:36 INFO mapred.MapTask: Processing split: hdfs://localhost:54310/input/enronemail_1h.txt:0+203979\n",
      "15/09/15 11:48:36 INFO mapred.MapTask: numReduceTasks: 1\n",
      "15/09/15 11:48:36 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "15/09/15 11:48:36 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "15/09/15 11:48:36 INFO mapred.MapTask: soft limit at 83886080\n",
      "15/09/15 11:48:36 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "15/09/15 11:48:36 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "15/09/15 11:48:36 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "15/09/15 11:48:36 INFO streaming.PipeMapRed: PipeMapRed exec [/home/hadoop/w2/./mapper.py]\n",
      "15/09/15 11:48:36 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "15/09/15 11:48:36 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "15/09/15 11:48:36 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "15/09/15 11:48:36 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "15/09/15 11:48:36 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "15/09/15 11:48:36 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "15/09/15 11:48:36 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "15/09/15 11:48:36 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "15/09/15 11:48:36 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "15/09/15 11:48:36 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "15/09/15 11:48:36 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "15/09/15 11:48:36 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "15/09/15 11:48:36 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 11:48:36 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 11:48:36 INFO streaming.PipeMapRed: Records R/W=72/1\n",
      "15/09/15 11:48:37 INFO streaming.PipeMapRed: R/W/S=100/4058/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 11:48:37 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/15 11:48:37 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/15 11:48:37 INFO mapred.LocalJobRunner: \n",
      "15/09/15 11:48:37 INFO mapred.MapTask: Starting flush of map output\n",
      "15/09/15 11:48:37 INFO mapred.MapTask: Spilling map output\n",
      "15/09/15 11:48:37 INFO mapred.MapTask: bufstart = 0; bufend = 486538; bufvoid = 104857600\n",
      "15/09/15 11:48:37 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26156868(104627472); length = 57529/6553600\n",
      "15/09/15 11:48:37 INFO mapred.MapTask: Finished spill 0\n",
      "15/09/15 11:48:37 INFO mapred.Task: Task:attempt_local1657829270_0001_m_000000_0 is done. And is in the process of committing\n",
      "15/09/15 11:48:37 INFO mapred.LocalJobRunner: Records R/W=72/1\n",
      "15/09/15 11:48:37 INFO mapred.Task: Task 'attempt_local1657829270_0001_m_000000_0' done.\n",
      "15/09/15 11:48:37 INFO mapred.LocalJobRunner: Finishing task: attempt_local1657829270_0001_m_000000_0\n",
      "15/09/15 11:48:37 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "15/09/15 11:48:37 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "15/09/15 11:48:37 INFO mapred.LocalJobRunner: Starting task: attempt_local1657829270_0001_r_000000_0\n",
      "15/09/15 11:48:37 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/09/15 11:48:37 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@665db0a0\n",
      "15/09/15 11:48:37 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=333971456, maxSingleShuffleLimit=83492864, mergeThreshold=220421168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "15/09/15 11:48:37 INFO reduce.EventFetcher: attempt_local1657829270_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "15/09/15 11:48:37 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1657829270_0001_m_000000_0 decomp: 515306 len: 515310 to MEMORY\n",
      "15/09/15 11:48:37 INFO reduce.InMemoryMapOutput: Read 515306 bytes from map-output for attempt_local1657829270_0001_m_000000_0\n",
      "15/09/15 11:48:37 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 515306, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->515306\n",
      "15/09/15 11:48:37 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "15/09/15 11:48:37 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/15 11:48:37 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "15/09/15 11:48:37 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/15 11:48:37 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 515302 bytes\n",
      "15/09/15 11:48:37 INFO reduce.MergeManagerImpl: Merged 1 segments, 515306 bytes to disk to satisfy reduce memory limit\n",
      "15/09/15 11:48:37 INFO reduce.MergeManagerImpl: Merging 1 files, 515310 bytes from disk\n",
      "15/09/15 11:48:37 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "15/09/15 11:48:37 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/15 11:48:37 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 515302 bytes\n",
      "15/09/15 11:48:37 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/15 11:48:37 INFO streaming.PipeMapRed: PipeMapRed exec [/home/hadoop/w2/./reducer.py]\n",
      "15/09/15 11:48:37 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "15/09/15 11:48:37 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "15/09/15 11:48:37 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 11:48:37 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 11:48:37 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 11:48:37 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 11:48:37 INFO mapreduce.Job: Job job_local1657829270_0001 running in uber mode : false\n",
      "15/09/15 11:48:37 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "15/09/15 11:48:37 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 11:48:42 INFO streaming.PipeMapRed: Records R/W=14383/1\n",
      "15/09/15 11:48:43 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/15 11:48:43 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/15 11:48:43 INFO mapred.Task: Task:attempt_local1657829270_0001_r_000000_0 is done. And is in the process of committing\n",
      "15/09/15 11:48:43 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/15 11:48:43 INFO mapred.Task: Task attempt_local1657829270_0001_r_000000_0 is allowed to commit now\n",
      "15/09/15 11:48:43 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1657829270_0001_r_000000_0' to hdfs://localhost:54310/output/_temporary/0/task_local1657829270_0001_r_000000\n",
      "15/09/15 11:48:43 INFO mapred.LocalJobRunner: Records R/W=14383/1 > reduce\n",
      "15/09/15 11:48:43 INFO mapred.Task: Task 'attempt_local1657829270_0001_r_000000_0' done.\n",
      "15/09/15 11:48:43 INFO mapred.LocalJobRunner: Finishing task: attempt_local1657829270_0001_r_000000_0\n",
      "15/09/15 11:48:43 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "15/09/15 11:48:43 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "15/09/15 11:48:43 INFO mapreduce.Job: Job job_local1657829270_0001 completed successfully\n",
      "15/09/15 11:48:43 INFO mapreduce.Job: Counters: 38\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1240910\n",
      "\t\tFILE: Number of bytes written=2268532\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=407958\n",
      "\t\tHDFS: Number of bytes written=2888\n",
      "\t\tHDFS: Number of read operations=15\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=14383\n",
      "\t\tMap output bytes=486538\n",
      "\t\tMap output materialized bytes=515310\n",
      "\t\tInput split bytes=98\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=5016\n",
      "\t\tReduce shuffle bytes=515310\n",
      "\t\tReduce input records=14383\n",
      "\t\tReduce output records=101\n",
      "\t\tSpilled Records=28766\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=15\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=571473920\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=203979\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2888\n",
      "15/09/15 11:48:43 INFO streaming.StreamJob: Output directory: /output/\n",
      "0010.2003-12-18.GP \t1 \t0\n",
      "0010.2001-06-28.SA_and_HP \t1 \t1\n",
      "0001.2000-01-17.beck \t0 \t0\n",
      "0018.1999-12-14.kaminski \t0 \t0\n",
      "0005.1999-12-12.kaminski \t0 \t0\n",
      "0011.2001-06-29.SA_and_HP \t1 \t1\n",
      "0008.2004-08-01.BG \t1 \t1\n",
      "0009.1999-12-14.farmer \t0 \t0\n",
      "0017.2003-12-18.GP \t1 \t1\n",
      "0011.2001-06-28.SA_and_HP \t1 \t1\n",
      "0015.2001-07-05.SA_and_HP \t1 \t1\n",
      "0015.2001-02-12.kitchen \t0 \t0\n",
      "0009.2001-06-26.SA_and_HP \t1 \t1\n",
      "0017.1999-12-14.kaminski \t0 \t0\n",
      "0012.2000-01-17.beck \t0 \t0\n",
      "0003.2000-01-17.beck \t0 \t0\n",
      "0004.2001-06-12.SA_and_HP \t1 \t1\n",
      "0008.2001-06-12.SA_and_HP \t1 \t1\n",
      "0007.2001-02-09.kitchen \t0 \t0\n",
      "0016.2004-08-01.BG \t1 \t1\n",
      "0015.2000-06-09.lokay \t0 \t0\n",
      "0016.1999-12-15.farmer \t0 \t0\n",
      "0013.2004-08-01.BG \t1 \t1\n",
      "0005.2003-12-18.GP \t1 \t1\n",
      "0012.2001-02-09.kitchen \t0 \t0\n",
      "0011.1999-12-14.farmer \t0 \t0\n",
      "0009.2001-02-09.kitchen \t0 \t0\n",
      "0006.2001-02-08.kitchen \t0 \t0\n",
      "0014.2003-12-19.GP \t1 \t1\n",
      "0010.1999-12-14.farmer \t0 \t0\n",
      "0010.2004-08-01.BG \t1 \t1\n",
      "0014.1999-12-14.kaminski \t0 \t0\n",
      "0006.1999-12-13.kaminski \t0 \t0\n",
      "0005.1999-12-14.farmer \t0 \t0\n",
      "0003.2001-02-08.kitchen \t0 \t0\n",
      "0001.2001-02-07.kitchen \t0 \t0\n",
      "0008.2001-02-09.kitchen \t0 \t0\n",
      "0007.2003-12-18.GP \t1 \t1\n",
      "0017.2004-08-02.BG \t1 \t1\n",
      "0014.2004-08-01.BG \t1 \t1\n",
      "0006.2003-12-18.GP \t1 \t1\n",
      "0016.2001-07-05.SA_and_HP \t1 \t1\n",
      "0008.2003-12-18.GP \t1 \t1\n",
      "0014.2001-07-04.SA_and_HP \t1 \t1\n",
      "0001.2001-04-02.williams \t0 \t0\n",
      "0012.2000-06-08.lokay \t0 \t0\n",
      "0014.1999-12-15.farmer \t0 \t0\n",
      "0009.2000-06-07.lokay \t0 \t0\n",
      "0001.1999-12-10.farmer \t0 \t0\n",
      "0008.2001-06-25.SA_and_HP \t1 \t1\n",
      "0017.2001-04-03.williams \t0 \t0\n",
      "0014.2001-02-12.kitchen \t0 \t0\n",
      "0016.2001-07-06.SA_and_HP \t1 \t1\n",
      "0015.1999-12-15.farmer \t0 \t0\n",
      "0004.1999-12-10.kaminski \t0 \t0\n",
      "0001.2000-06-06.lokay \t0 \t0\n",
      "0011.2004-08-01.BG \t1 \t1\n",
      "0004.2004-08-01.BG \t1 \t1\n",
      "0018.2003-12-18.GP \t1 \t1\n",
      "0002.1999-12-13.farmer \t0 \t0\n",
      "0016.2003-12-19.GP \t1 \t1\n",
      "0004.1999-12-14.farmer \t0 \t0\n",
      "0015.2003-12-19.GP \t1 \t1\n",
      "0006.2004-08-01.BG \t1 \t1\n",
      "0009.2003-12-18.GP \t1 \t1\n",
      "0007.1999-12-14.farmer \t0 \t0\n",
      "0002.2004-08-01.BG \t1 \t1\n",
      "0010.1999-12-14.kaminski \t0 \t0\n",
      "0007.2000-01-17.beck \t0 \t0\n",
      "0003.1999-12-14.farmer \t0 \t0\n",
      "0003.2004-08-01.BG \t1 \t1\n",
      "0017.2004-08-01.BG \t1 \t1\n",
      "0013.2001-06-30.SA_and_HP \t1 \t1\n",
      "0003.1999-12-10.kaminski \t0 \t0\n",
      "0012.1999-12-14.farmer \t0 \t0\n",
      "0009.1999-12-13.kaminski \t0 \t0\n",
      "0018.2001-07-13.SA_and_HP \t1 \t1\n",
      "0002.2001-02-07.kitchen \t0 \t0\n",
      "0007.2004-08-01.BG \t1 \t1\n",
      "0012.1999-12-14.kaminski \t0 \t0\n",
      "0005.2001-06-23.SA_and_HP \t1 \t1\n",
      "0005.2000-06-06.lokay \t0 \t0\n",
      "0013.1999-12-14.kaminski \t0 \t0\n",
      "0007.1999-12-13.kaminski \t0 \t0\n",
      "0017.2000-01-17.beck \t0 \t0\n",
      "0006.2001-06-25.SA_and_HP \t1 \t1\n",
      "0006.2001-04-03.williams \t0 \t0\n",
      "0005.2001-02-08.kitchen \t0 \t0\n",
      "0002.2003-12-18.GP \t1 \t1\n",
      "0003.2003-12-18.GP \t1 \t1\n",
      "0013.2001-04-03.williams \t0 \t0\n",
      "0004.2001-04-02.williams \t0 \t0\n",
      "0010.2001-02-09.kitchen \t0 \t0\n",
      "0001.1999-12-10.kaminski \t0 \t1\n",
      "0013.1999-12-14.farmer \t0 \t0\n",
      "0015.1999-12-14.kaminski \t0 \t0\n",
      "0012.2003-12-19.GP \t1 \t1\n",
      "0016.2001-02-12.kitchen \t0 \t0\n",
      "0002.2001-05-25.SA_and_HP \t1 \t1\n",
      "0011.2003-12-18.GP \t1 \t1\n",
      "Accuracy: 0.98\t\n"
     ]
    }
   ],
   "source": [
    "# Deleting folders from HDFS\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rm -r /input\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rm -r /output\n",
    "\n",
    "# Creating folder\n",
    "!/usr/local/hadoop/bin/hdfs dfs -mkdir -p /input\n",
    "\n",
    "# Copying input file for the job\n",
    "!/usr/local/hadoop/bin/hdfs dfs -put enronemail_1h.txt /input/\n",
    "\n",
    "!/usr/local/hadoop/bin/hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.6.0.jar -mapper mapper.py -reducer reducer.py -cmdenv vocab_input=* -cmdenv drop_cols=yes -input /input/* -output /output/\n",
    "\n",
    "#Reading output into a variable\n",
    "job_output = !/usr/local/hadoop/bin/hdfs dfs -cat /output/*\n",
    "\n",
    "# Printing output\n",
    "for line in job_output[1:]:\n",
    "    print line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm reported an accuracy of 0.98 when using all words and not dropping terms with a frequency of less than 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/15 11:48:51 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/15 11:48:51 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /input\n",
      "15/09/15 11:48:52 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/15 11:48:52 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /output\n",
      "15/09/15 11:48:53 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/15 11:48:55 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/15 11:48:56 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/15 11:48:57 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "15/09/15 11:48:57 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "15/09/15 11:48:57 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "15/09/15 11:48:57 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "15/09/15 11:48:57 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "15/09/15 11:48:57 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1999419019_0001\n",
      "15/09/15 11:48:57 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "15/09/15 11:48:57 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "15/09/15 11:48:57 INFO mapreduce.Job: Running job: job_local1999419019_0001\n",
      "15/09/15 11:48:57 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "15/09/15 11:48:57 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "15/09/15 11:48:58 INFO mapred.LocalJobRunner: Starting task: attempt_local1999419019_0001_m_000000_0\n",
      "15/09/15 11:48:58 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/09/15 11:48:58 INFO mapred.MapTask: Processing split: hdfs://localhost:54310/input/enronemail_1h.txt:0+203979\n",
      "15/09/15 11:48:58 INFO mapred.MapTask: numReduceTasks: 1\n",
      "15/09/15 11:48:58 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "15/09/15 11:48:58 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "15/09/15 11:48:58 INFO mapred.MapTask: soft limit at 83886080\n",
      "15/09/15 11:48:58 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "15/09/15 11:48:58 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "15/09/15 11:48:58 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "15/09/15 11:48:58 INFO streaming.PipeMapRed: PipeMapRed exec [/home/hadoop/w2/./mapper.py]\n",
      "15/09/15 11:48:58 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "15/09/15 11:48:58 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "15/09/15 11:48:58 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "15/09/15 11:48:58 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "15/09/15 11:48:58 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "15/09/15 11:48:58 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "15/09/15 11:48:58 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "15/09/15 11:48:58 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "15/09/15 11:48:58 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "15/09/15 11:48:58 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "15/09/15 11:48:58 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "15/09/15 11:48:58 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "15/09/15 11:48:58 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 11:48:58 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 11:48:58 INFO streaming.PipeMapRed: Records R/W=72/1\n",
      "15/09/15 11:48:58 INFO streaming.PipeMapRed: R/W/S=100/2830/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 11:48:58 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/15 11:48:58 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/15 11:48:58 INFO mapred.LocalJobRunner: \n",
      "15/09/15 11:48:58 INFO mapred.MapTask: Starting flush of map output\n",
      "15/09/15 11:48:58 INFO mapred.MapTask: Spilling map output\n",
      "15/09/15 11:48:58 INFO mapred.MapTask: bufstart = 0; bufend = 486538; bufvoid = 104857600\n",
      "15/09/15 11:48:58 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26156868(104627472); length = 57529/6553600\n",
      "15/09/15 11:48:58 INFO mapred.MapTask: Finished spill 0\n",
      "15/09/15 11:48:58 INFO mapred.Task: Task:attempt_local1999419019_0001_m_000000_0 is done. And is in the process of committing\n",
      "15/09/15 11:48:58 INFO mapred.LocalJobRunner: Records R/W=72/1\n",
      "15/09/15 11:48:58 INFO mapred.Task: Task 'attempt_local1999419019_0001_m_000000_0' done.\n",
      "15/09/15 11:48:58 INFO mapred.LocalJobRunner: Finishing task: attempt_local1999419019_0001_m_000000_0\n",
      "15/09/15 11:48:58 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "15/09/15 11:48:58 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "15/09/15 11:48:58 INFO mapred.LocalJobRunner: Starting task: attempt_local1999419019_0001_r_000000_0\n",
      "15/09/15 11:48:58 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/09/15 11:48:58 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@67ac602\n",
      "15/09/15 11:48:58 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=333971456, maxSingleShuffleLimit=83492864, mergeThreshold=220421168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "15/09/15 11:48:58 INFO reduce.EventFetcher: attempt_local1999419019_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "15/09/15 11:48:58 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1999419019_0001_m_000000_0 decomp: 515306 len: 515310 to MEMORY\n",
      "15/09/15 11:48:58 INFO reduce.InMemoryMapOutput: Read 515306 bytes from map-output for attempt_local1999419019_0001_m_000000_0\n",
      "15/09/15 11:48:58 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 515306, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->515306\n",
      "15/09/15 11:48:58 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "15/09/15 11:48:58 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/15 11:48:58 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "15/09/15 11:48:58 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/15 11:48:58 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 515302 bytes\n",
      "15/09/15 11:48:58 INFO reduce.MergeManagerImpl: Merged 1 segments, 515306 bytes to disk to satisfy reduce memory limit\n",
      "15/09/15 11:48:58 INFO reduce.MergeManagerImpl: Merging 1 files, 515310 bytes from disk\n",
      "15/09/15 11:48:58 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "15/09/15 11:48:58 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/15 11:48:58 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 515302 bytes\n",
      "15/09/15 11:48:58 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/15 11:48:58 INFO streaming.PipeMapRed: PipeMapRed exec [/home/hadoop/w2/./reducer.py]\n",
      "15/09/15 11:48:58 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "15/09/15 11:48:58 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "15/09/15 11:48:58 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 11:48:58 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 11:48:58 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 11:48:58 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 11:48:58 INFO mapreduce.Job: Job job_local1999419019_0001 running in uber mode : false\n",
      "15/09/15 11:48:58 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "15/09/15 11:48:58 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 11:49:04 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "15/09/15 11:49:04 INFO streaming.PipeMapRed: Records R/W=14383/1\n",
      "15/09/15 11:49:04 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/15 11:49:04 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/15 11:49:04 INFO mapred.Task: Task:attempt_local1999419019_0001_r_000000_0 is done. And is in the process of committing\n",
      "15/09/15 11:49:04 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "15/09/15 11:49:04 INFO mapred.Task: Task attempt_local1999419019_0001_r_000000_0 is allowed to commit now\n",
      "15/09/15 11:49:04 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1999419019_0001_r_000000_0' to hdfs://localhost:54310/output/_temporary/0/task_local1999419019_0001_r_000000\n",
      "15/09/15 11:49:04 INFO mapred.LocalJobRunner: Records R/W=14383/1 > reduce\n",
      "15/09/15 11:49:04 INFO mapred.Task: Task 'attempt_local1999419019_0001_r_000000_0' done.\n",
      "15/09/15 11:49:04 INFO mapred.LocalJobRunner: Finishing task: attempt_local1999419019_0001_r_000000_0\n",
      "15/09/15 11:49:04 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "15/09/15 11:49:04 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "15/09/15 11:49:04 INFO mapreduce.Job: Job job_local1999419019_0001 completed successfully\n",
      "15/09/15 11:49:04 INFO mapreduce.Job: Counters: 38\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1240910\n",
      "\t\tFILE: Number of bytes written=2268528\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=407958\n",
      "\t\tHDFS: Number of bytes written=2888\n",
      "\t\tHDFS: Number of read operations=15\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=14383\n",
      "\t\tMap output bytes=486538\n",
      "\t\tMap output materialized bytes=515310\n",
      "\t\tInput split bytes=98\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=5016\n",
      "\t\tReduce shuffle bytes=515310\n",
      "\t\tReduce input records=14383\n",
      "\t\tReduce output records=101\n",
      "\t\tSpilled Records=28766\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=13\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=571473920\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=203979\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2888\n",
      "15/09/15 11:49:04 INFO streaming.StreamJob: Output directory: /output/\n",
      "0010.2003-12-18.GP \t1 \t0\n",
      "0010.2001-06-28.SA_and_HP \t1 \t1\n",
      "0001.2000-01-17.beck \t0 \t0\n",
      "0018.1999-12-14.kaminski \t0 \t0\n",
      "0005.1999-12-12.kaminski \t0 \t0\n",
      "0011.2001-06-29.SA_and_HP \t1 \t1\n",
      "0008.2004-08-01.BG \t1 \t1\n",
      "0009.1999-12-14.farmer \t0 \t0\n",
      "0017.2003-12-18.GP \t1 \t1\n",
      "0011.2001-06-28.SA_and_HP \t1 \t1\n",
      "0015.2001-07-05.SA_and_HP \t1 \t1\n",
      "0015.2001-02-12.kitchen \t0 \t0\n",
      "0009.2001-06-26.SA_and_HP \t1 \t1\n",
      "0017.1999-12-14.kaminski \t0 \t0\n",
      "0012.2000-01-17.beck \t0 \t0\n",
      "0003.2000-01-17.beck \t0 \t0\n",
      "0004.2001-06-12.SA_and_HP \t1 \t1\n",
      "0008.2001-06-12.SA_and_HP \t1 \t1\n",
      "0007.2001-02-09.kitchen \t0 \t0\n",
      "0016.2004-08-01.BG \t1 \t1\n",
      "0015.2000-06-09.lokay \t0 \t0\n",
      "0016.1999-12-15.farmer \t0 \t0\n",
      "0013.2004-08-01.BG \t1 \t1\n",
      "0005.2003-12-18.GP \t1 \t1\n",
      "0012.2001-02-09.kitchen \t0 \t0\n",
      "0011.1999-12-14.farmer \t0 \t0\n",
      "0009.2001-02-09.kitchen \t0 \t0\n",
      "0006.2001-02-08.kitchen \t0 \t0\n",
      "0014.2003-12-19.GP \t1 \t1\n",
      "0010.1999-12-14.farmer \t0 \t0\n",
      "0010.2004-08-01.BG \t1 \t1\n",
      "0014.1999-12-14.kaminski \t0 \t0\n",
      "0006.1999-12-13.kaminski \t0 \t0\n",
      "0005.1999-12-14.farmer \t0 \t0\n",
      "0003.2001-02-08.kitchen \t0 \t0\n",
      "0001.2001-02-07.kitchen \t0 \t0\n",
      "0008.2001-02-09.kitchen \t0 \t0\n",
      "0007.2003-12-18.GP \t1 \t1\n",
      "0017.2004-08-02.BG \t1 \t1\n",
      "0014.2004-08-01.BG \t1 \t1\n",
      "0006.2003-12-18.GP \t1 \t1\n",
      "0016.2001-07-05.SA_and_HP \t1 \t1\n",
      "0008.2003-12-18.GP \t1 \t1\n",
      "0014.2001-07-04.SA_and_HP \t1 \t1\n",
      "0001.2001-04-02.williams \t0 \t0\n",
      "0012.2000-06-08.lokay \t0 \t0\n",
      "0014.1999-12-15.farmer \t0 \t0\n",
      "0009.2000-06-07.lokay \t0 \t0\n",
      "0001.1999-12-10.farmer \t0 \t0\n",
      "0008.2001-06-25.SA_and_HP \t1 \t1\n",
      "0017.2001-04-03.williams \t0 \t0\n",
      "0014.2001-02-12.kitchen \t0 \t0\n",
      "0016.2001-07-06.SA_and_HP \t1 \t1\n",
      "0015.1999-12-15.farmer \t0 \t0\n",
      "0004.1999-12-10.kaminski \t0 \t0\n",
      "0001.2000-06-06.lokay \t0 \t0\n",
      "0011.2004-08-01.BG \t1 \t1\n",
      "0004.2004-08-01.BG \t1 \t1\n",
      "0018.2003-12-18.GP \t1 \t1\n",
      "0002.1999-12-13.farmer \t0 \t0\n",
      "0016.2003-12-19.GP \t1 \t1\n",
      "0004.1999-12-14.farmer \t0 \t0\n",
      "0015.2003-12-19.GP \t1 \t1\n",
      "0006.2004-08-01.BG \t1 \t1\n",
      "0009.2003-12-18.GP \t1 \t1\n",
      "0007.1999-12-14.farmer \t0 \t0\n",
      "0002.2004-08-01.BG \t1 \t1\n",
      "0010.1999-12-14.kaminski \t0 \t0\n",
      "0007.2000-01-17.beck \t0 \t0\n",
      "0003.1999-12-14.farmer \t0 \t0\n",
      "0003.2004-08-01.BG \t1 \t1\n",
      "0017.2004-08-01.BG \t1 \t1\n",
      "0013.2001-06-30.SA_and_HP \t1 \t1\n",
      "0003.1999-12-10.kaminski \t0 \t0\n",
      "0012.1999-12-14.farmer \t0 \t0\n",
      "0009.1999-12-13.kaminski \t0 \t0\n",
      "0018.2001-07-13.SA_and_HP \t1 \t1\n",
      "0002.2001-02-07.kitchen \t0 \t0\n",
      "0007.2004-08-01.BG \t1 \t1\n",
      "0012.1999-12-14.kaminski \t0 \t0\n",
      "0005.2001-06-23.SA_and_HP \t1 \t1\n",
      "0005.2000-06-06.lokay \t0 \t0\n",
      "0013.1999-12-14.kaminski \t0 \t0\n",
      "0007.1999-12-13.kaminski \t0 \t0\n",
      "0017.2000-01-17.beck \t0 \t0\n",
      "0006.2001-06-25.SA_and_HP \t1 \t1\n",
      "0006.2001-04-03.williams \t0 \t0\n",
      "0005.2001-02-08.kitchen \t0 \t0\n",
      "0002.2003-12-18.GP \t1 \t1\n",
      "0003.2003-12-18.GP \t1 \t1\n",
      "0013.2001-04-03.williams \t0 \t0\n",
      "0004.2001-04-02.williams \t0 \t0\n",
      "0010.2001-02-09.kitchen \t0 \t0\n",
      "0001.1999-12-10.kaminski \t0 \t1\n",
      "0013.1999-12-14.farmer \t0 \t0\n",
      "0015.1999-12-14.kaminski \t0 \t0\n",
      "0012.2003-12-19.GP \t1 \t1\n",
      "0016.2001-02-12.kitchen \t0 \t0\n",
      "0002.2001-05-25.SA_and_HP \t1 \t1\n",
      "0011.2003-12-18.GP \t1 \t1\n",
      "Accuracy: 0.98\t\n"
     ]
    }
   ],
   "source": [
    "# Deleting folders from HDFS\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rm -r /input\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rm -r /output\n",
    "\n",
    "# Creating folder\n",
    "!/usr/local/hadoop/bin/hdfs dfs -mkdir -p /input\n",
    "\n",
    "# Copying input file for the job\n",
    "!/usr/local/hadoop/bin/hdfs dfs -put enronemail_1h.txt /input/\n",
    "\n",
    "!/usr/local/hadoop/bin/hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.6.0.jar -mapper mapper.py -reducer reducer.py -cmdenv vocab_input=* -cmdenv drop_cols=no -input /input/* -output /output/\n",
    "\n",
    "#Reading output into a variable\n",
    "job_output = !/usr/local/hadoop/bin/hdfs dfs -cat /output/*\n",
    "\n",
    "# Printing output\n",
    "for line in job_output[1:]:\n",
    "    print line"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
