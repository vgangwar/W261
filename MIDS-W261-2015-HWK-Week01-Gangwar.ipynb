{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DATASCI W261: Machine Learning at Scale  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Vineet Gangwar\n",
    "**vineet.gangwar@gmail.com **  \n",
    "**W261-2: Machine Learning at Scale**  \n",
    "**Assignment #1**   \n",
    "**Date: Sep - 15 - 2015**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#HW1.0.0  \n",
    "Define big data. Provide an example of a big data problem in your domain of expertise  \n",
    "  \n",
    "**Answer:**  \n",
    "*Big data definition*  \n",
    "Big data refers to the following:\n",
    "- Data so large that traditional application are inadequate for both storage and analysis\n",
    "- Data that is large enough to require more than one machine to store and process it\n",
    "\n",
    "*Example of Big Data*  \n",
    "My domain is Enterprise Monitoring. In this domain, we monitor parameters of running IT systems such as operating systems, networks, applications) and generate alerts based on thresholds. The parameters monitored include: CPU, Memory, Disk, Network, logs and many more. The parameter list can go into hundreds.  \n",
    "Apart from alerting, we also need to store historical data of all monitored parameters. An organization with a few thousand machines, can generate a few terabytes of data per day. Storing this data and making it available for efficient historical analysis is a big data problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#HW1.0.1  \n",
    "In 500 words (English or pseudo code or a combination) describe how to estimate the bias, the variance, the irreduciable error for a test dataset T when using polynomial regression models of degree 1, 2,3, 4,5 are considered. How would you select a model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**  \n",
    "Statistical models attempt to model reality. So there will be an error between the model and reality (true function). The error has 2 parts - Irreducible and Reducible. The Reducible error also has two parts - squared bias and variance.  \n",
    "\n",
    "Calculation of Squared Bias and Variance:  \n",
    "> If we take 100 different training dataset with 50 observations each, then we can estimate 100 different models.  \n",
    "\n",
    "Error due to squared bias is the expected error (of these 100 models) where the error of each model is calculated as the error between the prediction and the y in the training data\n",
    "\n",
    "Error due to variance is the expected variance in the predictions of these 100 models against a test dataset.\n",
    "\n",
    "As the flexibility of a statistical model increases, the variance increases and bias decreases. The bias decreases because the flexible model starts to learn the noise in the training datasets.\n",
    "\n",
    "> Method to select a polynomial regression models of degree 1, 2, 3, 4, 5\n",
    "\n",
    "- Choose m different training datasets with n datasets each\n",
    "- For each of the m training datasets, generate 5 models of degrees 1 to 5\n",
    "- For each 'degree',  calculate expected squared bias and expected variance\n",
    "- Choose the 'degree' that generated the least reducible error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#HW1.1\n",
    "Read through the provided control script (pNaiveBayes.sh) and all of its comments. When you are comfortable with their purpose and function, respond to the remaining homework questions below. A simple cell in the notebook with a print statmement with  a \"done\" string will suffice here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print 'Done'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#HW1.2\n",
    "Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will determine the number of occurrences of a single, user-specified word. Examine the word “assistance” and report your results.  \n",
    "\n",
    "To do so, make sure that\n",
    "   \n",
    "   - mapper.py counts all occurrences of a single word, and\n",
    "   - reducer.py collates the counts of the single word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mapper**  \n",
    "The mapper uses words from just the email message/content. This is as per Jake's suggestion in the Google Groups. As a result it outputs 9 occurrences for the words assistance.\n",
    "\n",
    "Interface:  \n",
    "*Input*: The mapper takes a filename and a list of words as inputs  \n",
    "*Output*: It outputs word, word count, email id and TRUTH for each input word in separate lines  \n",
    "\n",
    "The map reduce code below can handle single words, multiple words and also \\* for all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/home/vineetgangwar/anaconda/bin/python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "# Reading file into memory and converting into lowercase\n",
    "filename = sys.argv[1]\n",
    "filetxt = str()\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for line in myfile:\n",
    "        # Splitting each line/email based on tab\n",
    "        fields = line.strip().split('\\t')\n",
    "        email_id = fields[0]\n",
    "        truth = fields[1]\n",
    "        if len(fields) == 4:\n",
    "            subject = fields[2]\n",
    "            message = fields[3]\n",
    "        else:\n",
    "            subject = \"\"\n",
    "            message = fields[2]\n",
    "        filetxt = filetxt + ' ' + message\n",
    "# ====\n",
    "\n",
    "# This function returns words for which to count occurrences of\n",
    "def wordlist():\n",
    "    inputlist = sys.argv[2].lower()\n",
    "    if inputlist == '*':        # If * then using all words from the input file\n",
    "        file_aplhanum = re.sub('[^a-z]', ' ', filetxt)    # Converting non alpha characters to space\n",
    "        words = set(file_aplhanum.split())\n",
    "    else:\n",
    "    \twords = inputlist.split()                         # Splitting list of words input by the user\n",
    "    return words \n",
    "\n",
    "def main():\n",
    "    for word in wordlist():\n",
    "        #Reading email file line by line where each line is an email\n",
    "        with open(filename, 'r') as myfile:\n",
    "            for line in myfile:\n",
    "                # Splitting each line/email based on tab\n",
    "                fields = line.strip().split('\\t')\n",
    "                email_id = fields[0]\n",
    "                truth = fields[1]\n",
    "                if len(fields) == 4:\n",
    "                    subject = fields[2]\n",
    "                    message = fields[3]\n",
    "                else:\n",
    "                    subject = \"\"\n",
    "                    message = fields[2]\n",
    "                # Printing word, count, email_id and Truth\n",
    "                print word, message.lower().count(word), email_id, truth\n",
    "\n",
    "if __name__ == '__main__':\n",
    "        main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reducer**  \n",
    "The reducer first reads all intermediate files and accumulates all the mapper output data in a list object. It then loops through the list and does two things:\n",
    "- Uses the words in the list to create dictionary keys\n",
    "- Uses the word_counts in the list to increment the dictionary values where the word == dictionary key\n",
    "\n",
    "It then prints out the dictionary which gives words and their corresponding counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/home/vineetgangwar/anaconda/bin/python\n",
    "import sys\n",
    "\n",
    "# The function 'readfiles()' reads all the intermediate files generated by the mappers and returns a list of lists in the following format:\n",
    "# [word, word_count, email_id, TRUTH]\n",
    "# e.g.\n",
    "# [\n",
    "# ['assistance', '1', '0018.2003-12-18.GP', '1'],\n",
    "# ['assistance', '3', '0018.2001-07-13.SA_and_HP', '1'],\n",
    "# ['enlargementwithatypo', '0', '0001.1999-12-10.farmer', '0']\n",
    "# ]\n",
    "\n",
    "def readfiles():\n",
    "    # Opening all files for reading\n",
    "    filehandles = [open(file, 'r') for file in sys.argv[1:]]\n",
    "    \n",
    "    # Reading all files - strip the last newline and then split on new lines \n",
    "    all_file_data = [fh.read().strip().split('\\n') for fh in filehandles]\n",
    "\n",
    "    # Closing all files\n",
    "    retval = [fh.close() for fh in filehandles]\n",
    "    \n",
    "    # Flattening and sorting list\n",
    "    flat_list = [l for sublist in all_file_data for l in sublist]\n",
    "    flat_list.sort()\n",
    "\n",
    "    # Splitting and creating list of lists\n",
    "    key_value_list = [item.split() for item in flat_list] \n",
    "    \n",
    "    return key_value_list\n",
    "\n",
    "def main():\n",
    "    # This dict will be used to store counts of terms\n",
    "    word_count_dict = dict()\n",
    "    key_value_list = readfiles()\n",
    "\n",
    "    # Looping through the list of lists created by def readfiles()\n",
    "    for item in key_value_list:\n",
    "        if item[0] in word_count_dict.keys():   # If word exists update count\n",
    "            word_count_dict[item[0]] = int(word_count_dict[item[0]]) + int(item[1])\n",
    "        else:                                   # If new word then create key and store count\n",
    "            word_count_dict[item[0]] = int(item[1])\n",
    "\n",
    "    # Printing dictionary contents as output\n",
    "    for key, value in word_count_dict.iteritems():\n",
    "        print  key + '\\t' + str(value)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "        main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**pNaiveBayes**  \n",
    "Writing pNaiveBayes out to the filesystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pNaiveBayes.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile pNaiveBayes.sh\n",
    "## pNaiveBayes.sh\n",
    "## Author: Jake Ryland Williams\n",
    "## Usage: pNaiveBayes.sh m wordlist\n",
    "## Input:\n",
    "##       m = number of processes (maps), e.g., 4\n",
    "##       wordlist = a space-separated list of words in quotes, e.g., \"the and of\"\n",
    "##\n",
    "## Instructions: Read this script and its comments closely.\n",
    "##               Do your best to understand the purpose of each command,\n",
    "##               and focus on how arguments are supplied to mapper.py/reducer.py,\n",
    "##               as this will determine how the python scripts take input.\n",
    "##               When you are comfortable with the unix code below,\n",
    "##               answer the questions on the LMS for HW1 about the starter code.\n",
    "\n",
    "## collect user input\n",
    "m=$1 ## the number of parallel processes (maps) to run\n",
    "\n",
    "wordlist=$2 ## if set to \"*\", then all words are used\n",
    "\n",
    "## a test set data of 100 messages\n",
    "data=\"enronemail_1h.txt\" \n",
    "\n",
    "## the full set of data (33746 messages)\n",
    "# data=\"enronemail.txt\" \n",
    "\n",
    "## 'wc' determines the number of lines in the data\n",
    "## 'perl -pe' regex strips the piped wc output to a number\n",
    "linesindata=`wc -l $data | perl -pe 's/^.*?(\\d+).*?$/$1/'`\n",
    "\n",
    "## determine the lines per chunk for the desired number of processes\n",
    "linesinchunk=`echo \"$linesindata/$m+1\" | bc`\n",
    "\n",
    "## split the original file into chunks by line\n",
    "split -l $linesinchunk $data $data.chunk.\n",
    "\n",
    "## assign python mappers (mapper.py) to the chunks of data\n",
    "## and emit their output to temporary files\n",
    "for datachunk in $data.chunk.*; do\n",
    "    ## feed word list to the python mapper here and redirect STDOUT to a temporary file on disk\n",
    "    ####\n",
    "    ####\n",
    "    ./mapper.py $datachunk \"$wordlist\" > $datachunk.counts &\n",
    "    ####\n",
    "    ####\n",
    "done\n",
    "## wait for the mappers to finish their work\n",
    "wait\n",
    "\n",
    "## 'ls' makes a list of the temporary count files\n",
    "## 'perl -pe' regex replaces line breaks with spaces\n",
    "countfiles=`ls $data.chunk.*.counts | perl -pe 's/\\n/ /'`\n",
    "\n",
    "## feed the list of countfiles to the python reducer and redirect STDOUT to disk\n",
    "####\n",
    "####\n",
    "./reducer.py $countfiles > $data.output\n",
    "####\n",
    "####\n",
    "\n",
    "## clean up the data chunks and temporary count files\n",
    "rm $data.chunk.*\n",
    "cat $data.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing execute permissions and executing pNaiveBayes.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistance\t9\r\n"
     ]
    }
   ],
   "source": [
    "!chmod a+x mapper.py\n",
    "!chmod a+x reducer.py\n",
    "!chmod a+x pNaiveBayes.sh\n",
    "!./pNaiveBayes.sh 4 \"assistance\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#HW1.3\n",
    "Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will classify the email messages by a single, user-specified word using the Naive Bayes Formulation. Examine the word “assistance” and report your results.  \n",
    "\n",
    "To do so, make sure that\n",
    "   \n",
    "   - mapper.py and\n",
    "   - reducer.py \n",
    "\n",
    "that performs a single word Naive Bayes classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result**  \n",
    "The algorithm obtained an accurracy of 0.56"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mapper**  \n",
    "Same as **HW1.2** above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reducer**  \n",
    "Step 1:  \n",
    "The reducer first reads all the intermediate files and accumulates all the mapper output data in a list object. It then uses the list object to create a Pandas DataFrame that contains the term frequency per document:\n",
    "- Index/Row names are the email_ids\n",
    "- Column headers are words/features. If the user inputs a word or a list of words from the command line then the headers are those words. This includes words such as enlargementWITHATypo. If the user specified * then all unique words from all emails become the column headers\n",
    "- The DataFrame also contains another column called 'TRUTH' that contains the true class of each email\n",
    "\n",
    "Step 2:  \n",
    "Next, the reducer calculates probabilities from the above DataFrame and stores them in new objects. It used Pandas DataFrame methods such as groupby, sum for the calculations:\n",
    "- It calculates and stores the prior probabilities in a dictionary\n",
    "- It calculates and stores P(word|class) in a Pandas DataFrame. The index of the DataFrame are words and the columns are 'SPAM' and 'HAM'. Laplace smoothing is applied in this step\n",
    "\n",
    "Step 3:  \n",
    "The Reducer then calculates P(email|class) and stores these in yet another Pandas DataFrame. This DataFrame has email_ids as the index and has the following column headings - SPAM, HAM, PREDICT, TRUTH. 'SPAM' stores the log probability of the email given SPAM. 'HAM' stores the log probability of the email given HAM. 'PREDICT' stores the predict class of the email based on the calculated log probabilities. 'TRUTH' contains the true class of the email.\n",
    "The log probabilities of email given class is calculated as follows:\n",
    "- For each email, the reducer refers the 1st DataFrame i.e. the one that contains the Term Frequecy per DataFrame. It gets a dictionary of words and counts where the counts are greater than zero.\n",
    "- It then uses the prior proabilities dictionary and the DataFrame containing P(word|class) to calculate P(email|document)\n",
    "- It then stores the log probabilities in the DataFrame created in step 3\n",
    "\n",
    "Step 4:\n",
    "The Reducer outputs the results in the format - Email_id \\t TRUTH \\t Predicted_Class. It also calculates the accuracy and prints it also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/home/vineetgangwar/anaconda/bin/python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "\n",
    "# The function 'readfiles()' reads all the intermediate files generated by the mappers and returns a list of lists in the following format:\n",
    "# [word, word_count, email_id, TRUTH]\n",
    "# e.g.\n",
    "# [\n",
    "# ['assistance', '1', '0018.2003-12-18.GP', '1', ['assistance','and','valium']],\n",
    "# ['assistance', '3', '0018.2001-07-13.SA_and_HP', '1', ['assistance','and','valium']],\n",
    "# ['enlargementwithatypo', '0', '0001.1999-12-10.farmer', '0', ['assistance','and','valium']]\n",
    "# ]\n",
    "\n",
    "def readfiles(filelist):\n",
    "    # Opening all files for reading\n",
    "    filehandles = [open(file, 'r') for file in filelist]\n",
    "    # Reading all files - strip the last newline and then split on new lines\n",
    "    all_file_data = [fh.read().strip().split('\\n') for fh in filehandles]\n",
    "    # Closing all files\n",
    "    retval = [fh.close() for fh in filehandles]\n",
    "    # Flattening and sorting list\n",
    "    flat_list = [l for sublist in all_file_data for l in sublist]\n",
    "    flat_list.sort()\n",
    "    # Splitting and creating list of lists\n",
    "    key_value_list = [item.split() for item in flat_list]\n",
    "    return key_value_list\n",
    "\n",
    "# This function creates a dataframe with email_ids as the index\n",
    "# and all words as the column headings. This data frame essentially contains the terms frequency per document\n",
    "# Each cell contains the count of occurrences of each word in each email\n",
    "# This function returns a tuple of vocab and the DataFrame\n",
    "\n",
    "def create_dataframe(key_value_list):\n",
    "    ## Creating dataframe of email id and truth pairs\n",
    "    # Creating list of email_ids and truths\n",
    "    email_ids = list()\n",
    "    truths = list()\n",
    "    for item in key_value_list:\n",
    "        email_id = item[2]\n",
    "        truth = int(item[3])\n",
    "        if email_id not in email_ids:\n",
    "            email_ids.append(email_id)\n",
    "            truths.append(truth)\n",
    "    # Creating dictionary\n",
    "    id_truth_dict = dict()\n",
    "    id_truth_dict['email_id'] = email_ids\n",
    "    id_truth_dict['TRUTH'] = truths\n",
    "    # Converting into dataframe\n",
    "    id_truth = pd.DataFrame(id_truth_dict)\n",
    "\n",
    "    ## Creating data frame to store word counts and email in matrix\n",
    "    # Creating words and ids list to create an empty data frame email_id X word list\n",
    "    set_of_words = set()\n",
    "    set_of_ids = set()\n",
    "    for item in key_value_list:\n",
    "        set_of_words.add(item[0])\n",
    "        set_of_ids.add(item[2])\n",
    "\n",
    "    set_of_words = list(set_of_words)\n",
    "    set_of_ids = list(set_of_ids)\n",
    "    num_of_ids = len(set_of_ids)\n",
    "\n",
    "    # Creating dict of zeros to convert into a dataframe\n",
    "    zeros_dict = dict()\n",
    "    for i in range(len(set_of_words)):\n",
    "        zeros_dict[set_of_words[i]] = [0 for x in range(num_of_ids)]\n",
    "    # Adding ids\n",
    "    zeros_dict['email_id'] = set_of_ids\n",
    "\n",
    "    # Converting into dataframe\n",
    "    id_wordlist = pd.DataFrame(zeros_dict)\n",
    "\n",
    "    # Merging dataframe to add truth also\n",
    "    df = pd.merge(id_wordlist, id_truth, on='email_id', how='inner')\n",
    "    df.set_index('email_id', inplace=True)\n",
    "\n",
    "    # Updating counts\n",
    "    for item in key_value_list:\n",
    "        email_id = item[2]\n",
    "        word_count = item[1]\n",
    "        word = item[0]\n",
    "        df.loc[email_id, word] = int(word_count)\n",
    "\n",
    "    return set_of_words, df\n",
    "\n",
    "# This function calcuates the following probabilities:\n",
    "# Priors in a Dict() called priors\n",
    "# A DataFrame containing probabilities of all words given class. This Dataframe called \n",
    "# word_prob_class has the following structure:\n",
    "# words X class\n",
    "\n",
    "def calculating_probs(vocab, df):\n",
    "    category = {'spam': 1, 'ham': 0}\n",
    "    ## Calculating probabilities\n",
    "    # Calculating priors probabilites and storing in a dict\n",
    "    prob_prior_spam = df.groupby('TRUTH').size()[1].astype(float) / len(df)\n",
    "    prob_prior_ham = df.groupby('TRUTH').size()[0].astype(float) / len(df)\n",
    "    priors = {'spam': prob_prior_spam, 'ham': prob_prior_ham}\n",
    "\n",
    "    # Calculating term count in spam and ham for the given vocab\n",
    "    term_count_spam = df.groupby('TRUTH').sum().sum(axis=1)[1]\n",
    "    term_count_ham = df.groupby('TRUTH').sum().sum(axis=1)[0]\n",
    "    term_count_category = {'spam': term_count_spam, 'ham': term_count_ham}\n",
    "\n",
    "    # Calculating counts of words in vocab per catergory\n",
    "    words_per_category = df.groupby('TRUTH').sum().transpose()\n",
    "\n",
    "    # Calculating word probabilities per class\n",
    "    word_probs_class = words_per_category.copy()\n",
    "    for cat_key, cat_value in category.iteritems():\n",
    "        word_probs_class[cat_value] = word_probs_class[cat_value] / term_count_category[cat_key]\n",
    "    # Applying laplace smoothing\n",
    "    # For Spam\n",
    "    word_probs_class[1][word_probs_class[1] == 0] = float(1) / (term_count_category['spam'] + len(vocab))\n",
    "    # For ham\n",
    "    word_probs_class[0][word_probs_class[0] == 0] = float(1) / (term_count_category['ham'] + len(vocab))\n",
    "    \n",
    "    return priors, word_probs_class \n",
    "\n",
    "def main():\n",
    "    filelist = sys.argv[1:]\n",
    "    # Aggregating input from all the mappers\n",
    "    key_value_list = readfiles(filelist)\n",
    "    # Creating vocabulary and Dataframe contains counts of terms per document\n",
    "    vocab, df = create_dataframe(key_value_list)\n",
    "    \n",
    "    # Writing term frequency per document matrix to file for use in HW1.6\n",
    "    df.to_csv('input_for_hw1.6.csv')\n",
    "\n",
    "    # Getting priors and words given class probabilities\n",
    "    priors, word_probs_class = calculating_probs(vocab, df)\n",
    "    \n",
    "    # Creating Pandas DataFrame to store final probabilities\n",
    "    # Creating dataframe to store probabilities\n",
    "    # Structure of DataFrame has email_ds in the index\n",
    "    # and spam, ham, TRUTH, PREDICT as columns\n",
    "    df_probs = df.copy(deep=True)\n",
    "    header_to_remove = list(df_probs.columns.values)\n",
    "    header_to_remove.remove('TRUTH')\n",
    "    tokens = header_to_remove\n",
    "    df_probs.drop(header_to_remove, inplace=True, axis=1)\n",
    "    df_probs['spam'] = [0 for x in range(df.index.values.shape[0])]\n",
    "    df_probs['ham'] = [0 for x in range(df.index.values.shape[0])]\n",
    "    df_probs['PREDICT'] = [0 for x in range(df.index.values.shape[0])]\n",
    "    \n",
    "    # Looping through all emails and calculating probabilites of email given class\n",
    "    # and storing in a DataFrame\n",
    "    \n",
    "    category = {'spam': 1, 'ham': 0}\n",
    "    for email_id in df_probs.index:\n",
    "        # Creating dict of all words whose count != 0 per email\n",
    "        words_in_email = dict(df.loc[email_id, df.loc[email_id] != 0])\n",
    "        # Removing the column 'TRUTH'\n",
    "        if 'TRUTH' in words_in_email:\n",
    "            words_in_email.pop('TRUTH')\n",
    "\n",
    "        for cat_key, cat_value in category.iteritems():\n",
    "            running_prob = math.log(priors[cat_key])\n",
    "\n",
    "            for word in words_in_email:\n",
    "                count = df.loc[email_id, word]\n",
    "                running_prob += count * math.log(word_probs_class.loc[word, cat_value])\n",
    "\n",
    "            df_probs.loc[email_id, cat_key] = running_prob\n",
    "\n",
    "    # Calculating predictions\n",
    "    df_probs['PREDICT'] = (df_probs['spam'] > df_probs['ham']).astype(int)\n",
    "\n",
    "    # Printing output\n",
    "    for email_id in df_probs.index:\n",
    "        print email_id, '\\t', int(df_probs.loc[email_id, 'TRUTH']), '\\t', int(df_probs.loc[email_id, 'PREDICT'])\n",
    "    \n",
    "    # Calculating and printing accuracy\n",
    "    correct = df_probs['TRUTH'] == df_probs['PREDICT']\n",
    "    print 'Accuracy:', float(np.sum(correct.astype(int))) / len(df_probs)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "        main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing pNaiveBayes with \"assistance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0010.2003-12-18.GP \t1 \t0\r\n",
      "0010.2001-06-28.SA_and_HP \t1 \t0\r\n",
      "0001.2000-01-17.beck \t0 \t0\r\n",
      "0018.1999-12-14.kaminski \t0 \t0\r\n",
      "0005.1999-12-12.kaminski \t0 \t0\r\n",
      "0011.2001-06-29.SA_and_HP \t1 \t0\r\n",
      "0008.2004-08-01.BG \t1 \t0\r\n",
      "0009.1999-12-14.farmer \t0 \t0\r\n",
      "0017.2003-12-18.GP \t1 \t0\r\n",
      "0011.2001-06-28.SA_and_HP \t1 \t0\r\n",
      "0015.2001-07-05.SA_and_HP \t1 \t0\r\n",
      "0015.2001-02-12.kitchen \t0 \t0\r\n",
      "0009.2001-06-26.SA_and_HP \t1 \t0\r\n",
      "0017.1999-12-14.kaminski \t0 \t0\r\n",
      "0012.2000-01-17.beck \t0 \t0\r\n",
      "0003.2000-01-17.beck \t0 \t0\r\n",
      "0004.2001-06-12.SA_and_HP \t1 \t0\r\n",
      "0008.2001-06-12.SA_and_HP \t1 \t0\r\n",
      "0007.2001-02-09.kitchen \t0 \t0\r\n",
      "0016.2004-08-01.BG \t1 \t0\r\n",
      "0015.2000-06-09.lokay \t0 \t0\r\n",
      "0016.1999-12-15.farmer \t0 \t0\r\n",
      "0013.2004-08-01.BG \t1 \t0\r\n",
      "0005.2003-12-18.GP \t1 \t0\r\n",
      "0012.2001-02-09.kitchen \t0 \t0\r\n",
      "0011.1999-12-14.farmer \t0 \t0\r\n",
      "0003.2001-02-08.kitchen \t0 \t0\r\n",
      "0009.2001-02-09.kitchen \t0 \t0\r\n",
      "0006.2001-02-08.kitchen \t0 \t0\r\n",
      "0014.2003-12-19.GP \t1 \t0\r\n",
      "0010.1999-12-14.farmer \t0 \t0\r\n",
      "0010.2004-08-01.BG \t1 \t0\r\n",
      "0014.1999-12-14.kaminski \t0 \t0\r\n",
      "0006.1999-12-13.kaminski \t0 \t0\r\n",
      "0005.1999-12-14.farmer \t0 \t0\r\n",
      "0013.1999-12-14.kaminski \t0 \t0\r\n",
      "0001.2001-02-07.kitchen \t0 \t0\r\n",
      "0008.2001-02-09.kitchen \t0 \t0\r\n",
      "0007.2003-12-18.GP \t1 \t0\r\n",
      "0017.2004-08-02.BG \t1 \t0\r\n",
      "0014.2004-08-01.BG \t1 \t0\r\n",
      "0006.2003-12-18.GP \t1 \t0\r\n",
      "0016.2001-07-05.SA_and_HP \t1 \t0\r\n",
      "0008.2003-12-18.GP \t1 \t0\r\n",
      "0014.2001-07-04.SA_and_HP \t1 \t0\r\n",
      "0001.2001-04-02.williams \t0 \t0\r\n",
      "0012.2000-06-08.lokay \t0 \t0\r\n",
      "0014.1999-12-15.farmer \t0 \t0\r\n",
      "0009.2000-06-07.lokay \t0 \t0\r\n",
      "0001.1999-12-10.farmer \t0 \t0\r\n",
      "0008.2001-06-25.SA_and_HP \t1 \t0\r\n",
      "0017.2001-04-03.williams \t0 \t0\r\n",
      "0014.2001-02-12.kitchen \t0 \t0\r\n",
      "0016.2001-07-06.SA_and_HP \t1 \t0\r\n",
      "0015.1999-12-15.farmer \t0 \t0\r\n",
      "0004.1999-12-10.kaminski \t0 \t0\r\n",
      "0001.2000-06-06.lokay \t0 \t0\r\n",
      "0011.2004-08-01.BG \t1 \t0\r\n",
      "0004.2004-08-01.BG \t1 \t0\r\n",
      "0018.2003-12-18.GP \t1 \t0\r\n",
      "0002.1999-12-13.farmer \t0 \t0\r\n",
      "0016.2003-12-19.GP \t1 \t0\r\n",
      "0004.1999-12-14.farmer \t0 \t0\r\n",
      "0015.2003-12-19.GP \t1 \t0\r\n",
      "0006.2004-08-01.BG \t1 \t0\r\n",
      "0009.2003-12-18.GP \t1 \t0\r\n",
      "0007.1999-12-14.farmer \t0 \t0\r\n",
      "0005.2000-06-06.lokay \t0 \t0\r\n",
      "0010.1999-12-14.kaminski \t0 \t0\r\n",
      "0007.2000-01-17.beck \t0 \t0\r\n",
      "0003.1999-12-14.farmer \t0 \t0\r\n",
      "0003.2004-08-01.BG \t1 \t0\r\n",
      "0017.2004-08-01.BG \t1 \t0\r\n",
      "0013.2001-06-30.SA_and_HP \t1 \t0\r\n",
      "0003.1999-12-10.kaminski \t0 \t0\r\n",
      "0012.1999-12-14.farmer \t0 \t0\r\n",
      "0009.1999-12-13.kaminski \t0 \t0\r\n",
      "0018.2001-07-13.SA_and_HP \t1 \t0\r\n",
      "0002.2001-02-07.kitchen \t0 \t0\r\n",
      "0007.2004-08-01.BG \t1 \t0\r\n",
      "0012.1999-12-14.kaminski \t0 \t0\r\n",
      "0005.2001-06-23.SA_and_HP \t1 \t0\r\n",
      "0007.1999-12-13.kaminski \t0 \t0\r\n",
      "0017.2000-01-17.beck \t0 \t0\r\n",
      "0006.2001-06-25.SA_and_HP \t1 \t0\r\n",
      "0006.2001-04-03.williams \t0 \t0\r\n",
      "0005.2001-02-08.kitchen \t0 \t0\r\n",
      "0002.2003-12-18.GP \t1 \t0\r\n",
      "0003.2003-12-18.GP \t1 \t0\r\n",
      "0013.2001-04-03.williams \t0 \t0\r\n",
      "0004.2001-04-02.williams \t0 \t0\r\n",
      "0010.2001-02-09.kitchen \t0 \t0\r\n",
      "0001.1999-12-10.kaminski \t0 \t0\r\n",
      "0013.1999-12-14.farmer \t0 \t0\r\n",
      "0015.1999-12-14.kaminski \t0 \t0\r\n",
      "0012.2003-12-19.GP \t1 \t0\r\n",
      "0016.2001-02-12.kitchen \t0 \t0\r\n",
      "0002.2004-08-01.BG \t1 \t0\r\n",
      "0002.2001-05-25.SA_and_HP \t1 \t0\r\n",
      "0011.2003-12-18.GP \t1 \t0\r\n",
      "Accuracy: 0.56\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 4 \"assistance\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#HW1.4\n",
    "Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will classify the email messages by a list of one or more user-specified words. Examine the words “assistance”, “valium”, and “enlargementWithATypo” and report your results  \n",
    "To do so, make sure that\n",
    "\n",
    "   - mapper.py counts all occurrences of a list of words, and\n",
    "   - reducer.py \n",
    "\n",
    "performs the multiple-word Naive Bayes classification via the chosen list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mapper**  \n",
    "Same as **HW1.2** above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reducer**  \n",
    "Same as **HW1.3** above "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm achieved an accuracy of 0.56. using the terms \"assistance valium enlargementWithATypo\".  \n",
    "To Note:  \n",
    "- The algorithm achieved an accuracy of of 0.59 when the entire email was used rather than just the message body.  \n",
    "- The algorithm achieved an accuracy of of 0.57 when the term 'him' was added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0010.2003-12-18.GP \t1 \t0\r\n",
      "0010.2001-06-28.SA_and_HP \t1 \t0\r\n",
      "0001.2000-01-17.beck \t0 \t0\r\n",
      "0018.1999-12-14.kaminski \t0 \t0\r\n",
      "0005.1999-12-12.kaminski \t0 \t0\r\n",
      "0011.2001-06-29.SA_and_HP \t1 \t0\r\n",
      "0008.2004-08-01.BG \t1 \t0\r\n",
      "0009.1999-12-14.farmer \t0 \t0\r\n",
      "0017.2003-12-18.GP \t1 \t0\r\n",
      "0011.2001-06-28.SA_and_HP \t1 \t0\r\n",
      "0015.2001-07-05.SA_and_HP \t1 \t0\r\n",
      "0015.2001-02-12.kitchen \t0 \t0\r\n",
      "0009.2001-06-26.SA_and_HP \t1 \t0\r\n",
      "0017.1999-12-14.kaminski \t0 \t0\r\n",
      "0012.2000-01-17.beck \t0 \t0\r\n",
      "0003.2000-01-17.beck \t0 \t0\r\n",
      "0004.2001-06-12.SA_and_HP \t1 \t0\r\n",
      "0008.2001-06-12.SA_and_HP \t1 \t0\r\n",
      "0007.2001-02-09.kitchen \t0 \t0\r\n",
      "0016.2004-08-01.BG \t1 \t0\r\n",
      "0015.2000-06-09.lokay \t0 \t0\r\n",
      "0016.1999-12-15.farmer \t0 \t0\r\n",
      "0013.2004-08-01.BG \t1 \t0\r\n",
      "0005.2003-12-18.GP \t1 \t0\r\n",
      "0012.2001-02-09.kitchen \t0 \t0\r\n",
      "0011.1999-12-14.farmer \t0 \t0\r\n",
      "0003.2001-02-08.kitchen \t0 \t0\r\n",
      "0009.2001-02-09.kitchen \t0 \t0\r\n",
      "0006.2001-02-08.kitchen \t0 \t0\r\n",
      "0014.2003-12-19.GP \t1 \t0\r\n",
      "0010.1999-12-14.farmer \t0 \t0\r\n",
      "0010.2004-08-01.BG \t1 \t0\r\n",
      "0014.1999-12-14.kaminski \t0 \t0\r\n",
      "0006.1999-12-13.kaminski \t0 \t0\r\n",
      "0005.1999-12-14.farmer \t0 \t0\r\n",
      "0013.1999-12-14.kaminski \t0 \t0\r\n",
      "0001.2001-02-07.kitchen \t0 \t0\r\n",
      "0008.2001-02-09.kitchen \t0 \t0\r\n",
      "0007.2003-12-18.GP \t1 \t0\r\n",
      "0017.2004-08-02.BG \t1 \t0\r\n",
      "0014.2004-08-01.BG \t1 \t0\r\n",
      "0006.2003-12-18.GP \t1 \t0\r\n",
      "0016.2001-07-05.SA_and_HP \t1 \t0\r\n",
      "0008.2003-12-18.GP \t1 \t0\r\n",
      "0014.2001-07-04.SA_and_HP \t1 \t0\r\n",
      "0001.2001-04-02.williams \t0 \t0\r\n",
      "0012.2000-06-08.lokay \t0 \t0\r\n",
      "0014.1999-12-15.farmer \t0 \t0\r\n",
      "0009.2000-06-07.lokay \t0 \t0\r\n",
      "0001.1999-12-10.farmer \t0 \t0\r\n",
      "0008.2001-06-25.SA_and_HP \t1 \t0\r\n",
      "0017.2001-04-03.williams \t0 \t0\r\n",
      "0014.2001-02-12.kitchen \t0 \t0\r\n",
      "0016.2001-07-06.SA_and_HP \t1 \t0\r\n",
      "0015.1999-12-15.farmer \t0 \t0\r\n",
      "0004.1999-12-10.kaminski \t0 \t0\r\n",
      "0001.2000-06-06.lokay \t0 \t0\r\n",
      "0011.2004-08-01.BG \t1 \t0\r\n",
      "0004.2004-08-01.BG \t1 \t0\r\n",
      "0018.2003-12-18.GP \t1 \t0\r\n",
      "0002.1999-12-13.farmer \t0 \t0\r\n",
      "0016.2003-12-19.GP \t1 \t0\r\n",
      "0004.1999-12-14.farmer \t0 \t0\r\n",
      "0015.2003-12-19.GP \t1 \t0\r\n",
      "0006.2004-08-01.BG \t1 \t0\r\n",
      "0009.2003-12-18.GP \t1 \t0\r\n",
      "0007.1999-12-14.farmer \t0 \t0\r\n",
      "0005.2000-06-06.lokay \t0 \t0\r\n",
      "0010.1999-12-14.kaminski \t0 \t0\r\n",
      "0007.2000-01-17.beck \t0 \t0\r\n",
      "0003.1999-12-14.farmer \t0 \t0\r\n",
      "0003.2004-08-01.BG \t1 \t0\r\n",
      "0017.2004-08-01.BG \t1 \t0\r\n",
      "0013.2001-06-30.SA_and_HP \t1 \t0\r\n",
      "0003.1999-12-10.kaminski \t0 \t0\r\n",
      "0012.1999-12-14.farmer \t0 \t0\r\n",
      "0009.1999-12-13.kaminski \t0 \t0\r\n",
      "0018.2001-07-13.SA_and_HP \t1 \t0\r\n",
      "0002.2001-02-07.kitchen \t0 \t0\r\n",
      "0007.2004-08-01.BG \t1 \t0\r\n",
      "0012.1999-12-14.kaminski \t0 \t0\r\n",
      "0005.2001-06-23.SA_and_HP \t1 \t0\r\n",
      "0007.1999-12-13.kaminski \t0 \t0\r\n",
      "0017.2000-01-17.beck \t0 \t0\r\n",
      "0006.2001-06-25.SA_and_HP \t1 \t0\r\n",
      "0006.2001-04-03.williams \t0 \t0\r\n",
      "0005.2001-02-08.kitchen \t0 \t0\r\n",
      "0002.2003-12-18.GP \t1 \t0\r\n",
      "0003.2003-12-18.GP \t1 \t0\r\n",
      "0013.2001-04-03.williams \t0 \t0\r\n",
      "0004.2001-04-02.williams \t0 \t0\r\n",
      "0010.2001-02-09.kitchen \t0 \t0\r\n",
      "0001.1999-12-10.kaminski \t0 \t0\r\n",
      "0013.1999-12-14.farmer \t0 \t0\r\n",
      "0015.1999-12-14.kaminski \t0 \t0\r\n",
      "0012.2003-12-19.GP \t1 \t0\r\n",
      "0016.2001-02-12.kitchen \t0 \t0\r\n",
      "0002.2004-08-01.BG \t1 \t0\r\n",
      "0002.2001-05-25.SA_and_HP \t1 \t0\r\n",
      "0011.2003-12-18.GP \t1 \t0\r\n",
      "Accuracy: 0.56\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 4 \"assistance valium enlargementWithATypo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm achieved an accuracy of 0.57 when the term 'him' was added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0010.2003-12-18.GP \t1 \t0\r\n",
      "0010.2001-06-28.SA_and_HP \t1 \t0\r\n",
      "0001.2000-01-17.beck \t0 \t0\r\n",
      "0018.1999-12-14.kaminski \t0 \t0\r\n",
      "0005.1999-12-12.kaminski \t0 \t0\r\n",
      "0011.2001-06-29.SA_and_HP \t1 \t0\r\n",
      "0008.2004-08-01.BG \t1 \t0\r\n",
      "0009.1999-12-14.farmer \t0 \t0\r\n",
      "0017.2003-12-18.GP \t1 \t0\r\n",
      "0011.2001-06-28.SA_and_HP \t1 \t0\r\n",
      "0015.2001-07-05.SA_and_HP \t1 \t0\r\n",
      "0015.2001-02-12.kitchen \t0 \t0\r\n",
      "0009.2001-06-26.SA_and_HP \t1 \t0\r\n",
      "0017.1999-12-14.kaminski \t0 \t0\r\n",
      "0012.2000-01-17.beck \t0 \t0\r\n",
      "0003.2000-01-17.beck \t0 \t0\r\n",
      "0004.2001-06-12.SA_and_HP \t1 \t0\r\n",
      "0008.2001-06-12.SA_and_HP \t1 \t0\r\n",
      "0007.2001-02-09.kitchen \t0 \t0\r\n",
      "0016.2004-08-01.BG \t1 \t0\r\n",
      "0015.2000-06-09.lokay \t0 \t0\r\n",
      "0016.1999-12-15.farmer \t0 \t0\r\n",
      "0013.2004-08-01.BG \t1 \t0\r\n",
      "0005.2003-12-18.GP \t1 \t0\r\n",
      "0012.2001-02-09.kitchen \t0 \t0\r\n",
      "0011.1999-12-14.farmer \t0 \t0\r\n",
      "0003.2001-02-08.kitchen \t0 \t0\r\n",
      "0009.2001-02-09.kitchen \t0 \t0\r\n",
      "0006.2001-02-08.kitchen \t0 \t0\r\n",
      "0014.2003-12-19.GP \t1 \t0\r\n",
      "0010.1999-12-14.farmer \t0 \t0\r\n",
      "0010.2004-08-01.BG \t1 \t0\r\n",
      "0014.1999-12-14.kaminski \t0 \t0\r\n",
      "0006.1999-12-13.kaminski \t0 \t0\r\n",
      "0005.1999-12-14.farmer \t0 \t0\r\n",
      "0013.1999-12-14.kaminski \t0 \t0\r\n",
      "0001.2001-02-07.kitchen \t0 \t0\r\n",
      "0008.2001-02-09.kitchen \t0 \t0\r\n",
      "0007.2003-12-18.GP \t1 \t0\r\n",
      "0017.2004-08-02.BG \t1 \t0\r\n",
      "0014.2004-08-01.BG \t1 \t0\r\n",
      "0006.2003-12-18.GP \t1 \t0\r\n",
      "0016.2001-07-05.SA_and_HP \t1 \t0\r\n",
      "0008.2003-12-18.GP \t1 \t0\r\n",
      "0014.2001-07-04.SA_and_HP \t1 \t0\r\n",
      "0001.2001-04-02.williams \t0 \t0\r\n",
      "0012.2000-06-08.lokay \t0 \t0\r\n",
      "0014.1999-12-15.farmer \t0 \t0\r\n",
      "0009.2000-06-07.lokay \t0 \t0\r\n",
      "0001.1999-12-10.farmer \t0 \t0\r\n",
      "0008.2001-06-25.SA_and_HP \t1 \t0\r\n",
      "0017.2001-04-03.williams \t0 \t0\r\n",
      "0014.2001-02-12.kitchen \t0 \t0\r\n",
      "0016.2001-07-06.SA_and_HP \t1 \t0\r\n",
      "0015.1999-12-15.farmer \t0 \t0\r\n",
      "0004.1999-12-10.kaminski \t0 \t0\r\n",
      "0001.2000-06-06.lokay \t0 \t0\r\n",
      "0011.2004-08-01.BG \t1 \t0\r\n",
      "0004.2004-08-01.BG \t1 \t0\r\n",
      "0018.2003-12-18.GP \t1 \t0\r\n",
      "0002.1999-12-13.farmer \t0 \t0\r\n",
      "0016.2003-12-19.GP \t1 \t0\r\n",
      "0004.1999-12-14.farmer \t0 \t0\r\n",
      "0015.2003-12-19.GP \t1 \t0\r\n",
      "0006.2004-08-01.BG \t1 \t0\r\n",
      "0009.2003-12-18.GP \t1 \t0\r\n",
      "0007.1999-12-14.farmer \t0 \t0\r\n",
      "0005.2000-06-06.lokay \t0 \t0\r\n",
      "0010.1999-12-14.kaminski \t0 \t0\r\n",
      "0007.2000-01-17.beck \t0 \t0\r\n",
      "0003.1999-12-14.farmer \t0 \t0\r\n",
      "0003.2004-08-01.BG \t1 \t0\r\n",
      "0017.2004-08-01.BG \t1 \t0\r\n",
      "0013.2001-06-30.SA_and_HP \t1 \t0\r\n",
      "0003.1999-12-10.kaminski \t0 \t0\r\n",
      "0012.1999-12-14.farmer \t0 \t0\r\n",
      "0009.1999-12-13.kaminski \t0 \t0\r\n",
      "0018.2001-07-13.SA_and_HP \t1 \t1\r\n",
      "0002.2001-02-07.kitchen \t0 \t0\r\n",
      "0007.2004-08-01.BG \t1 \t0\r\n",
      "0012.1999-12-14.kaminski \t0 \t0\r\n",
      "0005.2001-06-23.SA_and_HP \t1 \t0\r\n",
      "0007.1999-12-13.kaminski \t0 \t0\r\n",
      "0017.2000-01-17.beck \t0 \t0\r\n",
      "0006.2001-06-25.SA_and_HP \t1 \t0\r\n",
      "0006.2001-04-03.williams \t0 \t0\r\n",
      "0005.2001-02-08.kitchen \t0 \t0\r\n",
      "0002.2003-12-18.GP \t1 \t0\r\n",
      "0003.2003-12-18.GP \t1 \t0\r\n",
      "0013.2001-04-03.williams \t0 \t0\r\n",
      "0004.2001-04-02.williams \t0 \t0\r\n",
      "0010.2001-02-09.kitchen \t0 \t0\r\n",
      "0001.1999-12-10.kaminski \t0 \t0\r\n",
      "0013.1999-12-14.farmer \t0 \t0\r\n",
      "0015.1999-12-14.kaminski \t0 \t0\r\n",
      "0012.2003-12-19.GP \t1 \t0\r\n",
      "0016.2001-02-12.kitchen \t0 \t0\r\n",
      "0002.2004-08-01.BG \t1 \t0\r\n",
      "0002.2001-05-25.SA_and_HP \t1 \t0\r\n",
      "0011.2003-12-18.GP \t1 \t0\r\n",
      "Accuracy: 0.57\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 4 \"assistance valium enlargementWithATypo him\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#HW1.5\n",
    "Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will classify the email messages by all words present.\n",
    "To do so, make sure that\n",
    "\n",
    "   - mapper.py counts all occurrences of all words, and\n",
    "   - reducer.py performs a word-distribution-wide Naive Bayes classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mapper**  \n",
    "Same as **HW1.2** above. Mapper explanation given in section 1.2 above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reducer**  \n",
    "Same as **HW1.3** above. Reducer explaination provided in section 1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing pNaiveBayes with all words - the algorithm achieved and accuracy of 0.94"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0010.2003-12-18.GP \t1 \t0\r\n",
      "0010.2001-06-28.SA_and_HP \t1 \t1\r\n",
      "0001.2000-01-17.beck \t0 \t0\r\n",
      "0018.1999-12-14.kaminski \t0 \t0\r\n",
      "0005.1999-12-12.kaminski \t0 \t0\r\n",
      "0011.2001-06-29.SA_and_HP \t1 \t1\r\n",
      "0008.2004-08-01.BG \t1 \t0\r\n",
      "0009.1999-12-14.farmer \t0 \t0\r\n",
      "0017.2003-12-18.GP \t1 \t1\r\n",
      "0011.2001-06-28.SA_and_HP \t1 \t1\r\n",
      "0015.2001-07-05.SA_and_HP \t1 \t1\r\n",
      "0015.2001-02-12.kitchen \t0 \t0\r\n",
      "0009.2001-06-26.SA_and_HP \t1 \t1\r\n",
      "0018.2001-07-13.SA_and_HP \t1 \t1\r\n",
      "0012.2000-01-17.beck \t0 \t0\r\n",
      "0003.2000-01-17.beck \t0 \t0\r\n",
      "0004.2001-06-12.SA_and_HP \t1 \t1\r\n",
      "0008.2001-06-12.SA_and_HP \t1 \t1\r\n",
      "0007.2001-02-09.kitchen \t0 \t0\r\n",
      "0016.2004-08-01.BG \t1 \t1\r\n",
      "0015.2000-06-09.lokay \t0 \t0\r\n",
      "0016.1999-12-15.farmer \t0 \t0\r\n",
      "0013.2004-08-01.BG \t1 \t1\r\n",
      "0005.2003-12-18.GP \t1 \t1\r\n",
      "0012.2001-02-09.kitchen \t0 \t0\r\n",
      "0011.1999-12-14.farmer \t0 \t0\r\n",
      "0009.2001-02-09.kitchen \t0 \t0\r\n",
      "0006.2001-02-08.kitchen \t0 \t0\r\n",
      "0014.2003-12-19.GP \t1 \t1\r\n",
      "0010.1999-12-14.farmer \t0 \t0\r\n",
      "0010.2004-08-01.BG \t1 \t1\r\n",
      "0014.1999-12-14.kaminski \t0 \t0\r\n",
      "0006.1999-12-13.kaminski \t0 \t0\r\n",
      "0005.1999-12-14.farmer \t0 \t0\r\n",
      "0013.1999-12-14.kaminski \t0 \t0\r\n",
      "0001.2001-02-07.kitchen \t0 \t0\r\n",
      "0008.2001-02-09.kitchen \t0 \t0\r\n",
      "0007.2003-12-18.GP \t1 \t1\r\n",
      "0017.2004-08-02.BG \t1 \t1\r\n",
      "0014.2004-08-01.BG \t1 \t1\r\n",
      "0006.2003-12-18.GP \t1 \t0\r\n",
      "0016.2001-07-05.SA_and_HP \t1 \t1\r\n",
      "0008.2003-12-18.GP \t1 \t1\r\n",
      "0014.2001-07-04.SA_and_HP \t1 \t1\r\n",
      "0001.2001-04-02.williams \t0 \t0\r\n",
      "0012.2000-06-08.lokay \t0 \t0\r\n",
      "0014.1999-12-15.farmer \t0 \t0\r\n",
      "0009.2000-06-07.lokay \t0 \t0\r\n",
      "0001.1999-12-10.farmer \t0 \t0\r\n",
      "0008.2001-06-25.SA_and_HP \t1 \t1\r\n",
      "0017.2001-04-03.williams \t0 \t0\r\n",
      "0014.2001-02-12.kitchen \t0 \t0\r\n",
      "0016.2001-07-06.SA_and_HP \t1 \t1\r\n",
      "0015.1999-12-15.farmer \t0 \t0\r\n",
      "0009.1999-12-13.kaminski \t0 \t0\r\n",
      "0001.2000-06-06.lokay \t0 \t0\r\n",
      "0011.2004-08-01.BG \t1 \t1\r\n",
      "0004.2004-08-01.BG \t1 \t0\r\n",
      "0018.2003-12-18.GP \t1 \t1\r\n",
      "0007.1999-12-14.farmer \t0 \t0\r\n",
      "0016.2003-12-19.GP \t1 \t1\r\n",
      "0004.1999-12-14.farmer \t0 \t0\r\n",
      "0015.2003-12-19.GP \t1 \t1\r\n",
      "0006.2004-08-01.BG \t1 \t0\r\n",
      "0009.2003-12-18.GP \t1 \t1\r\n",
      "0002.1999-12-13.farmer \t0 \t0\r\n",
      "0002.2004-08-01.BG \t1 \t1\r\n",
      "0010.1999-12-14.kaminski \t0 \t0\r\n",
      "0007.2000-01-17.beck \t0 \t0\r\n",
      "0003.1999-12-14.farmer \t0 \t0\r\n",
      "0003.2004-08-01.BG \t1 \t1\r\n",
      "0017.2004-08-01.BG \t1 \t1\r\n",
      "0013.2001-06-30.SA_and_HP \t1 \t1\r\n",
      "0003.1999-12-10.kaminski \t0 \t0\r\n",
      "0012.1999-12-14.farmer \t0 \t0\r\n",
      "0004.1999-12-10.kaminski \t0 \t0\r\n",
      "0017.1999-12-14.kaminski \t0 \t0\r\n",
      "0002.2001-02-07.kitchen \t0 \t0\r\n",
      "0007.2004-08-01.BG \t1 \t1\r\n",
      "0012.1999-12-14.kaminski \t0 \t0\r\n",
      "0005.2001-06-23.SA_and_HP \t1 \t1\r\n",
      "0005.2000-06-06.lokay \t0 \t0\r\n",
      "0007.1999-12-13.kaminski \t0 \t0\r\n",
      "0017.2000-01-17.beck \t0 \t0\r\n",
      "0003.2001-02-08.kitchen \t0 \t0\r\n",
      "0006.2001-06-25.SA_and_HP \t1 \t0\r\n",
      "0006.2001-04-03.williams \t0 \t0\r\n",
      "0005.2001-02-08.kitchen \t0 \t0\r\n",
      "0002.2003-12-18.GP \t1 \t1\r\n",
      "0003.2003-12-18.GP \t1 \t1\r\n",
      "0013.2001-04-03.williams \t0 \t0\r\n",
      "0004.2001-04-02.williams \t0 \t0\r\n",
      "0010.2001-02-09.kitchen \t0 \t0\r\n",
      "0001.1999-12-10.kaminski \t0 \t0\r\n",
      "0013.1999-12-14.farmer \t0 \t0\r\n",
      "0015.1999-12-14.kaminski \t0 \t0\r\n",
      "0012.2003-12-19.GP \t1 \t1\r\n",
      "0016.2001-02-12.kitchen \t0 \t0\r\n",
      "0002.2001-05-25.SA_and_HP \t1 \t1\r\n",
      "0011.2003-12-18.GP \t1 \t1\r\n",
      "Accuracy: 0.94\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 4 \"*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#HW1.6\n",
    "Benchmark your code with the Python SciKit-Learn implementation of Naive Bayes\n",
    "\n",
    "It always a good idea to test your solutions against publicly available libraries such as SciKit-Learn, The Machine Learning toolkit available in Python. In this exercise, we benchmark ourselves against the SciKit-Learn implementation of Naive Bayes.  For more information on this implementation see: http://scikit-learn.org/stable/modules/naive_bayes.html more  \n",
    "\n",
    "Lets define  Training error = misclassification rate with respect to a training set. It is more formally defined here:\n",
    "\n",
    "Let DF represent the training set in the following:\n",
    "Err(Model, DF) = |{(X, c(X)) ∈ DF : c(X) != Model(x)}| / |DF|\n",
    "\n",
    "Where || denotes set cardinality; c(X) denotes the class of the tuple X in DF; and Model(X) denotes the class inferred by the Model “Model”\n",
    "\n",
    "In this exercise, please complete the following:\n",
    "\n",
    "- Run the Multinomial Naive Bayes algorithm (using default settings) from SciKit-Learn over the same training data used in HW1.5 and report the Training error (please note some data preparation might be needed to get the Multinomial Naive Bayes algorithm from SkiKit-Learn to run over this dataset)\n",
    "- Run the Bernoulli Naive Bayes algorithm from SciKit-Learn (using default settings) over the same training data used in HW1.5 and report the Training error \n",
    "- Run the Multinomial Naive Bayes algorithm you developed for HW1.5 over the same data used HW1.5 and report the Training error \n",
    "- Please prepare a table to present your results\n",
    "- Explain/justify any differences in terms of training error rates over the dataset in HW1.5 between your Multinomial Naive Bayes implementation (in Map Reduce) versus the Multinomial Naive Bayes implementation in SciKit-Learn\n",
    "- Discuss the performance differences in terms of training error rates over the dataset in HW1.5 between the  Multinomial Naive Bayes implementation in SciKit-Learn with the  Bernoulli Naive Bayes implementation in SciKit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HW1.6 - i: sklearn Multinomial Naive Bayes Training Error =\t0.02\n",
      "\n",
      "HW1.6 - ii: sklearn Bernoulli Naive Bayes Training Error =\t0.19\n",
      "\n",
      "HW1.6 - iii: HW1.5 Multinomial Naive Bayes Training Error =\t0.06\n",
      "\n",
      "HW1.6 - iv: Table with Results\n",
      "                   Training_Error\n",
      "Algorithm                        \n",
      "sklearn Multi                0.02\n",
      "sklearn Bernoulli            0.19\n",
      "HW1.5 Multi                  0.06\n",
      "\n",
      "HW1.6 - v:\n",
      "Number of tokens created by CountVectorizer: 5322\n",
      "sklearn Multinomial Naive Bayes Training Error with DataFrame from HW1.5: 0.03\n",
      "Number of Tokens created in HW1.5: 5016\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "filename = 'enronemail_1h.txt'\n",
    "train_label = list()\n",
    "train_data = list()\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for line in myfile:\n",
    "        fields = line.strip().split('\\t')\n",
    "        email_id = fields[0]\n",
    "        truth = fields[1]\n",
    "        if len(fields) == 4:\n",
    "            subject = fields[2]\n",
    "            message = fields[3]\n",
    "        else:\n",
    "            subject = \"\"\n",
    "            message = fields[2]\n",
    "        \n",
    "        # Updating train label and train data list\n",
    "        train_label.append(truth)\n",
    "        train_data.append(message)\n",
    "\n",
    "# CountVectorizer as feature extraction\n",
    "cv = CountVectorizer()\n",
    "cv.fit(train_data)\n",
    "cv_matrix = cv.transform(train_data)\n",
    "\n",
    "# Calculating sklearn Multinomial Naive Bayes Training Error\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(cv_matrix, train_label)\n",
    "mnb_train_err = 1 - mnb.score(cv_matrix, train_label)\n",
    "print 'HW1.6 - i: sklearn Multinomial Naive Bayes Training Error =\\t', mnb_train_err\n",
    "\n",
    "# Calculating skearn Bernoulli Naive Bayes Training Error\n",
    "bnb = BernoulliNB()\n",
    "bnb.fit(cv_matrix, train_label)\n",
    "bnb_train_err = 1 - bnb.score(cv_matrix, train_label)\n",
    "print '\\nHW1.6 - ii: sklearn Bernoulli Naive Bayes Training Error =\\t', bnb_train_err\n",
    "\n",
    "# Calculating HW1.5 Multinomial Naive Bayes Training Error\n",
    "tempvar = !./pNaiveBayes.sh 4 \"*\"\n",
    "hw1_5_train_err = 1 - float(tempvar[-1].split(':')[1].strip())\n",
    "print '\\nHW1.6 - iii: HW1.5 Multinomial Naive Bayes Training Error =\\t', hw1_5_train_err\n",
    "\n",
    "# Printing table\n",
    "print \"\\nHW1.6 - iv: Table with Results\"\n",
    "result = pd.DataFrame({'Algorithm':['sklearn Multi', 'sklearn Bernoulli', 'HW1.5 Multi'], 'Training_Error':[mnb_train_err, bnb_train_err, hw1_5_train_err]})\n",
    "result.set_index('Algorithm', inplace=True)\n",
    "print result\n",
    "\n",
    "# Number v\n",
    "print \"\\nHW1.6 - v:\"\n",
    "print \"Number of tokens created by CountVectorizer:\", len(cv.vocabulary_)\n",
    "\n",
    "# Testing sklearn with DataFrame from HW1.5\n",
    "# Reading dataframe created in HW1.5 above\n",
    "filename = 'input_for_hw1.6.csv'\n",
    "df = pd.read_csv(filename)\n",
    "\n",
    "def prepare_data_mnb(df):\n",
    "    # Creating train_data and train_label\n",
    "    train_data = df.copy()\n",
    "\n",
    "    train_label = train_data['TRUTH']\n",
    "    train_label = train_label.as_matrix()\n",
    "\n",
    "    train_data.drop(['email_id', 'TRUTH'], axis=1, inplace=True)\n",
    "    train_data = train_data.as_matrix()\n",
    "    return train_data, train_label\n",
    "\n",
    "# Multinomial Naive Bayes model\n",
    "train_data, train_label = prepare_data_mnb(df)\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(train_data, train_label)\n",
    "print 'sklearn Multinomial Naive Bayes Training Error with DataFrame from HW1.5:', 1 - mnb.score(train_data, train_label)\n",
    "\n",
    "print \"Number of Tokens created in HW1.5:\", train_data.shape[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Multinomial Naive Bayes classifier in HW1.5 has a higher training error rate than sklearn's Multinomial classifier. There is a difference of 0.04.  \n",
    "One reason is that the tokenization is different. In HW1.5 tokenization resulted in 5016 features while sklearn's CountVectorizer created 5322 tokens. Also, as there are many variations of implementations of Multinomial Naive Bayes, it is possible that sklearn uses a different algorithm that what I had implemented in HW1.5. Infact, as a test I had passed the DataFrame (term frequency per document matrix) of HW1.5 to sklearn (by writing it out to a file, then reading and converting it into a numpy ndarray). Sklearn obtained a training error rate of 0.03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HW1.6 - vi:  \n",
    "Sklearn Multinomial Naive Bayes algorithm uses term frequencies to determine probabilities. While sklearn Bernoulli uses a binary representation of whether a term exists in a document (ofcourse Bernoulli also includes probabilities of non-occurrence of terms also). This means that Bernoulli Naive Bayes throws away a lot of information. This is the reason why Bernoulli training error rate is higher."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
